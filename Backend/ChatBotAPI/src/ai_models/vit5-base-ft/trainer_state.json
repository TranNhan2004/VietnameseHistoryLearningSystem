{
  "best_global_step": 3120,
  "best_metric": 1.0630760192871094,
  "best_model_checkpoint": "/kaggle/working/vit5-base-ft/checkpoint-3120",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 3120,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0009615384615384616,
      "grad_norm": 43.48529815673828,
      "learning_rate": 2e-05,
      "loss": 3.8417,
      "step": 1
    },
    {
      "epoch": 0.0019230769230769232,
      "grad_norm": 46.514137268066406,
      "learning_rate": 1.9998076923076924e-05,
      "loss": 4.7131,
      "step": 2
    },
    {
      "epoch": 0.0028846153846153848,
      "grad_norm": 36.87860107421875,
      "learning_rate": 1.9996153846153847e-05,
      "loss": 3.547,
      "step": 3
    },
    {
      "epoch": 0.0038461538461538464,
      "grad_norm": 73.7000961303711,
      "learning_rate": 1.999423076923077e-05,
      "loss": 5.3169,
      "step": 4
    },
    {
      "epoch": 0.004807692307692308,
      "grad_norm": 21.08724021911621,
      "learning_rate": 1.9992307692307693e-05,
      "loss": 3.0802,
      "step": 5
    },
    {
      "epoch": 0.0057692307692307696,
      "grad_norm": 31.281267166137695,
      "learning_rate": 1.9990384615384615e-05,
      "loss": 2.9843,
      "step": 6
    },
    {
      "epoch": 0.006730769230769231,
      "grad_norm": 50.03453826904297,
      "learning_rate": 1.998846153846154e-05,
      "loss": 2.5844,
      "step": 7
    },
    {
      "epoch": 0.007692307692307693,
      "grad_norm": 12.34208869934082,
      "learning_rate": 1.9986538461538464e-05,
      "loss": 2.072,
      "step": 8
    },
    {
      "epoch": 0.008653846153846154,
      "grad_norm": 13.26830005645752,
      "learning_rate": 1.9984615384615387e-05,
      "loss": 2.7442,
      "step": 9
    },
    {
      "epoch": 0.009615384615384616,
      "grad_norm": 28.247831344604492,
      "learning_rate": 1.998269230769231e-05,
      "loss": 2.4438,
      "step": 10
    },
    {
      "epoch": 0.010576923076923078,
      "grad_norm": 30.174386978149414,
      "learning_rate": 1.9980769230769233e-05,
      "loss": 2.16,
      "step": 11
    },
    {
      "epoch": 0.011538461538461539,
      "grad_norm": 11.61202335357666,
      "learning_rate": 1.9978846153846155e-05,
      "loss": 1.9858,
      "step": 12
    },
    {
      "epoch": 0.0125,
      "grad_norm": 10.354681968688965,
      "learning_rate": 1.9976923076923078e-05,
      "loss": 2.291,
      "step": 13
    },
    {
      "epoch": 0.013461538461538462,
      "grad_norm": 9.606993675231934,
      "learning_rate": 1.9975e-05,
      "loss": 1.9359,
      "step": 14
    },
    {
      "epoch": 0.014423076923076924,
      "grad_norm": 11.70853042602539,
      "learning_rate": 1.9973076923076924e-05,
      "loss": 2.2047,
      "step": 15
    },
    {
      "epoch": 0.015384615384615385,
      "grad_norm": 9.536673545837402,
      "learning_rate": 1.9971153846153847e-05,
      "loss": 2.3023,
      "step": 16
    },
    {
      "epoch": 0.016346153846153847,
      "grad_norm": 13.776545524597168,
      "learning_rate": 1.9969230769230773e-05,
      "loss": 2.7576,
      "step": 17
    },
    {
      "epoch": 0.01730769230769231,
      "grad_norm": 12.758790969848633,
      "learning_rate": 1.9967307692307692e-05,
      "loss": 3.0737,
      "step": 18
    },
    {
      "epoch": 0.01826923076923077,
      "grad_norm": 8.328989028930664,
      "learning_rate": 1.9965384615384618e-05,
      "loss": 1.7077,
      "step": 19
    },
    {
      "epoch": 0.019230769230769232,
      "grad_norm": 24.20398712158203,
      "learning_rate": 1.9963461538461538e-05,
      "loss": 3.9401,
      "step": 20
    },
    {
      "epoch": 0.020192307692307693,
      "grad_norm": 15.839702606201172,
      "learning_rate": 1.9961538461538464e-05,
      "loss": 2.6987,
      "step": 21
    },
    {
      "epoch": 0.021153846153846155,
      "grad_norm": 11.742331504821777,
      "learning_rate": 1.9959615384615387e-05,
      "loss": 2.5915,
      "step": 22
    },
    {
      "epoch": 0.022115384615384617,
      "grad_norm": 9.703397750854492,
      "learning_rate": 1.995769230769231e-05,
      "loss": 1.6754,
      "step": 23
    },
    {
      "epoch": 0.023076923076923078,
      "grad_norm": 11.67813491821289,
      "learning_rate": 1.9955769230769232e-05,
      "loss": 2.7728,
      "step": 24
    },
    {
      "epoch": 0.02403846153846154,
      "grad_norm": 7.772160530090332,
      "learning_rate": 1.9953846153846155e-05,
      "loss": 2.3638,
      "step": 25
    },
    {
      "epoch": 0.025,
      "grad_norm": 49.89653396606445,
      "learning_rate": 1.9951923076923078e-05,
      "loss": 2.8682,
      "step": 26
    },
    {
      "epoch": 0.025961538461538463,
      "grad_norm": 12.282271385192871,
      "learning_rate": 1.9950000000000004e-05,
      "loss": 2.017,
      "step": 27
    },
    {
      "epoch": 0.026923076923076925,
      "grad_norm": 10.224042892456055,
      "learning_rate": 1.9948076923076923e-05,
      "loss": 2.276,
      "step": 28
    },
    {
      "epoch": 0.027884615384615386,
      "grad_norm": 8.747052192687988,
      "learning_rate": 1.994615384615385e-05,
      "loss": 1.8478,
      "step": 29
    },
    {
      "epoch": 0.028846153846153848,
      "grad_norm": 7.588023662567139,
      "learning_rate": 1.994423076923077e-05,
      "loss": 1.7763,
      "step": 30
    },
    {
      "epoch": 0.02980769230769231,
      "grad_norm": 7.843837261199951,
      "learning_rate": 1.9942307692307695e-05,
      "loss": 2.0259,
      "step": 31
    },
    {
      "epoch": 0.03076923076923077,
      "grad_norm": 27.13433837890625,
      "learning_rate": 1.9940384615384618e-05,
      "loss": 1.9725,
      "step": 32
    },
    {
      "epoch": 0.03173076923076923,
      "grad_norm": 18.78215980529785,
      "learning_rate": 1.993846153846154e-05,
      "loss": 1.9797,
      "step": 33
    },
    {
      "epoch": 0.032692307692307694,
      "grad_norm": 7.773266315460205,
      "learning_rate": 1.9936538461538463e-05,
      "loss": 2.0839,
      "step": 34
    },
    {
      "epoch": 0.03365384615384615,
      "grad_norm": 9.239110946655273,
      "learning_rate": 1.9934615384615386e-05,
      "loss": 1.9959,
      "step": 35
    },
    {
      "epoch": 0.03461538461538462,
      "grad_norm": 22.626340866088867,
      "learning_rate": 1.993269230769231e-05,
      "loss": 1.678,
      "step": 36
    },
    {
      "epoch": 0.035576923076923075,
      "grad_norm": 9.792143821716309,
      "learning_rate": 1.993076923076923e-05,
      "loss": 0.8946,
      "step": 37
    },
    {
      "epoch": 0.03653846153846154,
      "grad_norm": 19.609678268432617,
      "learning_rate": 1.9928846153846154e-05,
      "loss": 1.488,
      "step": 38
    },
    {
      "epoch": 0.0375,
      "grad_norm": 20.72040367126465,
      "learning_rate": 1.992692307692308e-05,
      "loss": 1.1477,
      "step": 39
    },
    {
      "epoch": 0.038461538461538464,
      "grad_norm": 12.876666069030762,
      "learning_rate": 1.9925e-05,
      "loss": 0.824,
      "step": 40
    },
    {
      "epoch": 0.03942307692307692,
      "grad_norm": 18.24617576599121,
      "learning_rate": 1.9923076923076926e-05,
      "loss": 2.7937,
      "step": 41
    },
    {
      "epoch": 0.04038461538461539,
      "grad_norm": 9.19570541381836,
      "learning_rate": 1.9921153846153845e-05,
      "loss": 1.2183,
      "step": 42
    },
    {
      "epoch": 0.041346153846153845,
      "grad_norm": 10.383305549621582,
      "learning_rate": 1.991923076923077e-05,
      "loss": 2.2919,
      "step": 43
    },
    {
      "epoch": 0.04230769230769231,
      "grad_norm": 9.474349975585938,
      "learning_rate": 1.9917307692307694e-05,
      "loss": 2.3129,
      "step": 44
    },
    {
      "epoch": 0.04326923076923077,
      "grad_norm": 9.755414009094238,
      "learning_rate": 1.9915384615384617e-05,
      "loss": 2.2893,
      "step": 45
    },
    {
      "epoch": 0.04423076923076923,
      "grad_norm": 9.263575553894043,
      "learning_rate": 1.991346153846154e-05,
      "loss": 2.0509,
      "step": 46
    },
    {
      "epoch": 0.04519230769230769,
      "grad_norm": 16.038949966430664,
      "learning_rate": 1.9911538461538463e-05,
      "loss": 2.0935,
      "step": 47
    },
    {
      "epoch": 0.046153846153846156,
      "grad_norm": 10.238194465637207,
      "learning_rate": 1.9909615384615385e-05,
      "loss": 1.5123,
      "step": 48
    },
    {
      "epoch": 0.047115384615384615,
      "grad_norm": 6.731190204620361,
      "learning_rate": 1.990769230769231e-05,
      "loss": 2.1254,
      "step": 49
    },
    {
      "epoch": 0.04807692307692308,
      "grad_norm": 9.435776710510254,
      "learning_rate": 1.990576923076923e-05,
      "loss": 1.3331,
      "step": 50
    },
    {
      "epoch": 0.04903846153846154,
      "grad_norm": 9.739371299743652,
      "learning_rate": 1.9903846153846157e-05,
      "loss": 1.9746,
      "step": 51
    },
    {
      "epoch": 0.05,
      "grad_norm": 8.698495864868164,
      "learning_rate": 1.9901923076923076e-05,
      "loss": 2.1445,
      "step": 52
    },
    {
      "epoch": 0.05096153846153846,
      "grad_norm": 8.84399127960205,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 2.2999,
      "step": 53
    },
    {
      "epoch": 0.051923076923076926,
      "grad_norm": 26.830280303955078,
      "learning_rate": 1.9898076923076925e-05,
      "loss": 2.9293,
      "step": 54
    },
    {
      "epoch": 0.052884615384615384,
      "grad_norm": 9.715052604675293,
      "learning_rate": 1.9896153846153848e-05,
      "loss": 1.322,
      "step": 55
    },
    {
      "epoch": 0.05384615384615385,
      "grad_norm": 20.492769241333008,
      "learning_rate": 1.989423076923077e-05,
      "loss": 0.7269,
      "step": 56
    },
    {
      "epoch": 0.05480769230769231,
      "grad_norm": 26.108627319335938,
      "learning_rate": 1.9892307692307694e-05,
      "loss": 2.0071,
      "step": 57
    },
    {
      "epoch": 0.05576923076923077,
      "grad_norm": 9.843894004821777,
      "learning_rate": 1.9890384615384616e-05,
      "loss": 2.1404,
      "step": 58
    },
    {
      "epoch": 0.05673076923076923,
      "grad_norm": 10.547977447509766,
      "learning_rate": 1.9888461538461543e-05,
      "loss": 1.7191,
      "step": 59
    },
    {
      "epoch": 0.057692307692307696,
      "grad_norm": 8.84010124206543,
      "learning_rate": 1.9886538461538462e-05,
      "loss": 1.9857,
      "step": 60
    },
    {
      "epoch": 0.058653846153846154,
      "grad_norm": 9.138113975524902,
      "learning_rate": 1.9884615384615388e-05,
      "loss": 0.4615,
      "step": 61
    },
    {
      "epoch": 0.05961538461538462,
      "grad_norm": 8.186662673950195,
      "learning_rate": 1.9882692307692308e-05,
      "loss": 1.8523,
      "step": 62
    },
    {
      "epoch": 0.06057692307692308,
      "grad_norm": 10.253911018371582,
      "learning_rate": 1.9880769230769234e-05,
      "loss": 1.8189,
      "step": 63
    },
    {
      "epoch": 0.06153846153846154,
      "grad_norm": 14.781126022338867,
      "learning_rate": 1.9878846153846156e-05,
      "loss": 0.4429,
      "step": 64
    },
    {
      "epoch": 0.0625,
      "grad_norm": 10.087759017944336,
      "learning_rate": 1.987692307692308e-05,
      "loss": 1.5529,
      "step": 65
    },
    {
      "epoch": 0.06346153846153846,
      "grad_norm": 8.190285682678223,
      "learning_rate": 1.9875000000000002e-05,
      "loss": 2.3372,
      "step": 66
    },
    {
      "epoch": 0.06442307692307692,
      "grad_norm": 15.576761245727539,
      "learning_rate": 1.9873076923076925e-05,
      "loss": 1.9456,
      "step": 67
    },
    {
      "epoch": 0.06538461538461539,
      "grad_norm": 13.012580871582031,
      "learning_rate": 1.9871153846153848e-05,
      "loss": 1.8619,
      "step": 68
    },
    {
      "epoch": 0.06634615384615385,
      "grad_norm": 7.909182071685791,
      "learning_rate": 1.986923076923077e-05,
      "loss": 1.5737,
      "step": 69
    },
    {
      "epoch": 0.0673076923076923,
      "grad_norm": 7.331230640411377,
      "learning_rate": 1.9867307692307693e-05,
      "loss": 2.4315,
      "step": 70
    },
    {
      "epoch": 0.06826923076923076,
      "grad_norm": 13.980448722839355,
      "learning_rate": 1.9865384615384616e-05,
      "loss": 1.1153,
      "step": 71
    },
    {
      "epoch": 0.06923076923076923,
      "grad_norm": 8.79509162902832,
      "learning_rate": 1.986346153846154e-05,
      "loss": 2.6035,
      "step": 72
    },
    {
      "epoch": 0.07019230769230769,
      "grad_norm": 5.976876258850098,
      "learning_rate": 1.9861538461538465e-05,
      "loss": 1.9829,
      "step": 73
    },
    {
      "epoch": 0.07115384615384615,
      "grad_norm": 13.78649616241455,
      "learning_rate": 1.9859615384615388e-05,
      "loss": 1.4569,
      "step": 74
    },
    {
      "epoch": 0.07211538461538461,
      "grad_norm": 8.018126487731934,
      "learning_rate": 1.985769230769231e-05,
      "loss": 2.488,
      "step": 75
    },
    {
      "epoch": 0.07307692307692308,
      "grad_norm": 7.7750420570373535,
      "learning_rate": 1.9855769230769233e-05,
      "loss": 1.5744,
      "step": 76
    },
    {
      "epoch": 0.07403846153846154,
      "grad_norm": 14.754722595214844,
      "learning_rate": 1.9853846153846156e-05,
      "loss": 1.0228,
      "step": 77
    },
    {
      "epoch": 0.075,
      "grad_norm": 8.976117134094238,
      "learning_rate": 1.985192307692308e-05,
      "loss": 0.3738,
      "step": 78
    },
    {
      "epoch": 0.07596153846153846,
      "grad_norm": 13.569693565368652,
      "learning_rate": 1.985e-05,
      "loss": 1.5876,
      "step": 79
    },
    {
      "epoch": 0.07692307692307693,
      "grad_norm": 9.046676635742188,
      "learning_rate": 1.9848076923076924e-05,
      "loss": 2.1264,
      "step": 80
    },
    {
      "epoch": 0.07788461538461539,
      "grad_norm": 9.169219017028809,
      "learning_rate": 1.9846153846153847e-05,
      "loss": 0.7731,
      "step": 81
    },
    {
      "epoch": 0.07884615384615384,
      "grad_norm": 8.973785400390625,
      "learning_rate": 1.984423076923077e-05,
      "loss": 2.0111,
      "step": 82
    },
    {
      "epoch": 0.0798076923076923,
      "grad_norm": 20.876041412353516,
      "learning_rate": 1.9842307692307692e-05,
      "loss": 0.6046,
      "step": 83
    },
    {
      "epoch": 0.08076923076923077,
      "grad_norm": 15.950969696044922,
      "learning_rate": 1.984038461538462e-05,
      "loss": 0.9256,
      "step": 84
    },
    {
      "epoch": 0.08173076923076923,
      "grad_norm": 8.438873291015625,
      "learning_rate": 1.983846153846154e-05,
      "loss": 2.0589,
      "step": 85
    },
    {
      "epoch": 0.08269230769230769,
      "grad_norm": 7.946158409118652,
      "learning_rate": 1.9836538461538464e-05,
      "loss": 2.1594,
      "step": 86
    },
    {
      "epoch": 0.08365384615384615,
      "grad_norm": 9.294726371765137,
      "learning_rate": 1.9834615384615387e-05,
      "loss": 1.5106,
      "step": 87
    },
    {
      "epoch": 0.08461538461538462,
      "grad_norm": 18.123994827270508,
      "learning_rate": 1.983269230769231e-05,
      "loss": 2.0719,
      "step": 88
    },
    {
      "epoch": 0.08557692307692308,
      "grad_norm": 9.624259948730469,
      "learning_rate": 1.9830769230769232e-05,
      "loss": 1.9945,
      "step": 89
    },
    {
      "epoch": 0.08653846153846154,
      "grad_norm": 9.458894729614258,
      "learning_rate": 1.9828846153846155e-05,
      "loss": 1.9077,
      "step": 90
    },
    {
      "epoch": 0.0875,
      "grad_norm": 6.773076057434082,
      "learning_rate": 1.9826923076923078e-05,
      "loss": 1.7707,
      "step": 91
    },
    {
      "epoch": 0.08846153846153847,
      "grad_norm": 11.603113174438477,
      "learning_rate": 1.9825e-05,
      "loss": 1.6301,
      "step": 92
    },
    {
      "epoch": 0.08942307692307692,
      "grad_norm": 7.4515461921691895,
      "learning_rate": 1.9823076923076924e-05,
      "loss": 1.7042,
      "step": 93
    },
    {
      "epoch": 0.09038461538461538,
      "grad_norm": 8.470282554626465,
      "learning_rate": 1.982115384615385e-05,
      "loss": 1.7289,
      "step": 94
    },
    {
      "epoch": 0.09134615384615384,
      "grad_norm": 7.625721454620361,
      "learning_rate": 1.981923076923077e-05,
      "loss": 1.9646,
      "step": 95
    },
    {
      "epoch": 0.09230769230769231,
      "grad_norm": 10.164276123046875,
      "learning_rate": 1.9817307692307695e-05,
      "loss": 1.8493,
      "step": 96
    },
    {
      "epoch": 0.09326923076923077,
      "grad_norm": 10.533658981323242,
      "learning_rate": 1.9815384615384618e-05,
      "loss": 1.7789,
      "step": 97
    },
    {
      "epoch": 0.09423076923076923,
      "grad_norm": 8.359477043151855,
      "learning_rate": 1.981346153846154e-05,
      "loss": 1.5324,
      "step": 98
    },
    {
      "epoch": 0.09519230769230769,
      "grad_norm": 7.71774435043335,
      "learning_rate": 1.9811538461538464e-05,
      "loss": 1.4946,
      "step": 99
    },
    {
      "epoch": 0.09615384615384616,
      "grad_norm": 10.110370635986328,
      "learning_rate": 1.9809615384615386e-05,
      "loss": 2.4323,
      "step": 100
    },
    {
      "epoch": 0.09711538461538462,
      "grad_norm": 21.769386291503906,
      "learning_rate": 1.980769230769231e-05,
      "loss": 1.9126,
      "step": 101
    },
    {
      "epoch": 0.09807692307692308,
      "grad_norm": 10.683867454528809,
      "learning_rate": 1.9805769230769232e-05,
      "loss": 0.6022,
      "step": 102
    },
    {
      "epoch": 0.09903846153846153,
      "grad_norm": 10.697609901428223,
      "learning_rate": 1.9803846153846155e-05,
      "loss": 1.086,
      "step": 103
    },
    {
      "epoch": 0.1,
      "grad_norm": 11.561178207397461,
      "learning_rate": 1.9801923076923077e-05,
      "loss": 2.7777,
      "step": 104
    },
    {
      "epoch": 0.10096153846153846,
      "grad_norm": 13.19663143157959,
      "learning_rate": 1.98e-05,
      "loss": 1.9682,
      "step": 105
    },
    {
      "epoch": 0.10192307692307692,
      "grad_norm": 7.2306599617004395,
      "learning_rate": 1.9798076923076926e-05,
      "loss": 1.9372,
      "step": 106
    },
    {
      "epoch": 0.10288461538461538,
      "grad_norm": 12.557513236999512,
      "learning_rate": 1.9796153846153846e-05,
      "loss": 1.7518,
      "step": 107
    },
    {
      "epoch": 0.10384615384615385,
      "grad_norm": 8.99621295928955,
      "learning_rate": 1.9794230769230772e-05,
      "loss": 1.0358,
      "step": 108
    },
    {
      "epoch": 0.10480769230769231,
      "grad_norm": 12.438272476196289,
      "learning_rate": 1.979230769230769e-05,
      "loss": 0.7736,
      "step": 109
    },
    {
      "epoch": 0.10576923076923077,
      "grad_norm": 12.918782234191895,
      "learning_rate": 1.9790384615384617e-05,
      "loss": 1.0603,
      "step": 110
    },
    {
      "epoch": 0.10673076923076923,
      "grad_norm": 9.818550109863281,
      "learning_rate": 1.978846153846154e-05,
      "loss": 1.931,
      "step": 111
    },
    {
      "epoch": 0.1076923076923077,
      "grad_norm": 9.960639953613281,
      "learning_rate": 1.9786538461538463e-05,
      "loss": 1.2827,
      "step": 112
    },
    {
      "epoch": 0.10865384615384616,
      "grad_norm": 10.351763725280762,
      "learning_rate": 1.9784615384615386e-05,
      "loss": 0.577,
      "step": 113
    },
    {
      "epoch": 0.10961538461538461,
      "grad_norm": 9.918054580688477,
      "learning_rate": 1.978269230769231e-05,
      "loss": 1.8707,
      "step": 114
    },
    {
      "epoch": 0.11057692307692307,
      "grad_norm": 12.611089706420898,
      "learning_rate": 1.978076923076923e-05,
      "loss": 1.5161,
      "step": 115
    },
    {
      "epoch": 0.11153846153846154,
      "grad_norm": 9.659815788269043,
      "learning_rate": 1.9778846153846157e-05,
      "loss": 1.364,
      "step": 116
    },
    {
      "epoch": 0.1125,
      "grad_norm": 9.164697647094727,
      "learning_rate": 1.9776923076923077e-05,
      "loss": 1.5919,
      "step": 117
    },
    {
      "epoch": 0.11346153846153846,
      "grad_norm": 11.52639102935791,
      "learning_rate": 1.9775000000000003e-05,
      "loss": 1.731,
      "step": 118
    },
    {
      "epoch": 0.11442307692307692,
      "grad_norm": 16.67502212524414,
      "learning_rate": 1.9773076923076922e-05,
      "loss": 2.1127,
      "step": 119
    },
    {
      "epoch": 0.11538461538461539,
      "grad_norm": 7.161319732666016,
      "learning_rate": 1.977115384615385e-05,
      "loss": 0.7492,
      "step": 120
    },
    {
      "epoch": 0.11634615384615385,
      "grad_norm": 6.69520902633667,
      "learning_rate": 1.976923076923077e-05,
      "loss": 1.6278,
      "step": 121
    },
    {
      "epoch": 0.11730769230769231,
      "grad_norm": 7.903555393218994,
      "learning_rate": 1.9767307692307694e-05,
      "loss": 1.9839,
      "step": 122
    },
    {
      "epoch": 0.11826923076923077,
      "grad_norm": 9.589085578918457,
      "learning_rate": 1.9765384615384617e-05,
      "loss": 1.499,
      "step": 123
    },
    {
      "epoch": 0.11923076923076924,
      "grad_norm": 6.949929237365723,
      "learning_rate": 1.976346153846154e-05,
      "loss": 2.0188,
      "step": 124
    },
    {
      "epoch": 0.1201923076923077,
      "grad_norm": 8.039728164672852,
      "learning_rate": 1.9761538461538462e-05,
      "loss": 1.7519,
      "step": 125
    },
    {
      "epoch": 0.12115384615384615,
      "grad_norm": 10.98059368133545,
      "learning_rate": 1.975961538461539e-05,
      "loss": 1.8551,
      "step": 126
    },
    {
      "epoch": 0.12211538461538461,
      "grad_norm": 11.69779109954834,
      "learning_rate": 1.9757692307692308e-05,
      "loss": 2.3334,
      "step": 127
    },
    {
      "epoch": 0.12307692307692308,
      "grad_norm": 8.14645004272461,
      "learning_rate": 1.9755769230769234e-05,
      "loss": 1.5343,
      "step": 128
    },
    {
      "epoch": 0.12403846153846154,
      "grad_norm": 13.090764999389648,
      "learning_rate": 1.9753846153846153e-05,
      "loss": 2.022,
      "step": 129
    },
    {
      "epoch": 0.125,
      "grad_norm": 12.982156753540039,
      "learning_rate": 1.975192307692308e-05,
      "loss": 0.574,
      "step": 130
    },
    {
      "epoch": 0.12596153846153846,
      "grad_norm": 19.96062660217285,
      "learning_rate": 1.9750000000000002e-05,
      "loss": 3.732,
      "step": 131
    },
    {
      "epoch": 0.12692307692307692,
      "grad_norm": 9.072748184204102,
      "learning_rate": 1.9748076923076925e-05,
      "loss": 1.7091,
      "step": 132
    },
    {
      "epoch": 0.12788461538461537,
      "grad_norm": 9.574533462524414,
      "learning_rate": 1.9746153846153848e-05,
      "loss": 1.8488,
      "step": 133
    },
    {
      "epoch": 0.12884615384615383,
      "grad_norm": 7.988066673278809,
      "learning_rate": 1.974423076923077e-05,
      "loss": 1.4199,
      "step": 134
    },
    {
      "epoch": 0.12980769230769232,
      "grad_norm": 9.434931755065918,
      "learning_rate": 1.9742307692307693e-05,
      "loss": 1.8248,
      "step": 135
    },
    {
      "epoch": 0.13076923076923078,
      "grad_norm": 6.678743362426758,
      "learning_rate": 1.974038461538462e-05,
      "loss": 1.7143,
      "step": 136
    },
    {
      "epoch": 0.13173076923076923,
      "grad_norm": 8.29311466217041,
      "learning_rate": 1.973846153846154e-05,
      "loss": 1.7961,
      "step": 137
    },
    {
      "epoch": 0.1326923076923077,
      "grad_norm": 8.013113021850586,
      "learning_rate": 1.9736538461538465e-05,
      "loss": 1.9805,
      "step": 138
    },
    {
      "epoch": 0.13365384615384615,
      "grad_norm": 6.903805255889893,
      "learning_rate": 1.9734615384615384e-05,
      "loss": 1.8173,
      "step": 139
    },
    {
      "epoch": 0.1346153846153846,
      "grad_norm": 7.433210849761963,
      "learning_rate": 1.973269230769231e-05,
      "loss": 1.9381,
      "step": 140
    },
    {
      "epoch": 0.13557692307692307,
      "grad_norm": 8.84521198272705,
      "learning_rate": 1.9730769230769233e-05,
      "loss": 2.0314,
      "step": 141
    },
    {
      "epoch": 0.13653846153846153,
      "grad_norm": 8.000662803649902,
      "learning_rate": 1.9728846153846156e-05,
      "loss": 1.2931,
      "step": 142
    },
    {
      "epoch": 0.1375,
      "grad_norm": 6.799877643585205,
      "learning_rate": 1.972692307692308e-05,
      "loss": 2.1836,
      "step": 143
    },
    {
      "epoch": 0.13846153846153847,
      "grad_norm": 7.99531888961792,
      "learning_rate": 1.9725000000000002e-05,
      "loss": 0.9166,
      "step": 144
    },
    {
      "epoch": 0.13942307692307693,
      "grad_norm": 9.408036231994629,
      "learning_rate": 1.9723076923076924e-05,
      "loss": 1.0285,
      "step": 145
    },
    {
      "epoch": 0.14038461538461539,
      "grad_norm": 8.925758361816406,
      "learning_rate": 1.9721153846153847e-05,
      "loss": 1.6873,
      "step": 146
    },
    {
      "epoch": 0.14134615384615384,
      "grad_norm": 7.725054740905762,
      "learning_rate": 1.971923076923077e-05,
      "loss": 1.5258,
      "step": 147
    },
    {
      "epoch": 0.1423076923076923,
      "grad_norm": 8.111167907714844,
      "learning_rate": 1.9717307692307693e-05,
      "loss": 1.6889,
      "step": 148
    },
    {
      "epoch": 0.14326923076923076,
      "grad_norm": 7.910286903381348,
      "learning_rate": 1.9715384615384616e-05,
      "loss": 1.4871,
      "step": 149
    },
    {
      "epoch": 0.14423076923076922,
      "grad_norm": 7.539527416229248,
      "learning_rate": 1.9713461538461542e-05,
      "loss": 2.8451,
      "step": 150
    },
    {
      "epoch": 0.1451923076923077,
      "grad_norm": 10.854071617126465,
      "learning_rate": 1.9711538461538465e-05,
      "loss": 1.9732,
      "step": 151
    },
    {
      "epoch": 0.14615384615384616,
      "grad_norm": 10.270037651062012,
      "learning_rate": 1.9709615384615387e-05,
      "loss": 1.4524,
      "step": 152
    },
    {
      "epoch": 0.14711538461538462,
      "grad_norm": 6.909096717834473,
      "learning_rate": 1.970769230769231e-05,
      "loss": 1.5618,
      "step": 153
    },
    {
      "epoch": 0.14807692307692308,
      "grad_norm": 9.106178283691406,
      "learning_rate": 1.9705769230769233e-05,
      "loss": 1.1394,
      "step": 154
    },
    {
      "epoch": 0.14903846153846154,
      "grad_norm": 45.27668762207031,
      "learning_rate": 1.9703846153846156e-05,
      "loss": 1.4047,
      "step": 155
    },
    {
      "epoch": 0.15,
      "grad_norm": 8.775711059570312,
      "learning_rate": 1.970192307692308e-05,
      "loss": 1.0237,
      "step": 156
    },
    {
      "epoch": 0.15096153846153845,
      "grad_norm": 6.736706256866455,
      "learning_rate": 1.97e-05,
      "loss": 1.1799,
      "step": 157
    },
    {
      "epoch": 0.1519230769230769,
      "grad_norm": 29.021345138549805,
      "learning_rate": 1.9698076923076924e-05,
      "loss": 1.3826,
      "step": 158
    },
    {
      "epoch": 0.1528846153846154,
      "grad_norm": 45.611488342285156,
      "learning_rate": 1.9696153846153847e-05,
      "loss": 0.8527,
      "step": 159
    },
    {
      "epoch": 0.15384615384615385,
      "grad_norm": 13.118330001831055,
      "learning_rate": 1.969423076923077e-05,
      "loss": 1.0613,
      "step": 160
    },
    {
      "epoch": 0.1548076923076923,
      "grad_norm": 8.830142974853516,
      "learning_rate": 1.9692307692307696e-05,
      "loss": 1.9678,
      "step": 161
    },
    {
      "epoch": 0.15576923076923077,
      "grad_norm": 8.397336959838867,
      "learning_rate": 1.969038461538462e-05,
      "loss": 1.326,
      "step": 162
    },
    {
      "epoch": 0.15673076923076923,
      "grad_norm": 8.119606971740723,
      "learning_rate": 1.968846153846154e-05,
      "loss": 1.9641,
      "step": 163
    },
    {
      "epoch": 0.1576923076923077,
      "grad_norm": 11.605634689331055,
      "learning_rate": 1.9686538461538464e-05,
      "loss": 1.6275,
      "step": 164
    },
    {
      "epoch": 0.15865384615384615,
      "grad_norm": 10.721351623535156,
      "learning_rate": 1.9684615384615387e-05,
      "loss": 0.6309,
      "step": 165
    },
    {
      "epoch": 0.1596153846153846,
      "grad_norm": 53.59333038330078,
      "learning_rate": 1.968269230769231e-05,
      "loss": 1.4482,
      "step": 166
    },
    {
      "epoch": 0.1605769230769231,
      "grad_norm": 7.338930606842041,
      "learning_rate": 1.9680769230769232e-05,
      "loss": 2.1366,
      "step": 167
    },
    {
      "epoch": 0.16153846153846155,
      "grad_norm": 8.999205589294434,
      "learning_rate": 1.9678846153846155e-05,
      "loss": 2.4111,
      "step": 168
    },
    {
      "epoch": 0.1625,
      "grad_norm": 5.405247211456299,
      "learning_rate": 1.9676923076923078e-05,
      "loss": 1.155,
      "step": 169
    },
    {
      "epoch": 0.16346153846153846,
      "grad_norm": 10.606736183166504,
      "learning_rate": 1.9675e-05,
      "loss": 1.5861,
      "step": 170
    },
    {
      "epoch": 0.16442307692307692,
      "grad_norm": 8.330092430114746,
      "learning_rate": 1.9673076923076923e-05,
      "loss": 1.3949,
      "step": 171
    },
    {
      "epoch": 0.16538461538461538,
      "grad_norm": 14.433353424072266,
      "learning_rate": 1.9671153846153846e-05,
      "loss": 1.9645,
      "step": 172
    },
    {
      "epoch": 0.16634615384615384,
      "grad_norm": 11.952242851257324,
      "learning_rate": 1.9669230769230772e-05,
      "loss": 1.335,
      "step": 173
    },
    {
      "epoch": 0.1673076923076923,
      "grad_norm": 7.922763824462891,
      "learning_rate": 1.9667307692307695e-05,
      "loss": 1.3953,
      "step": 174
    },
    {
      "epoch": 0.16826923076923078,
      "grad_norm": 9.636284828186035,
      "learning_rate": 1.9665384615384618e-05,
      "loss": 1.3584,
      "step": 175
    },
    {
      "epoch": 0.16923076923076924,
      "grad_norm": 6.065515041351318,
      "learning_rate": 1.966346153846154e-05,
      "loss": 1.9454,
      "step": 176
    },
    {
      "epoch": 0.1701923076923077,
      "grad_norm": 5.583823204040527,
      "learning_rate": 1.9661538461538463e-05,
      "loss": 0.1199,
      "step": 177
    },
    {
      "epoch": 0.17115384615384616,
      "grad_norm": 12.425978660583496,
      "learning_rate": 1.9659615384615386e-05,
      "loss": 1.3833,
      "step": 178
    },
    {
      "epoch": 0.17211538461538461,
      "grad_norm": 8.333022117614746,
      "learning_rate": 1.965769230769231e-05,
      "loss": 1.9719,
      "step": 179
    },
    {
      "epoch": 0.17307692307692307,
      "grad_norm": 13.03521728515625,
      "learning_rate": 1.965576923076923e-05,
      "loss": 1.5973,
      "step": 180
    },
    {
      "epoch": 0.17403846153846153,
      "grad_norm": 9.91159725189209,
      "learning_rate": 1.9653846153846154e-05,
      "loss": 0.3324,
      "step": 181
    },
    {
      "epoch": 0.175,
      "grad_norm": 10.399205207824707,
      "learning_rate": 1.9651923076923077e-05,
      "loss": 2.2294,
      "step": 182
    },
    {
      "epoch": 0.17596153846153847,
      "grad_norm": 14.101957321166992,
      "learning_rate": 1.9650000000000003e-05,
      "loss": 1.1763,
      "step": 183
    },
    {
      "epoch": 0.17692307692307693,
      "grad_norm": 7.8153977394104,
      "learning_rate": 1.9648076923076923e-05,
      "loss": 1.2863,
      "step": 184
    },
    {
      "epoch": 0.1778846153846154,
      "grad_norm": 10.960132598876953,
      "learning_rate": 1.964615384615385e-05,
      "loss": 2.0695,
      "step": 185
    },
    {
      "epoch": 0.17884615384615385,
      "grad_norm": 6.969501972198486,
      "learning_rate": 1.9644230769230768e-05,
      "loss": 1.2116,
      "step": 186
    },
    {
      "epoch": 0.1798076923076923,
      "grad_norm": 7.118475437164307,
      "learning_rate": 1.9642307692307694e-05,
      "loss": 1.5597,
      "step": 187
    },
    {
      "epoch": 0.18076923076923077,
      "grad_norm": 7.82936954498291,
      "learning_rate": 1.9640384615384617e-05,
      "loss": 0.4137,
      "step": 188
    },
    {
      "epoch": 0.18173076923076922,
      "grad_norm": 22.45087432861328,
      "learning_rate": 1.963846153846154e-05,
      "loss": 1.7735,
      "step": 189
    },
    {
      "epoch": 0.18269230769230768,
      "grad_norm": 12.508747100830078,
      "learning_rate": 1.9636538461538463e-05,
      "loss": 1.1154,
      "step": 190
    },
    {
      "epoch": 0.18365384615384617,
      "grad_norm": 30.064647674560547,
      "learning_rate": 1.9634615384615385e-05,
      "loss": 1.4239,
      "step": 191
    },
    {
      "epoch": 0.18461538461538463,
      "grad_norm": 12.094758987426758,
      "learning_rate": 1.9632692307692308e-05,
      "loss": 1.3392,
      "step": 192
    },
    {
      "epoch": 0.18557692307692308,
      "grad_norm": 10.342024803161621,
      "learning_rate": 1.9630769230769234e-05,
      "loss": 0.2549,
      "step": 193
    },
    {
      "epoch": 0.18653846153846154,
      "grad_norm": 6.047560691833496,
      "learning_rate": 1.9628846153846154e-05,
      "loss": 1.543,
      "step": 194
    },
    {
      "epoch": 0.1875,
      "grad_norm": 12.019491195678711,
      "learning_rate": 1.962692307692308e-05,
      "loss": 1.8626,
      "step": 195
    },
    {
      "epoch": 0.18846153846153846,
      "grad_norm": 8.68238353729248,
      "learning_rate": 1.9625e-05,
      "loss": 0.9793,
      "step": 196
    },
    {
      "epoch": 0.18942307692307692,
      "grad_norm": 11.497550010681152,
      "learning_rate": 1.9623076923076925e-05,
      "loss": 1.7646,
      "step": 197
    },
    {
      "epoch": 0.19038461538461537,
      "grad_norm": 6.27947473526001,
      "learning_rate": 1.9621153846153848e-05,
      "loss": 1.6783,
      "step": 198
    },
    {
      "epoch": 0.19134615384615383,
      "grad_norm": 7.895557403564453,
      "learning_rate": 1.961923076923077e-05,
      "loss": 1.4181,
      "step": 199
    },
    {
      "epoch": 0.19230769230769232,
      "grad_norm": 23.727651596069336,
      "learning_rate": 1.9617307692307694e-05,
      "loss": 0.4293,
      "step": 200
    },
    {
      "epoch": 0.19326923076923078,
      "grad_norm": 8.422698974609375,
      "learning_rate": 1.9615384615384617e-05,
      "loss": 0.9166,
      "step": 201
    },
    {
      "epoch": 0.19423076923076923,
      "grad_norm": 7.615626811981201,
      "learning_rate": 1.961346153846154e-05,
      "loss": 1.3338,
      "step": 202
    },
    {
      "epoch": 0.1951923076923077,
      "grad_norm": 7.976497650146484,
      "learning_rate": 1.9611538461538465e-05,
      "loss": 1.945,
      "step": 203
    },
    {
      "epoch": 0.19615384615384615,
      "grad_norm": 11.663540840148926,
      "learning_rate": 1.9609615384615385e-05,
      "loss": 1.4155,
      "step": 204
    },
    {
      "epoch": 0.1971153846153846,
      "grad_norm": 30.218910217285156,
      "learning_rate": 1.960769230769231e-05,
      "loss": 1.6624,
      "step": 205
    },
    {
      "epoch": 0.19807692307692307,
      "grad_norm": 9.421281814575195,
      "learning_rate": 1.960576923076923e-05,
      "loss": 1.4291,
      "step": 206
    },
    {
      "epoch": 0.19903846153846153,
      "grad_norm": 13.168008804321289,
      "learning_rate": 1.9603846153846157e-05,
      "loss": 1.5497,
      "step": 207
    },
    {
      "epoch": 0.2,
      "grad_norm": 7.956195831298828,
      "learning_rate": 1.960192307692308e-05,
      "loss": 1.2709,
      "step": 208
    },
    {
      "epoch": 0.20096153846153847,
      "grad_norm": 11.81495189666748,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 1.7024,
      "step": 209
    },
    {
      "epoch": 0.20192307692307693,
      "grad_norm": 6.696154594421387,
      "learning_rate": 1.9598076923076925e-05,
      "loss": 1.8974,
      "step": 210
    },
    {
      "epoch": 0.20288461538461539,
      "grad_norm": 23.64401626586914,
      "learning_rate": 1.9596153846153848e-05,
      "loss": 0.3574,
      "step": 211
    },
    {
      "epoch": 0.20384615384615384,
      "grad_norm": 9.105941772460938,
      "learning_rate": 1.959423076923077e-05,
      "loss": 0.7186,
      "step": 212
    },
    {
      "epoch": 0.2048076923076923,
      "grad_norm": 11.533029556274414,
      "learning_rate": 1.9592307692307697e-05,
      "loss": 1.2194,
      "step": 213
    },
    {
      "epoch": 0.20576923076923076,
      "grad_norm": 7.940700054168701,
      "learning_rate": 1.9590384615384616e-05,
      "loss": 1.724,
      "step": 214
    },
    {
      "epoch": 0.20673076923076922,
      "grad_norm": 7.108241081237793,
      "learning_rate": 1.9588461538461542e-05,
      "loss": 1.9553,
      "step": 215
    },
    {
      "epoch": 0.2076923076923077,
      "grad_norm": 8.219494819641113,
      "learning_rate": 1.958653846153846e-05,
      "loss": 0.9959,
      "step": 216
    },
    {
      "epoch": 0.20865384615384616,
      "grad_norm": 7.1842145919799805,
      "learning_rate": 1.9584615384615388e-05,
      "loss": 2.3928,
      "step": 217
    },
    {
      "epoch": 0.20961538461538462,
      "grad_norm": 10.258916854858398,
      "learning_rate": 1.958269230769231e-05,
      "loss": 2.2993,
      "step": 218
    },
    {
      "epoch": 0.21057692307692308,
      "grad_norm": 6.789012908935547,
      "learning_rate": 1.9580769230769233e-05,
      "loss": 1.0649,
      "step": 219
    },
    {
      "epoch": 0.21153846153846154,
      "grad_norm": 10.539407730102539,
      "learning_rate": 1.9578846153846156e-05,
      "loss": 1.4989,
      "step": 220
    },
    {
      "epoch": 0.2125,
      "grad_norm": 9.504867553710938,
      "learning_rate": 1.957692307692308e-05,
      "loss": 1.8944,
      "step": 221
    },
    {
      "epoch": 0.21346153846153845,
      "grad_norm": 9.452640533447266,
      "learning_rate": 1.9575e-05,
      "loss": 1.8253,
      "step": 222
    },
    {
      "epoch": 0.2144230769230769,
      "grad_norm": 19.090890884399414,
      "learning_rate": 1.9573076923076924e-05,
      "loss": 2.6231,
      "step": 223
    },
    {
      "epoch": 0.2153846153846154,
      "grad_norm": 14.627067565917969,
      "learning_rate": 1.9571153846153847e-05,
      "loss": 1.0927,
      "step": 224
    },
    {
      "epoch": 0.21634615384615385,
      "grad_norm": 9.924495697021484,
      "learning_rate": 1.9569230769230773e-05,
      "loss": 1.8227,
      "step": 225
    },
    {
      "epoch": 0.2173076923076923,
      "grad_norm": 7.721246719360352,
      "learning_rate": 1.9567307692307693e-05,
      "loss": 1.5347,
      "step": 226
    },
    {
      "epoch": 0.21826923076923077,
      "grad_norm": 6.309359073638916,
      "learning_rate": 1.956538461538462e-05,
      "loss": 1.6337,
      "step": 227
    },
    {
      "epoch": 0.21923076923076923,
      "grad_norm": 48.89228439331055,
      "learning_rate": 1.956346153846154e-05,
      "loss": 2.2069,
      "step": 228
    },
    {
      "epoch": 0.2201923076923077,
      "grad_norm": 9.143671035766602,
      "learning_rate": 1.9561538461538464e-05,
      "loss": 1.0978,
      "step": 229
    },
    {
      "epoch": 0.22115384615384615,
      "grad_norm": 10.356322288513184,
      "learning_rate": 1.9559615384615387e-05,
      "loss": 1.074,
      "step": 230
    },
    {
      "epoch": 0.2221153846153846,
      "grad_norm": 7.336023330688477,
      "learning_rate": 1.955769230769231e-05,
      "loss": 1.6302,
      "step": 231
    },
    {
      "epoch": 0.2230769230769231,
      "grad_norm": 11.442567825317383,
      "learning_rate": 1.9555769230769233e-05,
      "loss": 1.9939,
      "step": 232
    },
    {
      "epoch": 0.22403846153846155,
      "grad_norm": 4.667815208435059,
      "learning_rate": 1.9553846153846155e-05,
      "loss": 0.052,
      "step": 233
    },
    {
      "epoch": 0.225,
      "grad_norm": 10.784536361694336,
      "learning_rate": 1.9551923076923078e-05,
      "loss": 2.1531,
      "step": 234
    },
    {
      "epoch": 0.22596153846153846,
      "grad_norm": 9.654261589050293,
      "learning_rate": 1.955e-05,
      "loss": 0.6033,
      "step": 235
    },
    {
      "epoch": 0.22692307692307692,
      "grad_norm": 13.34878158569336,
      "learning_rate": 1.9548076923076924e-05,
      "loss": 0.9061,
      "step": 236
    },
    {
      "epoch": 0.22788461538461538,
      "grad_norm": 8.579567909240723,
      "learning_rate": 1.9546153846153846e-05,
      "loss": 0.8147,
      "step": 237
    },
    {
      "epoch": 0.22884615384615384,
      "grad_norm": 9.953521728515625,
      "learning_rate": 1.954423076923077e-05,
      "loss": 1.6676,
      "step": 238
    },
    {
      "epoch": 0.2298076923076923,
      "grad_norm": 11.0885648727417,
      "learning_rate": 1.9542307692307695e-05,
      "loss": 1.4637,
      "step": 239
    },
    {
      "epoch": 0.23076923076923078,
      "grad_norm": 6.1397528648376465,
      "learning_rate": 1.9540384615384618e-05,
      "loss": 1.759,
      "step": 240
    },
    {
      "epoch": 0.23173076923076924,
      "grad_norm": 12.36445140838623,
      "learning_rate": 1.953846153846154e-05,
      "loss": 1.6727,
      "step": 241
    },
    {
      "epoch": 0.2326923076923077,
      "grad_norm": 9.153931617736816,
      "learning_rate": 1.9536538461538464e-05,
      "loss": 1.9089,
      "step": 242
    },
    {
      "epoch": 0.23365384615384616,
      "grad_norm": 23.013154983520508,
      "learning_rate": 1.9534615384615386e-05,
      "loss": 1.1557,
      "step": 243
    },
    {
      "epoch": 0.23461538461538461,
      "grad_norm": 23.62095832824707,
      "learning_rate": 1.953269230769231e-05,
      "loss": 2.5103,
      "step": 244
    },
    {
      "epoch": 0.23557692307692307,
      "grad_norm": 6.244285583496094,
      "learning_rate": 1.9530769230769232e-05,
      "loss": 1.5338,
      "step": 245
    },
    {
      "epoch": 0.23653846153846153,
      "grad_norm": 20.646087646484375,
      "learning_rate": 1.9528846153846155e-05,
      "loss": 1.9778,
      "step": 246
    },
    {
      "epoch": 0.2375,
      "grad_norm": 6.174270153045654,
      "learning_rate": 1.9526923076923077e-05,
      "loss": 1.6063,
      "step": 247
    },
    {
      "epoch": 0.23846153846153847,
      "grad_norm": 8.340909004211426,
      "learning_rate": 1.9525e-05,
      "loss": 1.1283,
      "step": 248
    },
    {
      "epoch": 0.23942307692307693,
      "grad_norm": 16.610153198242188,
      "learning_rate": 1.9523076923076923e-05,
      "loss": 0.6624,
      "step": 249
    },
    {
      "epoch": 0.2403846153846154,
      "grad_norm": 10.146742820739746,
      "learning_rate": 1.952115384615385e-05,
      "loss": 1.4494,
      "step": 250
    },
    {
      "epoch": 0.24134615384615385,
      "grad_norm": 11.150970458984375,
      "learning_rate": 1.9519230769230772e-05,
      "loss": 1.7172,
      "step": 251
    },
    {
      "epoch": 0.2423076923076923,
      "grad_norm": 10.127263069152832,
      "learning_rate": 1.9517307692307695e-05,
      "loss": 1.572,
      "step": 252
    },
    {
      "epoch": 0.24326923076923077,
      "grad_norm": 8.958888053894043,
      "learning_rate": 1.9515384615384617e-05,
      "loss": 1.4899,
      "step": 253
    },
    {
      "epoch": 0.24423076923076922,
      "grad_norm": 7.316047191619873,
      "learning_rate": 1.951346153846154e-05,
      "loss": 1.5192,
      "step": 254
    },
    {
      "epoch": 0.24519230769230768,
      "grad_norm": 11.132756233215332,
      "learning_rate": 1.9511538461538463e-05,
      "loss": 2.1126,
      "step": 255
    },
    {
      "epoch": 0.24615384615384617,
      "grad_norm": 8.821897506713867,
      "learning_rate": 1.9509615384615386e-05,
      "loss": 1.7196,
      "step": 256
    },
    {
      "epoch": 0.24711538461538463,
      "grad_norm": 9.07905387878418,
      "learning_rate": 1.950769230769231e-05,
      "loss": 1.7287,
      "step": 257
    },
    {
      "epoch": 0.24807692307692308,
      "grad_norm": 9.160577774047852,
      "learning_rate": 1.950576923076923e-05,
      "loss": 1.6861,
      "step": 258
    },
    {
      "epoch": 0.24903846153846154,
      "grad_norm": 16.56338882446289,
      "learning_rate": 1.9503846153846154e-05,
      "loss": 0.4394,
      "step": 259
    },
    {
      "epoch": 0.25,
      "grad_norm": 8.373758316040039,
      "learning_rate": 1.950192307692308e-05,
      "loss": 1.7052,
      "step": 260
    },
    {
      "epoch": 0.25096153846153846,
      "grad_norm": 6.939276218414307,
      "learning_rate": 1.95e-05,
      "loss": 1.5758,
      "step": 261
    },
    {
      "epoch": 0.2519230769230769,
      "grad_norm": 8.522533416748047,
      "learning_rate": 1.9498076923076926e-05,
      "loss": 1.9856,
      "step": 262
    },
    {
      "epoch": 0.2528846153846154,
      "grad_norm": 11.04680061340332,
      "learning_rate": 1.9496153846153845e-05,
      "loss": 1.2172,
      "step": 263
    },
    {
      "epoch": 0.25384615384615383,
      "grad_norm": 7.247354030609131,
      "learning_rate": 1.949423076923077e-05,
      "loss": 1.3961,
      "step": 264
    },
    {
      "epoch": 0.2548076923076923,
      "grad_norm": 9.346338272094727,
      "learning_rate": 1.9492307692307694e-05,
      "loss": 1.3773,
      "step": 265
    },
    {
      "epoch": 0.25576923076923075,
      "grad_norm": 9.262017250061035,
      "learning_rate": 1.9490384615384617e-05,
      "loss": 1.3081,
      "step": 266
    },
    {
      "epoch": 0.2567307692307692,
      "grad_norm": 12.25829029083252,
      "learning_rate": 1.948846153846154e-05,
      "loss": 0.2021,
      "step": 267
    },
    {
      "epoch": 0.25769230769230766,
      "grad_norm": 13.272665977478027,
      "learning_rate": 1.9486538461538462e-05,
      "loss": 0.8939,
      "step": 268
    },
    {
      "epoch": 0.2586538461538462,
      "grad_norm": 12.679719924926758,
      "learning_rate": 1.9484615384615385e-05,
      "loss": 2.4558,
      "step": 269
    },
    {
      "epoch": 0.25961538461538464,
      "grad_norm": 9.569689750671387,
      "learning_rate": 1.948269230769231e-05,
      "loss": 1.3766,
      "step": 270
    },
    {
      "epoch": 0.2605769230769231,
      "grad_norm": 2.2508902549743652,
      "learning_rate": 1.948076923076923e-05,
      "loss": 0.0357,
      "step": 271
    },
    {
      "epoch": 0.26153846153846155,
      "grad_norm": 10.732791900634766,
      "learning_rate": 1.9478846153846157e-05,
      "loss": 1.0011,
      "step": 272
    },
    {
      "epoch": 0.2625,
      "grad_norm": 7.465573310852051,
      "learning_rate": 1.9476923076923076e-05,
      "loss": 2.4539,
      "step": 273
    },
    {
      "epoch": 0.26346153846153847,
      "grad_norm": 37.21251678466797,
      "learning_rate": 1.9475000000000002e-05,
      "loss": 1.3558,
      "step": 274
    },
    {
      "epoch": 0.2644230769230769,
      "grad_norm": 9.293649673461914,
      "learning_rate": 1.9473076923076925e-05,
      "loss": 1.6432,
      "step": 275
    },
    {
      "epoch": 0.2653846153846154,
      "grad_norm": 9.65650463104248,
      "learning_rate": 1.9471153846153848e-05,
      "loss": 2.121,
      "step": 276
    },
    {
      "epoch": 0.26634615384615384,
      "grad_norm": 8.472443580627441,
      "learning_rate": 1.946923076923077e-05,
      "loss": 1.9184,
      "step": 277
    },
    {
      "epoch": 0.2673076923076923,
      "grad_norm": 6.655335903167725,
      "learning_rate": 1.9467307692307694e-05,
      "loss": 1.4769,
      "step": 278
    },
    {
      "epoch": 0.26826923076923076,
      "grad_norm": 7.08319616317749,
      "learning_rate": 1.9465384615384616e-05,
      "loss": 1.418,
      "step": 279
    },
    {
      "epoch": 0.2692307692307692,
      "grad_norm": 6.248092174530029,
      "learning_rate": 1.9463461538461542e-05,
      "loss": 1.7436,
      "step": 280
    },
    {
      "epoch": 0.2701923076923077,
      "grad_norm": 6.3380022048950195,
      "learning_rate": 1.9461538461538462e-05,
      "loss": 2.2908,
      "step": 281
    },
    {
      "epoch": 0.27115384615384613,
      "grad_norm": 10.908920288085938,
      "learning_rate": 1.9459615384615388e-05,
      "loss": 2.0722,
      "step": 282
    },
    {
      "epoch": 0.2721153846153846,
      "grad_norm": 7.022936820983887,
      "learning_rate": 1.9457692307692307e-05,
      "loss": 2.2655,
      "step": 283
    },
    {
      "epoch": 0.27307692307692305,
      "grad_norm": 11.718659400939941,
      "learning_rate": 1.9455769230769234e-05,
      "loss": 1.3927,
      "step": 284
    },
    {
      "epoch": 0.27403846153846156,
      "grad_norm": 8.283849716186523,
      "learning_rate": 1.9453846153846156e-05,
      "loss": 0.3041,
      "step": 285
    },
    {
      "epoch": 0.275,
      "grad_norm": 8.732378005981445,
      "learning_rate": 1.945192307692308e-05,
      "loss": 1.7627,
      "step": 286
    },
    {
      "epoch": 0.2759615384615385,
      "grad_norm": 5.543082237243652,
      "learning_rate": 1.9450000000000002e-05,
      "loss": 1.7205,
      "step": 287
    },
    {
      "epoch": 0.27692307692307694,
      "grad_norm": 8.891966819763184,
      "learning_rate": 1.9448076923076925e-05,
      "loss": 0.8547,
      "step": 288
    },
    {
      "epoch": 0.2778846153846154,
      "grad_norm": 7.2824387550354,
      "learning_rate": 1.9446153846153847e-05,
      "loss": 1.8751,
      "step": 289
    },
    {
      "epoch": 0.27884615384615385,
      "grad_norm": 11.203493118286133,
      "learning_rate": 1.9444230769230774e-05,
      "loss": 1.5788,
      "step": 290
    },
    {
      "epoch": 0.2798076923076923,
      "grad_norm": 10.249676704406738,
      "learning_rate": 1.9442307692307693e-05,
      "loss": 1.909,
      "step": 291
    },
    {
      "epoch": 0.28076923076923077,
      "grad_norm": 15.89976978302002,
      "learning_rate": 1.944038461538462e-05,
      "loss": 2.9177,
      "step": 292
    },
    {
      "epoch": 0.28173076923076923,
      "grad_norm": 6.795293807983398,
      "learning_rate": 1.943846153846154e-05,
      "loss": 1.2061,
      "step": 293
    },
    {
      "epoch": 0.2826923076923077,
      "grad_norm": 6.847870349884033,
      "learning_rate": 1.9436538461538465e-05,
      "loss": 0.2,
      "step": 294
    },
    {
      "epoch": 0.28365384615384615,
      "grad_norm": 8.795882225036621,
      "learning_rate": 1.9434615384615384e-05,
      "loss": 1.4649,
      "step": 295
    },
    {
      "epoch": 0.2846153846153846,
      "grad_norm": 5.985703468322754,
      "learning_rate": 1.943269230769231e-05,
      "loss": 1.5945,
      "step": 296
    },
    {
      "epoch": 0.28557692307692306,
      "grad_norm": 8.961812019348145,
      "learning_rate": 1.9430769230769233e-05,
      "loss": 1.997,
      "step": 297
    },
    {
      "epoch": 0.2865384615384615,
      "grad_norm": 8.216656684875488,
      "learning_rate": 1.9428846153846156e-05,
      "loss": 1.0734,
      "step": 298
    },
    {
      "epoch": 0.2875,
      "grad_norm": 5.722599029541016,
      "learning_rate": 1.942692307692308e-05,
      "loss": 1.8555,
      "step": 299
    },
    {
      "epoch": 0.28846153846153844,
      "grad_norm": 5.465817928314209,
      "learning_rate": 1.9425e-05,
      "loss": 1.4202,
      "step": 300
    },
    {
      "epoch": 0.28942307692307695,
      "grad_norm": 16.56787872314453,
      "learning_rate": 1.9423076923076924e-05,
      "loss": 2.2885,
      "step": 301
    },
    {
      "epoch": 0.2903846153846154,
      "grad_norm": 6.868011474609375,
      "learning_rate": 1.942115384615385e-05,
      "loss": 1.4446,
      "step": 302
    },
    {
      "epoch": 0.29134615384615387,
      "grad_norm": 13.91407299041748,
      "learning_rate": 1.941923076923077e-05,
      "loss": 1.2783,
      "step": 303
    },
    {
      "epoch": 0.2923076923076923,
      "grad_norm": 9.570608139038086,
      "learning_rate": 1.9417307692307696e-05,
      "loss": 1.3872,
      "step": 304
    },
    {
      "epoch": 0.2932692307692308,
      "grad_norm": 8.706635475158691,
      "learning_rate": 1.9415384615384615e-05,
      "loss": 1.0086,
      "step": 305
    },
    {
      "epoch": 0.29423076923076924,
      "grad_norm": 19.67965316772461,
      "learning_rate": 1.941346153846154e-05,
      "loss": 0.6319,
      "step": 306
    },
    {
      "epoch": 0.2951923076923077,
      "grad_norm": 6.4585347175598145,
      "learning_rate": 1.9411538461538464e-05,
      "loss": 1.686,
      "step": 307
    },
    {
      "epoch": 0.29615384615384616,
      "grad_norm": 6.341736316680908,
      "learning_rate": 1.9409615384615387e-05,
      "loss": 2.0525,
      "step": 308
    },
    {
      "epoch": 0.2971153846153846,
      "grad_norm": 8.412687301635742,
      "learning_rate": 1.940769230769231e-05,
      "loss": 1.9553,
      "step": 309
    },
    {
      "epoch": 0.2980769230769231,
      "grad_norm": 13.207808494567871,
      "learning_rate": 1.9405769230769232e-05,
      "loss": 1.5867,
      "step": 310
    },
    {
      "epoch": 0.29903846153846153,
      "grad_norm": 6.716866493225098,
      "learning_rate": 1.9403846153846155e-05,
      "loss": 0.8712,
      "step": 311
    },
    {
      "epoch": 0.3,
      "grad_norm": 10.791629791259766,
      "learning_rate": 1.9401923076923078e-05,
      "loss": 2.1295,
      "step": 312
    },
    {
      "epoch": 0.30096153846153845,
      "grad_norm": 9.892151832580566,
      "learning_rate": 1.94e-05,
      "loss": 2.0481,
      "step": 313
    },
    {
      "epoch": 0.3019230769230769,
      "grad_norm": 7.575615406036377,
      "learning_rate": 1.9398076923076923e-05,
      "loss": 1.4376,
      "step": 314
    },
    {
      "epoch": 0.30288461538461536,
      "grad_norm": 11.069925308227539,
      "learning_rate": 1.9396153846153846e-05,
      "loss": 0.9516,
      "step": 315
    },
    {
      "epoch": 0.3038461538461538,
      "grad_norm": 6.763210296630859,
      "learning_rate": 1.9394230769230772e-05,
      "loss": 1.9847,
      "step": 316
    },
    {
      "epoch": 0.30480769230769234,
      "grad_norm": 8.721841812133789,
      "learning_rate": 1.9392307692307695e-05,
      "loss": 0.3517,
      "step": 317
    },
    {
      "epoch": 0.3057692307692308,
      "grad_norm": 6.040032863616943,
      "learning_rate": 1.9390384615384618e-05,
      "loss": 2.1927,
      "step": 318
    },
    {
      "epoch": 0.30673076923076925,
      "grad_norm": 5.324972629547119,
      "learning_rate": 1.938846153846154e-05,
      "loss": 1.805,
      "step": 319
    },
    {
      "epoch": 0.3076923076923077,
      "grad_norm": 11.048864364624023,
      "learning_rate": 1.9386538461538463e-05,
      "loss": 1.0539,
      "step": 320
    },
    {
      "epoch": 0.30865384615384617,
      "grad_norm": 13.56892204284668,
      "learning_rate": 1.9384615384615386e-05,
      "loss": 1.6762,
      "step": 321
    },
    {
      "epoch": 0.3096153846153846,
      "grad_norm": 6.886023044586182,
      "learning_rate": 1.938269230769231e-05,
      "loss": 1.6341,
      "step": 322
    },
    {
      "epoch": 0.3105769230769231,
      "grad_norm": 9.37547779083252,
      "learning_rate": 1.938076923076923e-05,
      "loss": 1.4993,
      "step": 323
    },
    {
      "epoch": 0.31153846153846154,
      "grad_norm": 8.416069984436035,
      "learning_rate": 1.9378846153846154e-05,
      "loss": 2.2758,
      "step": 324
    },
    {
      "epoch": 0.3125,
      "grad_norm": 8.750123977661133,
      "learning_rate": 1.9376923076923077e-05,
      "loss": 1.6882,
      "step": 325
    },
    {
      "epoch": 0.31346153846153846,
      "grad_norm": 15.13723373413086,
      "learning_rate": 1.9375e-05,
      "loss": 1.6616,
      "step": 326
    },
    {
      "epoch": 0.3144230769230769,
      "grad_norm": 12.508066177368164,
      "learning_rate": 1.9373076923076926e-05,
      "loss": 1.7052,
      "step": 327
    },
    {
      "epoch": 0.3153846153846154,
      "grad_norm": 7.505585670471191,
      "learning_rate": 1.937115384615385e-05,
      "loss": 1.4575,
      "step": 328
    },
    {
      "epoch": 0.31634615384615383,
      "grad_norm": 7.247694969177246,
      "learning_rate": 1.936923076923077e-05,
      "loss": 1.2614,
      "step": 329
    },
    {
      "epoch": 0.3173076923076923,
      "grad_norm": 6.234418869018555,
      "learning_rate": 1.9367307692307694e-05,
      "loss": 0.764,
      "step": 330
    },
    {
      "epoch": 0.31826923076923075,
      "grad_norm": 7.702490329742432,
      "learning_rate": 1.9365384615384617e-05,
      "loss": 1.3665,
      "step": 331
    },
    {
      "epoch": 0.3192307692307692,
      "grad_norm": 5.812324523925781,
      "learning_rate": 1.936346153846154e-05,
      "loss": 1.7878,
      "step": 332
    },
    {
      "epoch": 0.32019230769230766,
      "grad_norm": 6.621466159820557,
      "learning_rate": 1.9361538461538463e-05,
      "loss": 0.9435,
      "step": 333
    },
    {
      "epoch": 0.3211538461538462,
      "grad_norm": 7.950897216796875,
      "learning_rate": 1.9359615384615386e-05,
      "loss": 1.8313,
      "step": 334
    },
    {
      "epoch": 0.32211538461538464,
      "grad_norm": 23.990236282348633,
      "learning_rate": 1.935769230769231e-05,
      "loss": 1.1571,
      "step": 335
    },
    {
      "epoch": 0.3230769230769231,
      "grad_norm": 13.466384887695312,
      "learning_rate": 1.935576923076923e-05,
      "loss": 1.4268,
      "step": 336
    },
    {
      "epoch": 0.32403846153846155,
      "grad_norm": 5.678049564361572,
      "learning_rate": 1.9353846153846157e-05,
      "loss": 2.0235,
      "step": 337
    },
    {
      "epoch": 0.325,
      "grad_norm": 6.943883895874023,
      "learning_rate": 1.9351923076923077e-05,
      "loss": 1.933,
      "step": 338
    },
    {
      "epoch": 0.32596153846153847,
      "grad_norm": 64.1224136352539,
      "learning_rate": 1.9350000000000003e-05,
      "loss": 1.2743,
      "step": 339
    },
    {
      "epoch": 0.3269230769230769,
      "grad_norm": 7.341131210327148,
      "learning_rate": 1.9348076923076926e-05,
      "loss": 1.7081,
      "step": 340
    },
    {
      "epoch": 0.3278846153846154,
      "grad_norm": 7.1841630935668945,
      "learning_rate": 1.934615384615385e-05,
      "loss": 2.0638,
      "step": 341
    },
    {
      "epoch": 0.32884615384615384,
      "grad_norm": 3.979182004928589,
      "learning_rate": 1.934423076923077e-05,
      "loss": 0.2144,
      "step": 342
    },
    {
      "epoch": 0.3298076923076923,
      "grad_norm": 6.921886920928955,
      "learning_rate": 1.9342307692307694e-05,
      "loss": 1.5779,
      "step": 343
    },
    {
      "epoch": 0.33076923076923076,
      "grad_norm": 5.901214122772217,
      "learning_rate": 1.9340384615384617e-05,
      "loss": 2.1929,
      "step": 344
    },
    {
      "epoch": 0.3317307692307692,
      "grad_norm": 6.753857612609863,
      "learning_rate": 1.933846153846154e-05,
      "loss": 1.0348,
      "step": 345
    },
    {
      "epoch": 0.3326923076923077,
      "grad_norm": 8.36176586151123,
      "learning_rate": 1.9336538461538462e-05,
      "loss": 1.6021,
      "step": 346
    },
    {
      "epoch": 0.33365384615384613,
      "grad_norm": 7.11743688583374,
      "learning_rate": 1.933461538461539e-05,
      "loss": 1.3249,
      "step": 347
    },
    {
      "epoch": 0.3346153846153846,
      "grad_norm": 11.738651275634766,
      "learning_rate": 1.9332692307692308e-05,
      "loss": 1.1032,
      "step": 348
    },
    {
      "epoch": 0.33557692307692305,
      "grad_norm": 10.136805534362793,
      "learning_rate": 1.9330769230769234e-05,
      "loss": 0.855,
      "step": 349
    },
    {
      "epoch": 0.33653846153846156,
      "grad_norm": 9.308761596679688,
      "learning_rate": 1.9328846153846153e-05,
      "loss": 1.2847,
      "step": 350
    },
    {
      "epoch": 0.3375,
      "grad_norm": 6.566943168640137,
      "learning_rate": 1.932692307692308e-05,
      "loss": 2.2139,
      "step": 351
    },
    {
      "epoch": 0.3384615384615385,
      "grad_norm": 9.016860008239746,
      "learning_rate": 1.9325000000000002e-05,
      "loss": 1.6293,
      "step": 352
    },
    {
      "epoch": 0.33942307692307694,
      "grad_norm": 13.026694297790527,
      "learning_rate": 1.9323076923076925e-05,
      "loss": 2.1818,
      "step": 353
    },
    {
      "epoch": 0.3403846153846154,
      "grad_norm": 10.768171310424805,
      "learning_rate": 1.9321153846153848e-05,
      "loss": 1.7667,
      "step": 354
    },
    {
      "epoch": 0.34134615384615385,
      "grad_norm": 8.78372573852539,
      "learning_rate": 1.931923076923077e-05,
      "loss": 1.2631,
      "step": 355
    },
    {
      "epoch": 0.3423076923076923,
      "grad_norm": 8.184530258178711,
      "learning_rate": 1.9317307692307693e-05,
      "loss": 1.104,
      "step": 356
    },
    {
      "epoch": 0.34326923076923077,
      "grad_norm": 7.652422904968262,
      "learning_rate": 1.931538461538462e-05,
      "loss": 1.7993,
      "step": 357
    },
    {
      "epoch": 0.34423076923076923,
      "grad_norm": 8.053833961486816,
      "learning_rate": 1.931346153846154e-05,
      "loss": 1.9207,
      "step": 358
    },
    {
      "epoch": 0.3451923076923077,
      "grad_norm": 7.388314723968506,
      "learning_rate": 1.9311538461538465e-05,
      "loss": 1.5408,
      "step": 359
    },
    {
      "epoch": 0.34615384615384615,
      "grad_norm": 6.483381748199463,
      "learning_rate": 1.9309615384615384e-05,
      "loss": 2.0587,
      "step": 360
    },
    {
      "epoch": 0.3471153846153846,
      "grad_norm": 10.740474700927734,
      "learning_rate": 1.930769230769231e-05,
      "loss": 1.2673,
      "step": 361
    },
    {
      "epoch": 0.34807692307692306,
      "grad_norm": 8.530540466308594,
      "learning_rate": 1.930576923076923e-05,
      "loss": 1.2493,
      "step": 362
    },
    {
      "epoch": 0.3490384615384615,
      "grad_norm": 9.302946090698242,
      "learning_rate": 1.9303846153846156e-05,
      "loss": 1.7391,
      "step": 363
    },
    {
      "epoch": 0.35,
      "grad_norm": 7.593328952789307,
      "learning_rate": 1.930192307692308e-05,
      "loss": 2.0997,
      "step": 364
    },
    {
      "epoch": 0.35096153846153844,
      "grad_norm": 7.8033366203308105,
      "learning_rate": 1.93e-05,
      "loss": 1.6087,
      "step": 365
    },
    {
      "epoch": 0.35192307692307695,
      "grad_norm": 6.569047927856445,
      "learning_rate": 1.9298076923076924e-05,
      "loss": 1.6852,
      "step": 366
    },
    {
      "epoch": 0.3528846153846154,
      "grad_norm": 6.15145206451416,
      "learning_rate": 1.9296153846153847e-05,
      "loss": 1.6195,
      "step": 367
    },
    {
      "epoch": 0.35384615384615387,
      "grad_norm": 15.8932466506958,
      "learning_rate": 1.929423076923077e-05,
      "loss": 1.8548,
      "step": 368
    },
    {
      "epoch": 0.3548076923076923,
      "grad_norm": 11.13694953918457,
      "learning_rate": 1.9292307692307696e-05,
      "loss": 0.7147,
      "step": 369
    },
    {
      "epoch": 0.3557692307692308,
      "grad_norm": 6.917645454406738,
      "learning_rate": 1.9290384615384615e-05,
      "loss": 1.5981,
      "step": 370
    },
    {
      "epoch": 0.35673076923076924,
      "grad_norm": 10.177809715270996,
      "learning_rate": 1.928846153846154e-05,
      "loss": 1.8861,
      "step": 371
    },
    {
      "epoch": 0.3576923076923077,
      "grad_norm": 8.009645462036133,
      "learning_rate": 1.928653846153846e-05,
      "loss": 1.9186,
      "step": 372
    },
    {
      "epoch": 0.35865384615384616,
      "grad_norm": 9.513726234436035,
      "learning_rate": 1.9284615384615387e-05,
      "loss": 1.6752,
      "step": 373
    },
    {
      "epoch": 0.3596153846153846,
      "grad_norm": 12.127252578735352,
      "learning_rate": 1.928269230769231e-05,
      "loss": 2.1325,
      "step": 374
    },
    {
      "epoch": 0.3605769230769231,
      "grad_norm": 9.597378730773926,
      "learning_rate": 1.9280769230769233e-05,
      "loss": 1.2984,
      "step": 375
    },
    {
      "epoch": 0.36153846153846153,
      "grad_norm": 5.215967178344727,
      "learning_rate": 1.9278846153846155e-05,
      "loss": 1.8045,
      "step": 376
    },
    {
      "epoch": 0.3625,
      "grad_norm": 8.59814167022705,
      "learning_rate": 1.9276923076923078e-05,
      "loss": 0.9433,
      "step": 377
    },
    {
      "epoch": 0.36346153846153845,
      "grad_norm": 8.963176727294922,
      "learning_rate": 1.9275e-05,
      "loss": 1.3186,
      "step": 378
    },
    {
      "epoch": 0.3644230769230769,
      "grad_norm": 9.500654220581055,
      "learning_rate": 1.9273076923076927e-05,
      "loss": 0.2532,
      "step": 379
    },
    {
      "epoch": 0.36538461538461536,
      "grad_norm": 18.611051559448242,
      "learning_rate": 1.9271153846153846e-05,
      "loss": 1.0577,
      "step": 380
    },
    {
      "epoch": 0.3663461538461538,
      "grad_norm": 7.722896099090576,
      "learning_rate": 1.9269230769230773e-05,
      "loss": 1.3156,
      "step": 381
    },
    {
      "epoch": 0.36730769230769234,
      "grad_norm": 5.965081214904785,
      "learning_rate": 1.9267307692307692e-05,
      "loss": 2.0124,
      "step": 382
    },
    {
      "epoch": 0.3682692307692308,
      "grad_norm": 10.13461971282959,
      "learning_rate": 1.9265384615384618e-05,
      "loss": 0.3401,
      "step": 383
    },
    {
      "epoch": 0.36923076923076925,
      "grad_norm": 5.624252796173096,
      "learning_rate": 1.926346153846154e-05,
      "loss": 1.7079,
      "step": 384
    },
    {
      "epoch": 0.3701923076923077,
      "grad_norm": 6.8658905029296875,
      "learning_rate": 1.9261538461538464e-05,
      "loss": 1.7612,
      "step": 385
    },
    {
      "epoch": 0.37115384615384617,
      "grad_norm": 5.656351566314697,
      "learning_rate": 1.9259615384615386e-05,
      "loss": 1.0831,
      "step": 386
    },
    {
      "epoch": 0.3721153846153846,
      "grad_norm": 11.365379333496094,
      "learning_rate": 1.925769230769231e-05,
      "loss": 1.5794,
      "step": 387
    },
    {
      "epoch": 0.3730769230769231,
      "grad_norm": 9.962361335754395,
      "learning_rate": 1.9255769230769232e-05,
      "loss": 1.708,
      "step": 388
    },
    {
      "epoch": 0.37403846153846154,
      "grad_norm": 8.652795791625977,
      "learning_rate": 1.9253846153846155e-05,
      "loss": 1.8505,
      "step": 389
    },
    {
      "epoch": 0.375,
      "grad_norm": 4.377523422241211,
      "learning_rate": 1.9251923076923078e-05,
      "loss": 1.8912,
      "step": 390
    },
    {
      "epoch": 0.37596153846153846,
      "grad_norm": 8.316871643066406,
      "learning_rate": 1.925e-05,
      "loss": 1.9045,
      "step": 391
    },
    {
      "epoch": 0.3769230769230769,
      "grad_norm": 8.846223831176758,
      "learning_rate": 1.9248076923076923e-05,
      "loss": 1.6412,
      "step": 392
    },
    {
      "epoch": 0.3778846153846154,
      "grad_norm": 7.450632095336914,
      "learning_rate": 1.924615384615385e-05,
      "loss": 1.6694,
      "step": 393
    },
    {
      "epoch": 0.37884615384615383,
      "grad_norm": 6.090971946716309,
      "learning_rate": 1.9244230769230772e-05,
      "loss": 2.2518,
      "step": 394
    },
    {
      "epoch": 0.3798076923076923,
      "grad_norm": 8.719858169555664,
      "learning_rate": 1.9242307692307695e-05,
      "loss": 1.6708,
      "step": 395
    },
    {
      "epoch": 0.38076923076923075,
      "grad_norm": 10.843974113464355,
      "learning_rate": 1.9240384615384618e-05,
      "loss": 1.3562,
      "step": 396
    },
    {
      "epoch": 0.3817307692307692,
      "grad_norm": 7.976405143737793,
      "learning_rate": 1.923846153846154e-05,
      "loss": 1.7715,
      "step": 397
    },
    {
      "epoch": 0.38269230769230766,
      "grad_norm": 7.556628704071045,
      "learning_rate": 1.9236538461538463e-05,
      "loss": 1.6708,
      "step": 398
    },
    {
      "epoch": 0.3836538461538462,
      "grad_norm": 12.287881851196289,
      "learning_rate": 1.9234615384615386e-05,
      "loss": 1.7072,
      "step": 399
    },
    {
      "epoch": 0.38461538461538464,
      "grad_norm": 7.216205596923828,
      "learning_rate": 1.923269230769231e-05,
      "loss": 1.5028,
      "step": 400
    },
    {
      "epoch": 0.3855769230769231,
      "grad_norm": 5.946415901184082,
      "learning_rate": 1.923076923076923e-05,
      "loss": 1.9092,
      "step": 401
    },
    {
      "epoch": 0.38653846153846155,
      "grad_norm": 7.655370235443115,
      "learning_rate": 1.9228846153846154e-05,
      "loss": 1.0879,
      "step": 402
    },
    {
      "epoch": 0.3875,
      "grad_norm": 9.506278991699219,
      "learning_rate": 1.9226923076923077e-05,
      "loss": 1.0458,
      "step": 403
    },
    {
      "epoch": 0.38846153846153847,
      "grad_norm": 8.12130355834961,
      "learning_rate": 1.9225000000000003e-05,
      "loss": 2.0096,
      "step": 404
    },
    {
      "epoch": 0.3894230769230769,
      "grad_norm": 29.10354995727539,
      "learning_rate": 1.9223076923076926e-05,
      "loss": 2.0455,
      "step": 405
    },
    {
      "epoch": 0.3903846153846154,
      "grad_norm": 5.596901893615723,
      "learning_rate": 1.922115384615385e-05,
      "loss": 1.5668,
      "step": 406
    },
    {
      "epoch": 0.39134615384615384,
      "grad_norm": 6.851656436920166,
      "learning_rate": 1.921923076923077e-05,
      "loss": 0.5599,
      "step": 407
    },
    {
      "epoch": 0.3923076923076923,
      "grad_norm": 8.384193420410156,
      "learning_rate": 1.9217307692307694e-05,
      "loss": 1.6825,
      "step": 408
    },
    {
      "epoch": 0.39326923076923076,
      "grad_norm": 6.057218074798584,
      "learning_rate": 1.9215384615384617e-05,
      "loss": 1.4396,
      "step": 409
    },
    {
      "epoch": 0.3942307692307692,
      "grad_norm": 6.769307613372803,
      "learning_rate": 1.921346153846154e-05,
      "loss": 1.7782,
      "step": 410
    },
    {
      "epoch": 0.3951923076923077,
      "grad_norm": 10.667290687561035,
      "learning_rate": 1.9211538461538463e-05,
      "loss": 1.0916,
      "step": 411
    },
    {
      "epoch": 0.39615384615384613,
      "grad_norm": 6.854666233062744,
      "learning_rate": 1.9209615384615385e-05,
      "loss": 1.3845,
      "step": 412
    },
    {
      "epoch": 0.3971153846153846,
      "grad_norm": 9.257776260375977,
      "learning_rate": 1.9207692307692308e-05,
      "loss": 1.5759,
      "step": 413
    },
    {
      "epoch": 0.39807692307692305,
      "grad_norm": 7.703135967254639,
      "learning_rate": 1.9205769230769234e-05,
      "loss": 1.6514,
      "step": 414
    },
    {
      "epoch": 0.39903846153846156,
      "grad_norm": 18.885496139526367,
      "learning_rate": 1.9203846153846154e-05,
      "loss": 1.7118,
      "step": 415
    },
    {
      "epoch": 0.4,
      "grad_norm": 6.321294784545898,
      "learning_rate": 1.920192307692308e-05,
      "loss": 1.2638,
      "step": 416
    },
    {
      "epoch": 0.4009615384615385,
      "grad_norm": 5.334461212158203,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 2.1973,
      "step": 417
    },
    {
      "epoch": 0.40192307692307694,
      "grad_norm": 12.563841819763184,
      "learning_rate": 1.9198076923076925e-05,
      "loss": 1.0537,
      "step": 418
    },
    {
      "epoch": 0.4028846153846154,
      "grad_norm": 7.131688117980957,
      "learning_rate": 1.9196153846153848e-05,
      "loss": 1.725,
      "step": 419
    },
    {
      "epoch": 0.40384615384615385,
      "grad_norm": 9.133442878723145,
      "learning_rate": 1.919423076923077e-05,
      "loss": 2.0632,
      "step": 420
    },
    {
      "epoch": 0.4048076923076923,
      "grad_norm": 7.578608512878418,
      "learning_rate": 1.9192307692307694e-05,
      "loss": 1.7279,
      "step": 421
    },
    {
      "epoch": 0.40576923076923077,
      "grad_norm": 10.581222534179688,
      "learning_rate": 1.9190384615384616e-05,
      "loss": 1.905,
      "step": 422
    },
    {
      "epoch": 0.40673076923076923,
      "grad_norm": 6.886626243591309,
      "learning_rate": 1.918846153846154e-05,
      "loss": 2.0385,
      "step": 423
    },
    {
      "epoch": 0.4076923076923077,
      "grad_norm": 11.168404579162598,
      "learning_rate": 1.9186538461538462e-05,
      "loss": 2.035,
      "step": 424
    },
    {
      "epoch": 0.40865384615384615,
      "grad_norm": 6.782038688659668,
      "learning_rate": 1.9184615384615385e-05,
      "loss": 1.6744,
      "step": 425
    },
    {
      "epoch": 0.4096153846153846,
      "grad_norm": 7.47141170501709,
      "learning_rate": 1.918269230769231e-05,
      "loss": 1.7215,
      "step": 426
    },
    {
      "epoch": 0.41057692307692306,
      "grad_norm": 19.571733474731445,
      "learning_rate": 1.918076923076923e-05,
      "loss": 0.2365,
      "step": 427
    },
    {
      "epoch": 0.4115384615384615,
      "grad_norm": 9.683822631835938,
      "learning_rate": 1.9178846153846156e-05,
      "loss": 1.0222,
      "step": 428
    },
    {
      "epoch": 0.4125,
      "grad_norm": 6.104222297668457,
      "learning_rate": 1.9176923076923076e-05,
      "loss": 2.04,
      "step": 429
    },
    {
      "epoch": 0.41346153846153844,
      "grad_norm": 5.39920711517334,
      "learning_rate": 1.9175000000000002e-05,
      "loss": 2.1937,
      "step": 430
    },
    {
      "epoch": 0.41442307692307695,
      "grad_norm": 6.914605140686035,
      "learning_rate": 1.9173076923076925e-05,
      "loss": 1.424,
      "step": 431
    },
    {
      "epoch": 0.4153846153846154,
      "grad_norm": 23.456233978271484,
      "learning_rate": 1.9171153846153847e-05,
      "loss": 1.8274,
      "step": 432
    },
    {
      "epoch": 0.41634615384615387,
      "grad_norm": 55.4368896484375,
      "learning_rate": 1.916923076923077e-05,
      "loss": 0.6913,
      "step": 433
    },
    {
      "epoch": 0.4173076923076923,
      "grad_norm": 8.16163158416748,
      "learning_rate": 1.9167307692307693e-05,
      "loss": 2.1031,
      "step": 434
    },
    {
      "epoch": 0.4182692307692308,
      "grad_norm": 7.080775737762451,
      "learning_rate": 1.9165384615384616e-05,
      "loss": 1.8378,
      "step": 435
    },
    {
      "epoch": 0.41923076923076924,
      "grad_norm": 7.192737579345703,
      "learning_rate": 1.9163461538461542e-05,
      "loss": 0.8504,
      "step": 436
    },
    {
      "epoch": 0.4201923076923077,
      "grad_norm": 12.400999069213867,
      "learning_rate": 1.916153846153846e-05,
      "loss": 0.991,
      "step": 437
    },
    {
      "epoch": 0.42115384615384616,
      "grad_norm": 23.01675796508789,
      "learning_rate": 1.9159615384615387e-05,
      "loss": 0.9337,
      "step": 438
    },
    {
      "epoch": 0.4221153846153846,
      "grad_norm": 6.9954514503479,
      "learning_rate": 1.9157692307692307e-05,
      "loss": 1.6497,
      "step": 439
    },
    {
      "epoch": 0.4230769230769231,
      "grad_norm": 8.186549186706543,
      "learning_rate": 1.9155769230769233e-05,
      "loss": 1.1995,
      "step": 440
    },
    {
      "epoch": 0.42403846153846153,
      "grad_norm": 7.715425491333008,
      "learning_rate": 1.9153846153846156e-05,
      "loss": 1.2375,
      "step": 441
    },
    {
      "epoch": 0.425,
      "grad_norm": 6.923596382141113,
      "learning_rate": 1.915192307692308e-05,
      "loss": 1.8621,
      "step": 442
    },
    {
      "epoch": 0.42596153846153845,
      "grad_norm": 6.528982162475586,
      "learning_rate": 1.915e-05,
      "loss": 1.8325,
      "step": 443
    },
    {
      "epoch": 0.4269230769230769,
      "grad_norm": 5.833257675170898,
      "learning_rate": 1.9148076923076924e-05,
      "loss": 1.51,
      "step": 444
    },
    {
      "epoch": 0.42788461538461536,
      "grad_norm": 8.107213020324707,
      "learning_rate": 1.9146153846153847e-05,
      "loss": 1.2846,
      "step": 445
    },
    {
      "epoch": 0.4288461538461538,
      "grad_norm": 5.43205451965332,
      "learning_rate": 1.9144230769230773e-05,
      "loss": 1.6282,
      "step": 446
    },
    {
      "epoch": 0.42980769230769234,
      "grad_norm": 5.560505390167236,
      "learning_rate": 1.9142307692307692e-05,
      "loss": 1.8886,
      "step": 447
    },
    {
      "epoch": 0.4307692307692308,
      "grad_norm": 6.771823406219482,
      "learning_rate": 1.914038461538462e-05,
      "loss": 1.4604,
      "step": 448
    },
    {
      "epoch": 0.43173076923076925,
      "grad_norm": 6.742474555969238,
      "learning_rate": 1.9138461538461538e-05,
      "loss": 1.5751,
      "step": 449
    },
    {
      "epoch": 0.4326923076923077,
      "grad_norm": 5.818939208984375,
      "learning_rate": 1.9136538461538464e-05,
      "loss": 1.7454,
      "step": 450
    },
    {
      "epoch": 0.43365384615384617,
      "grad_norm": 10.404285430908203,
      "learning_rate": 1.9134615384615387e-05,
      "loss": 2.1704,
      "step": 451
    },
    {
      "epoch": 0.4346153846153846,
      "grad_norm": 11.228034019470215,
      "learning_rate": 1.913269230769231e-05,
      "loss": 1.8298,
      "step": 452
    },
    {
      "epoch": 0.4355769230769231,
      "grad_norm": 12.730463027954102,
      "learning_rate": 1.9130769230769232e-05,
      "loss": 1.8668,
      "step": 453
    },
    {
      "epoch": 0.43653846153846154,
      "grad_norm": 9.365056991577148,
      "learning_rate": 1.9128846153846155e-05,
      "loss": 0.1962,
      "step": 454
    },
    {
      "epoch": 0.4375,
      "grad_norm": 5.775267601013184,
      "learning_rate": 1.9126923076923078e-05,
      "loss": 2.33,
      "step": 455
    },
    {
      "epoch": 0.43846153846153846,
      "grad_norm": 8.86157512664795,
      "learning_rate": 1.9125000000000004e-05,
      "loss": 0.5247,
      "step": 456
    },
    {
      "epoch": 0.4394230769230769,
      "grad_norm": 6.28350830078125,
      "learning_rate": 1.9123076923076923e-05,
      "loss": 1.4938,
      "step": 457
    },
    {
      "epoch": 0.4403846153846154,
      "grad_norm": 9.797233581542969,
      "learning_rate": 1.912115384615385e-05,
      "loss": 1.8177,
      "step": 458
    },
    {
      "epoch": 0.44134615384615383,
      "grad_norm": 12.293021202087402,
      "learning_rate": 1.911923076923077e-05,
      "loss": 1.3996,
      "step": 459
    },
    {
      "epoch": 0.4423076923076923,
      "grad_norm": 10.172860145568848,
      "learning_rate": 1.9117307692307695e-05,
      "loss": 1.9892,
      "step": 460
    },
    {
      "epoch": 0.44326923076923075,
      "grad_norm": 6.460251331329346,
      "learning_rate": 1.9115384615384618e-05,
      "loss": 0.8119,
      "step": 461
    },
    {
      "epoch": 0.4442307692307692,
      "grad_norm": 6.0309858322143555,
      "learning_rate": 1.911346153846154e-05,
      "loss": 1.2483,
      "step": 462
    },
    {
      "epoch": 0.44519230769230766,
      "grad_norm": 10.309659957885742,
      "learning_rate": 1.9111538461538463e-05,
      "loss": 2.4265,
      "step": 463
    },
    {
      "epoch": 0.4461538461538462,
      "grad_norm": 7.785675048828125,
      "learning_rate": 1.9109615384615386e-05,
      "loss": 0.968,
      "step": 464
    },
    {
      "epoch": 0.44711538461538464,
      "grad_norm": 6.710755825042725,
      "learning_rate": 1.910769230769231e-05,
      "loss": 2.3983,
      "step": 465
    },
    {
      "epoch": 0.4480769230769231,
      "grad_norm": 8.827452659606934,
      "learning_rate": 1.9105769230769232e-05,
      "loss": 0.5054,
      "step": 466
    },
    {
      "epoch": 0.44903846153846155,
      "grad_norm": 18.81873893737793,
      "learning_rate": 1.9103846153846155e-05,
      "loss": 0.7135,
      "step": 467
    },
    {
      "epoch": 0.45,
      "grad_norm": 5.8997039794921875,
      "learning_rate": 1.9101923076923077e-05,
      "loss": 1.6174,
      "step": 468
    },
    {
      "epoch": 0.45096153846153847,
      "grad_norm": 14.387227058410645,
      "learning_rate": 1.91e-05,
      "loss": 1.9514,
      "step": 469
    },
    {
      "epoch": 0.4519230769230769,
      "grad_norm": 8.487875938415527,
      "learning_rate": 1.9098076923076926e-05,
      "loss": 0.2052,
      "step": 470
    },
    {
      "epoch": 0.4528846153846154,
      "grad_norm": 9.74959659576416,
      "learning_rate": 1.909615384615385e-05,
      "loss": 1.7978,
      "step": 471
    },
    {
      "epoch": 0.45384615384615384,
      "grad_norm": 15.84633731842041,
      "learning_rate": 1.9094230769230772e-05,
      "loss": 2.0764,
      "step": 472
    },
    {
      "epoch": 0.4548076923076923,
      "grad_norm": 10.847352027893066,
      "learning_rate": 1.9092307692307695e-05,
      "loss": 1.7968,
      "step": 473
    },
    {
      "epoch": 0.45576923076923076,
      "grad_norm": 11.521749496459961,
      "learning_rate": 1.9090384615384617e-05,
      "loss": 1.6732,
      "step": 474
    },
    {
      "epoch": 0.4567307692307692,
      "grad_norm": 7.5276198387146,
      "learning_rate": 1.908846153846154e-05,
      "loss": 0.2259,
      "step": 475
    },
    {
      "epoch": 0.4576923076923077,
      "grad_norm": 11.159016609191895,
      "learning_rate": 1.9086538461538463e-05,
      "loss": 1.8588,
      "step": 476
    },
    {
      "epoch": 0.45865384615384613,
      "grad_norm": 6.9200944900512695,
      "learning_rate": 1.9084615384615386e-05,
      "loss": 0.1582,
      "step": 477
    },
    {
      "epoch": 0.4596153846153846,
      "grad_norm": 11.432770729064941,
      "learning_rate": 1.908269230769231e-05,
      "loss": 1.7569,
      "step": 478
    },
    {
      "epoch": 0.46057692307692305,
      "grad_norm": 6.56742525100708,
      "learning_rate": 1.908076923076923e-05,
      "loss": 2.0545,
      "step": 479
    },
    {
      "epoch": 0.46153846153846156,
      "grad_norm": 6.191407680511475,
      "learning_rate": 1.9078846153846154e-05,
      "loss": 1.0659,
      "step": 480
    },
    {
      "epoch": 0.4625,
      "grad_norm": 8.507264137268066,
      "learning_rate": 1.907692307692308e-05,
      "loss": 0.6524,
      "step": 481
    },
    {
      "epoch": 0.4634615384615385,
      "grad_norm": 6.973827362060547,
      "learning_rate": 1.9075000000000003e-05,
      "loss": 1.9151,
      "step": 482
    },
    {
      "epoch": 0.46442307692307694,
      "grad_norm": 5.29314661026001,
      "learning_rate": 1.9073076923076926e-05,
      "loss": 1.2919,
      "step": 483
    },
    {
      "epoch": 0.4653846153846154,
      "grad_norm": 7.807585716247559,
      "learning_rate": 1.907115384615385e-05,
      "loss": 1.9959,
      "step": 484
    },
    {
      "epoch": 0.46634615384615385,
      "grad_norm": 6.958465099334717,
      "learning_rate": 1.906923076923077e-05,
      "loss": 1.8801,
      "step": 485
    },
    {
      "epoch": 0.4673076923076923,
      "grad_norm": 6.45116662979126,
      "learning_rate": 1.9067307692307694e-05,
      "loss": 1.7992,
      "step": 486
    },
    {
      "epoch": 0.46826923076923077,
      "grad_norm": 5.480909824371338,
      "learning_rate": 1.9065384615384617e-05,
      "loss": 1.032,
      "step": 487
    },
    {
      "epoch": 0.46923076923076923,
      "grad_norm": 11.347328186035156,
      "learning_rate": 1.906346153846154e-05,
      "loss": 1.6802,
      "step": 488
    },
    {
      "epoch": 0.4701923076923077,
      "grad_norm": 7.222335338592529,
      "learning_rate": 1.9061538461538462e-05,
      "loss": 2.2272,
      "step": 489
    },
    {
      "epoch": 0.47115384615384615,
      "grad_norm": 7.301717758178711,
      "learning_rate": 1.9059615384615385e-05,
      "loss": 0.9187,
      "step": 490
    },
    {
      "epoch": 0.4721153846153846,
      "grad_norm": 4.797070503234863,
      "learning_rate": 1.9057692307692308e-05,
      "loss": 1.4598,
      "step": 491
    },
    {
      "epoch": 0.47307692307692306,
      "grad_norm": 7.655356407165527,
      "learning_rate": 1.905576923076923e-05,
      "loss": 2.0674,
      "step": 492
    },
    {
      "epoch": 0.4740384615384615,
      "grad_norm": 7.862003803253174,
      "learning_rate": 1.9053846153846157e-05,
      "loss": 2.0232,
      "step": 493
    },
    {
      "epoch": 0.475,
      "grad_norm": 8.312631607055664,
      "learning_rate": 1.905192307692308e-05,
      "loss": 2.032,
      "step": 494
    },
    {
      "epoch": 0.47596153846153844,
      "grad_norm": 7.998780250549316,
      "learning_rate": 1.9050000000000002e-05,
      "loss": 1.1284,
      "step": 495
    },
    {
      "epoch": 0.47692307692307695,
      "grad_norm": 5.7789435386657715,
      "learning_rate": 1.9048076923076925e-05,
      "loss": 1.7981,
      "step": 496
    },
    {
      "epoch": 0.4778846153846154,
      "grad_norm": 5.571367263793945,
      "learning_rate": 1.9046153846153848e-05,
      "loss": 2.2256,
      "step": 497
    },
    {
      "epoch": 0.47884615384615387,
      "grad_norm": 8.026618003845215,
      "learning_rate": 1.904423076923077e-05,
      "loss": 1.826,
      "step": 498
    },
    {
      "epoch": 0.4798076923076923,
      "grad_norm": 6.784913539886475,
      "learning_rate": 1.9042307692307693e-05,
      "loss": 1.591,
      "step": 499
    },
    {
      "epoch": 0.4807692307692308,
      "grad_norm": 9.978522300720215,
      "learning_rate": 1.9040384615384616e-05,
      "loss": 1.5936,
      "step": 500
    },
    {
      "epoch": 0.48173076923076924,
      "grad_norm": 6.044308662414551,
      "learning_rate": 1.903846153846154e-05,
      "loss": 1.5877,
      "step": 501
    },
    {
      "epoch": 0.4826923076923077,
      "grad_norm": 6.374733924865723,
      "learning_rate": 1.903653846153846e-05,
      "loss": 1.6138,
      "step": 502
    },
    {
      "epoch": 0.48365384615384616,
      "grad_norm": 8.857014656066895,
      "learning_rate": 1.9034615384615388e-05,
      "loss": 0.8994,
      "step": 503
    },
    {
      "epoch": 0.4846153846153846,
      "grad_norm": 7.032991409301758,
      "learning_rate": 1.9032692307692307e-05,
      "loss": 1.3895,
      "step": 504
    },
    {
      "epoch": 0.4855769230769231,
      "grad_norm": 6.318833827972412,
      "learning_rate": 1.9030769230769233e-05,
      "loss": 1.9247,
      "step": 505
    },
    {
      "epoch": 0.48653846153846153,
      "grad_norm": 7.594770908355713,
      "learning_rate": 1.9028846153846153e-05,
      "loss": 0.196,
      "step": 506
    },
    {
      "epoch": 0.4875,
      "grad_norm": 10.257827758789062,
      "learning_rate": 1.902692307692308e-05,
      "loss": 1.3446,
      "step": 507
    },
    {
      "epoch": 0.48846153846153845,
      "grad_norm": 8.893494606018066,
      "learning_rate": 1.9025e-05,
      "loss": 1.6973,
      "step": 508
    },
    {
      "epoch": 0.4894230769230769,
      "grad_norm": 15.162242889404297,
      "learning_rate": 1.9023076923076924e-05,
      "loss": 1.0074,
      "step": 509
    },
    {
      "epoch": 0.49038461538461536,
      "grad_norm": 5.920355319976807,
      "learning_rate": 1.9021153846153847e-05,
      "loss": 1.8339,
      "step": 510
    },
    {
      "epoch": 0.4913461538461538,
      "grad_norm": 4.795101642608643,
      "learning_rate": 1.901923076923077e-05,
      "loss": 1.9533,
      "step": 511
    },
    {
      "epoch": 0.49230769230769234,
      "grad_norm": 7.777580738067627,
      "learning_rate": 1.9017307692307693e-05,
      "loss": 0.6691,
      "step": 512
    },
    {
      "epoch": 0.4932692307692308,
      "grad_norm": 7.599001407623291,
      "learning_rate": 1.901538461538462e-05,
      "loss": 1.4463,
      "step": 513
    },
    {
      "epoch": 0.49423076923076925,
      "grad_norm": 8.570734024047852,
      "learning_rate": 1.9013461538461538e-05,
      "loss": 0.9839,
      "step": 514
    },
    {
      "epoch": 0.4951923076923077,
      "grad_norm": 8.404247283935547,
      "learning_rate": 1.9011538461538464e-05,
      "loss": 2.3967,
      "step": 515
    },
    {
      "epoch": 0.49615384615384617,
      "grad_norm": 10.02888011932373,
      "learning_rate": 1.9009615384615384e-05,
      "loss": 1.4832,
      "step": 516
    },
    {
      "epoch": 0.4971153846153846,
      "grad_norm": 7.578985691070557,
      "learning_rate": 1.900769230769231e-05,
      "loss": 1.8784,
      "step": 517
    },
    {
      "epoch": 0.4980769230769231,
      "grad_norm": 6.545218467712402,
      "learning_rate": 1.9005769230769233e-05,
      "loss": 1.6317,
      "step": 518
    },
    {
      "epoch": 0.49903846153846154,
      "grad_norm": 8.31453800201416,
      "learning_rate": 1.9003846153846156e-05,
      "loss": 2.7954,
      "step": 519
    },
    {
      "epoch": 0.5,
      "grad_norm": 7.494723796844482,
      "learning_rate": 1.9001923076923078e-05,
      "loss": 1.7262,
      "step": 520
    },
    {
      "epoch": 0.5009615384615385,
      "grad_norm": 7.0709052085876465,
      "learning_rate": 1.9e-05,
      "loss": 0.3024,
      "step": 521
    },
    {
      "epoch": 0.5019230769230769,
      "grad_norm": 7.989323139190674,
      "learning_rate": 1.8998076923076924e-05,
      "loss": 1.3351,
      "step": 522
    },
    {
      "epoch": 0.5028846153846154,
      "grad_norm": 7.342071533203125,
      "learning_rate": 1.899615384615385e-05,
      "loss": 1.4367,
      "step": 523
    },
    {
      "epoch": 0.5038461538461538,
      "grad_norm": 7.2725677490234375,
      "learning_rate": 1.899423076923077e-05,
      "loss": 1.2815,
      "step": 524
    },
    {
      "epoch": 0.5048076923076923,
      "grad_norm": 9.889816284179688,
      "learning_rate": 1.8992307692307696e-05,
      "loss": 1.6282,
      "step": 525
    },
    {
      "epoch": 0.5057692307692307,
      "grad_norm": 6.077330589294434,
      "learning_rate": 1.8990384615384615e-05,
      "loss": 1.7416,
      "step": 526
    },
    {
      "epoch": 0.5067307692307692,
      "grad_norm": 10.50168228149414,
      "learning_rate": 1.898846153846154e-05,
      "loss": 1.5442,
      "step": 527
    },
    {
      "epoch": 0.5076923076923077,
      "grad_norm": 16.89361000061035,
      "learning_rate": 1.8986538461538464e-05,
      "loss": 0.6757,
      "step": 528
    },
    {
      "epoch": 0.5086538461538461,
      "grad_norm": 6.982580661773682,
      "learning_rate": 1.8984615384615387e-05,
      "loss": 1.488,
      "step": 529
    },
    {
      "epoch": 0.5096153846153846,
      "grad_norm": 14.736286163330078,
      "learning_rate": 1.898269230769231e-05,
      "loss": 1.0485,
      "step": 530
    },
    {
      "epoch": 0.510576923076923,
      "grad_norm": 7.002066612243652,
      "learning_rate": 1.8980769230769232e-05,
      "loss": 1.4527,
      "step": 531
    },
    {
      "epoch": 0.5115384615384615,
      "grad_norm": 8.637829780578613,
      "learning_rate": 1.8978846153846155e-05,
      "loss": 1.0842,
      "step": 532
    },
    {
      "epoch": 0.5125,
      "grad_norm": 6.877862930297852,
      "learning_rate": 1.897692307692308e-05,
      "loss": 1.9957,
      "step": 533
    },
    {
      "epoch": 0.5134615384615384,
      "grad_norm": 10.603399276733398,
      "learning_rate": 1.8975e-05,
      "loss": 1.3239,
      "step": 534
    },
    {
      "epoch": 0.5144230769230769,
      "grad_norm": 7.528897762298584,
      "learning_rate": 1.8973076923076927e-05,
      "loss": 1.5846,
      "step": 535
    },
    {
      "epoch": 0.5153846153846153,
      "grad_norm": 7.4205145835876465,
      "learning_rate": 1.8971153846153846e-05,
      "loss": 0.9627,
      "step": 536
    },
    {
      "epoch": 0.5163461538461539,
      "grad_norm": 12.388703346252441,
      "learning_rate": 1.8969230769230772e-05,
      "loss": 1.757,
      "step": 537
    },
    {
      "epoch": 0.5173076923076924,
      "grad_norm": 6.902607440948486,
      "learning_rate": 1.8967307692307695e-05,
      "loss": 1.3834,
      "step": 538
    },
    {
      "epoch": 0.5182692307692308,
      "grad_norm": 6.317367076873779,
      "learning_rate": 1.8965384615384618e-05,
      "loss": 1.833,
      "step": 539
    },
    {
      "epoch": 0.5192307692307693,
      "grad_norm": 45.883121490478516,
      "learning_rate": 1.896346153846154e-05,
      "loss": 1.0892,
      "step": 540
    },
    {
      "epoch": 0.5201923076923077,
      "grad_norm": 7.756776332855225,
      "learning_rate": 1.8961538461538463e-05,
      "loss": 1.1896,
      "step": 541
    },
    {
      "epoch": 0.5211538461538462,
      "grad_norm": 8.648737907409668,
      "learning_rate": 1.8959615384615386e-05,
      "loss": 1.2402,
      "step": 542
    },
    {
      "epoch": 0.5221153846153846,
      "grad_norm": 9.171666145324707,
      "learning_rate": 1.895769230769231e-05,
      "loss": 1.978,
      "step": 543
    },
    {
      "epoch": 0.5230769230769231,
      "grad_norm": 5.806861877441406,
      "learning_rate": 1.895576923076923e-05,
      "loss": 1.9965,
      "step": 544
    },
    {
      "epoch": 0.5240384615384616,
      "grad_norm": 7.529050827026367,
      "learning_rate": 1.8953846153846158e-05,
      "loss": 2.4989,
      "step": 545
    },
    {
      "epoch": 0.525,
      "grad_norm": 15.871078491210938,
      "learning_rate": 1.8951923076923077e-05,
      "loss": 0.4694,
      "step": 546
    },
    {
      "epoch": 0.5259615384615385,
      "grad_norm": 6.019916534423828,
      "learning_rate": 1.8950000000000003e-05,
      "loss": 1.1271,
      "step": 547
    },
    {
      "epoch": 0.5269230769230769,
      "grad_norm": 14.74963665008545,
      "learning_rate": 1.8948076923076926e-05,
      "loss": 1.2922,
      "step": 548
    },
    {
      "epoch": 0.5278846153846154,
      "grad_norm": 6.446099281311035,
      "learning_rate": 1.894615384615385e-05,
      "loss": 1.799,
      "step": 549
    },
    {
      "epoch": 0.5288461538461539,
      "grad_norm": 6.964904308319092,
      "learning_rate": 1.894423076923077e-05,
      "loss": 2.0312,
      "step": 550
    },
    {
      "epoch": 0.5298076923076923,
      "grad_norm": 9.207331657409668,
      "learning_rate": 1.8942307692307694e-05,
      "loss": 1.8271,
      "step": 551
    },
    {
      "epoch": 0.5307692307692308,
      "grad_norm": 6.899868965148926,
      "learning_rate": 1.8940384615384617e-05,
      "loss": 0.8324,
      "step": 552
    },
    {
      "epoch": 0.5317307692307692,
      "grad_norm": 5.174905776977539,
      "learning_rate": 1.893846153846154e-05,
      "loss": 2.1015,
      "step": 553
    },
    {
      "epoch": 0.5326923076923077,
      "grad_norm": 5.53189754486084,
      "learning_rate": 1.8936538461538463e-05,
      "loss": 1.8996,
      "step": 554
    },
    {
      "epoch": 0.5336538461538461,
      "grad_norm": 9.955574989318848,
      "learning_rate": 1.8934615384615385e-05,
      "loss": 2.0089,
      "step": 555
    },
    {
      "epoch": 0.5346153846153846,
      "grad_norm": 8.021696090698242,
      "learning_rate": 1.8932692307692308e-05,
      "loss": 2.2734,
      "step": 556
    },
    {
      "epoch": 0.5355769230769231,
      "grad_norm": 10.378531455993652,
      "learning_rate": 1.893076923076923e-05,
      "loss": 1.8826,
      "step": 557
    },
    {
      "epoch": 0.5365384615384615,
      "grad_norm": 9.111212730407715,
      "learning_rate": 1.8928846153846154e-05,
      "loss": 1.1848,
      "step": 558
    },
    {
      "epoch": 0.5375,
      "grad_norm": 5.8353047370910645,
      "learning_rate": 1.892692307692308e-05,
      "loss": 1.7289,
      "step": 559
    },
    {
      "epoch": 0.5384615384615384,
      "grad_norm": 12.941784858703613,
      "learning_rate": 1.8925000000000003e-05,
      "loss": 1.5968,
      "step": 560
    },
    {
      "epoch": 0.5394230769230769,
      "grad_norm": 5.295007705688477,
      "learning_rate": 1.8923076923076925e-05,
      "loss": 1.3364,
      "step": 561
    },
    {
      "epoch": 0.5403846153846154,
      "grad_norm": 13.065424919128418,
      "learning_rate": 1.8921153846153848e-05,
      "loss": 2.0776,
      "step": 562
    },
    {
      "epoch": 0.5413461538461538,
      "grad_norm": 7.602801322937012,
      "learning_rate": 1.891923076923077e-05,
      "loss": 1.4898,
      "step": 563
    },
    {
      "epoch": 0.5423076923076923,
      "grad_norm": 8.116632461547852,
      "learning_rate": 1.8917307692307694e-05,
      "loss": 1.7489,
      "step": 564
    },
    {
      "epoch": 0.5432692307692307,
      "grad_norm": 8.733354568481445,
      "learning_rate": 1.8915384615384616e-05,
      "loss": 1.563,
      "step": 565
    },
    {
      "epoch": 0.5442307692307692,
      "grad_norm": 6.917823314666748,
      "learning_rate": 1.891346153846154e-05,
      "loss": 0.9427,
      "step": 566
    },
    {
      "epoch": 0.5451923076923076,
      "grad_norm": 11.001572608947754,
      "learning_rate": 1.8911538461538462e-05,
      "loss": 0.2838,
      "step": 567
    },
    {
      "epoch": 0.5461538461538461,
      "grad_norm": 6.4008378982543945,
      "learning_rate": 1.8909615384615385e-05,
      "loss": 0.6993,
      "step": 568
    },
    {
      "epoch": 0.5471153846153847,
      "grad_norm": 15.045835494995117,
      "learning_rate": 1.8907692307692308e-05,
      "loss": 1.9803,
      "step": 569
    },
    {
      "epoch": 0.5480769230769231,
      "grad_norm": 6.2299346923828125,
      "learning_rate": 1.8905769230769234e-05,
      "loss": 1.5866,
      "step": 570
    },
    {
      "epoch": 0.5490384615384616,
      "grad_norm": 16.564680099487305,
      "learning_rate": 1.8903846153846156e-05,
      "loss": 1.3381,
      "step": 571
    },
    {
      "epoch": 0.55,
      "grad_norm": 7.526439189910889,
      "learning_rate": 1.890192307692308e-05,
      "loss": 2.7806,
      "step": 572
    },
    {
      "epoch": 0.5509615384615385,
      "grad_norm": 8.007301330566406,
      "learning_rate": 1.8900000000000002e-05,
      "loss": 1.8618,
      "step": 573
    },
    {
      "epoch": 0.551923076923077,
      "grad_norm": 7.4391913414001465,
      "learning_rate": 1.8898076923076925e-05,
      "loss": 0.3786,
      "step": 574
    },
    {
      "epoch": 0.5528846153846154,
      "grad_norm": 6.14222526550293,
      "learning_rate": 1.8896153846153848e-05,
      "loss": 1.2006,
      "step": 575
    },
    {
      "epoch": 0.5538461538461539,
      "grad_norm": 7.328268051147461,
      "learning_rate": 1.889423076923077e-05,
      "loss": 1.9972,
      "step": 576
    },
    {
      "epoch": 0.5548076923076923,
      "grad_norm": 5.692862033843994,
      "learning_rate": 1.8892307692307693e-05,
      "loss": 1.3843,
      "step": 577
    },
    {
      "epoch": 0.5557692307692308,
      "grad_norm": 6.48848295211792,
      "learning_rate": 1.8890384615384616e-05,
      "loss": 0.9223,
      "step": 578
    },
    {
      "epoch": 0.5567307692307693,
      "grad_norm": 7.728119373321533,
      "learning_rate": 1.888846153846154e-05,
      "loss": 2.1579,
      "step": 579
    },
    {
      "epoch": 0.5576923076923077,
      "grad_norm": 9.01602840423584,
      "learning_rate": 1.8886538461538465e-05,
      "loss": 1.9309,
      "step": 580
    },
    {
      "epoch": 0.5586538461538462,
      "grad_norm": 6.882285118103027,
      "learning_rate": 1.8884615384615384e-05,
      "loss": 1.7623,
      "step": 581
    },
    {
      "epoch": 0.5596153846153846,
      "grad_norm": 9.001343727111816,
      "learning_rate": 1.888269230769231e-05,
      "loss": 1.8455,
      "step": 582
    },
    {
      "epoch": 0.5605769230769231,
      "grad_norm": 5.8853440284729,
      "learning_rate": 1.8880769230769233e-05,
      "loss": 1.3956,
      "step": 583
    },
    {
      "epoch": 0.5615384615384615,
      "grad_norm": 6.369146823883057,
      "learning_rate": 1.8878846153846156e-05,
      "loss": 0.8641,
      "step": 584
    },
    {
      "epoch": 0.5625,
      "grad_norm": 8.271303176879883,
      "learning_rate": 1.887692307692308e-05,
      "loss": 1.7502,
      "step": 585
    },
    {
      "epoch": 0.5634615384615385,
      "grad_norm": 8.594088554382324,
      "learning_rate": 1.8875e-05,
      "loss": 0.3336,
      "step": 586
    },
    {
      "epoch": 0.5644230769230769,
      "grad_norm": 7.125607490539551,
      "learning_rate": 1.8873076923076924e-05,
      "loss": 1.6199,
      "step": 587
    },
    {
      "epoch": 0.5653846153846154,
      "grad_norm": 13.007461547851562,
      "learning_rate": 1.8871153846153847e-05,
      "loss": 2.5742,
      "step": 588
    },
    {
      "epoch": 0.5663461538461538,
      "grad_norm": 5.453205585479736,
      "learning_rate": 1.886923076923077e-05,
      "loss": 1.9416,
      "step": 589
    },
    {
      "epoch": 0.5673076923076923,
      "grad_norm": 5.565654277801514,
      "learning_rate": 1.8867307692307696e-05,
      "loss": 1.6362,
      "step": 590
    },
    {
      "epoch": 0.5682692307692307,
      "grad_norm": 7.471407890319824,
      "learning_rate": 1.8865384615384615e-05,
      "loss": 1.759,
      "step": 591
    },
    {
      "epoch": 0.5692307692307692,
      "grad_norm": 8.642552375793457,
      "learning_rate": 1.886346153846154e-05,
      "loss": 1.1907,
      "step": 592
    },
    {
      "epoch": 0.5701923076923077,
      "grad_norm": 9.892276763916016,
      "learning_rate": 1.886153846153846e-05,
      "loss": 1.9446,
      "step": 593
    },
    {
      "epoch": 0.5711538461538461,
      "grad_norm": 4.7922468185424805,
      "learning_rate": 1.8859615384615387e-05,
      "loss": 1.9242,
      "step": 594
    },
    {
      "epoch": 0.5721153846153846,
      "grad_norm": 5.984920501708984,
      "learning_rate": 1.885769230769231e-05,
      "loss": 2.0688,
      "step": 595
    },
    {
      "epoch": 0.573076923076923,
      "grad_norm": 7.306873798370361,
      "learning_rate": 1.8855769230769232e-05,
      "loss": 1.7256,
      "step": 596
    },
    {
      "epoch": 0.5740384615384615,
      "grad_norm": 8.649005889892578,
      "learning_rate": 1.8853846153846155e-05,
      "loss": 1.3038,
      "step": 597
    },
    {
      "epoch": 0.575,
      "grad_norm": 48.304237365722656,
      "learning_rate": 1.8851923076923078e-05,
      "loss": 0.2534,
      "step": 598
    },
    {
      "epoch": 0.5759615384615384,
      "grad_norm": 6.1854753494262695,
      "learning_rate": 1.885e-05,
      "loss": 1.5941,
      "step": 599
    },
    {
      "epoch": 0.5769230769230769,
      "grad_norm": 6.19060754776001,
      "learning_rate": 1.8848076923076927e-05,
      "loss": 2.2948,
      "step": 600
    },
    {
      "epoch": 0.5778846153846153,
      "grad_norm": 8.893004417419434,
      "learning_rate": 1.8846153846153846e-05,
      "loss": 1.5479,
      "step": 601
    },
    {
      "epoch": 0.5788461538461539,
      "grad_norm": 6.727926254272461,
      "learning_rate": 1.8844230769230772e-05,
      "loss": 0.9431,
      "step": 602
    },
    {
      "epoch": 0.5798076923076924,
      "grad_norm": 11.47403335571289,
      "learning_rate": 1.8842307692307692e-05,
      "loss": 0.6374,
      "step": 603
    },
    {
      "epoch": 0.5807692307692308,
      "grad_norm": 6.694262981414795,
      "learning_rate": 1.8840384615384618e-05,
      "loss": 1.4688,
      "step": 604
    },
    {
      "epoch": 0.5817307692307693,
      "grad_norm": 7.158941745758057,
      "learning_rate": 1.883846153846154e-05,
      "loss": 1.5833,
      "step": 605
    },
    {
      "epoch": 0.5826923076923077,
      "grad_norm": 10.990889549255371,
      "learning_rate": 1.8836538461538464e-05,
      "loss": 0.3264,
      "step": 606
    },
    {
      "epoch": 0.5836538461538462,
      "grad_norm": 5.071773529052734,
      "learning_rate": 1.8834615384615386e-05,
      "loss": 1.8413,
      "step": 607
    },
    {
      "epoch": 0.5846153846153846,
      "grad_norm": 6.081483840942383,
      "learning_rate": 1.883269230769231e-05,
      "loss": 1.3949,
      "step": 608
    },
    {
      "epoch": 0.5855769230769231,
      "grad_norm": 10.100804328918457,
      "learning_rate": 1.8830769230769232e-05,
      "loss": 1.2227,
      "step": 609
    },
    {
      "epoch": 0.5865384615384616,
      "grad_norm": 45.66609191894531,
      "learning_rate": 1.8828846153846158e-05,
      "loss": 2.2826,
      "step": 610
    },
    {
      "epoch": 0.5875,
      "grad_norm": 84.56139373779297,
      "learning_rate": 1.8826923076923077e-05,
      "loss": 1.5716,
      "step": 611
    },
    {
      "epoch": 0.5884615384615385,
      "grad_norm": 5.183837413787842,
      "learning_rate": 1.8825000000000004e-05,
      "loss": 0.236,
      "step": 612
    },
    {
      "epoch": 0.5894230769230769,
      "grad_norm": 5.930318355560303,
      "learning_rate": 1.8823076923076923e-05,
      "loss": 1.4294,
      "step": 613
    },
    {
      "epoch": 0.5903846153846154,
      "grad_norm": 5.737025260925293,
      "learning_rate": 1.882115384615385e-05,
      "loss": 1.0444,
      "step": 614
    },
    {
      "epoch": 0.5913461538461539,
      "grad_norm": 11.920835494995117,
      "learning_rate": 1.8819230769230772e-05,
      "loss": 1.6005,
      "step": 615
    },
    {
      "epoch": 0.5923076923076923,
      "grad_norm": 6.352038860321045,
      "learning_rate": 1.8817307692307695e-05,
      "loss": 0.9256,
      "step": 616
    },
    {
      "epoch": 0.5932692307692308,
      "grad_norm": 6.145864009857178,
      "learning_rate": 1.8815384615384617e-05,
      "loss": 2.0199,
      "step": 617
    },
    {
      "epoch": 0.5942307692307692,
      "grad_norm": 11.5868558883667,
      "learning_rate": 1.881346153846154e-05,
      "loss": 0.205,
      "step": 618
    },
    {
      "epoch": 0.5951923076923077,
      "grad_norm": 7.028053283691406,
      "learning_rate": 1.8811538461538463e-05,
      "loss": 2.2408,
      "step": 619
    },
    {
      "epoch": 0.5961538461538461,
      "grad_norm": 7.6395263671875,
      "learning_rate": 1.8809615384615386e-05,
      "loss": 1.8101,
      "step": 620
    },
    {
      "epoch": 0.5971153846153846,
      "grad_norm": 4.992426872253418,
      "learning_rate": 1.880769230769231e-05,
      "loss": 2.0666,
      "step": 621
    },
    {
      "epoch": 0.5980769230769231,
      "grad_norm": 6.661545753479004,
      "learning_rate": 1.8805769230769235e-05,
      "loss": 1.9225,
      "step": 622
    },
    {
      "epoch": 0.5990384615384615,
      "grad_norm": 4.971176624298096,
      "learning_rate": 1.8803846153846154e-05,
      "loss": 1.5103,
      "step": 623
    },
    {
      "epoch": 0.6,
      "grad_norm": 10.070323944091797,
      "learning_rate": 1.880192307692308e-05,
      "loss": 1.5709,
      "step": 624
    },
    {
      "epoch": 0.6009615384615384,
      "grad_norm": 6.801747798919678,
      "learning_rate": 1.88e-05,
      "loss": 0.9406,
      "step": 625
    },
    {
      "epoch": 0.6019230769230769,
      "grad_norm": 6.625246047973633,
      "learning_rate": 1.8798076923076926e-05,
      "loss": 1.1392,
      "step": 626
    },
    {
      "epoch": 0.6028846153846154,
      "grad_norm": 5.875631332397461,
      "learning_rate": 1.879615384615385e-05,
      "loss": 1.0007,
      "step": 627
    },
    {
      "epoch": 0.6038461538461538,
      "grad_norm": 7.05941104888916,
      "learning_rate": 1.879423076923077e-05,
      "loss": 1.7803,
      "step": 628
    },
    {
      "epoch": 0.6048076923076923,
      "grad_norm": 10.442258834838867,
      "learning_rate": 1.8792307692307694e-05,
      "loss": 1.4953,
      "step": 629
    },
    {
      "epoch": 0.6057692307692307,
      "grad_norm": 10.106254577636719,
      "learning_rate": 1.8790384615384617e-05,
      "loss": 2.0134,
      "step": 630
    },
    {
      "epoch": 0.6067307692307692,
      "grad_norm": 7.542573928833008,
      "learning_rate": 1.878846153846154e-05,
      "loss": 1.57,
      "step": 631
    },
    {
      "epoch": 0.6076923076923076,
      "grad_norm": 7.6922383308410645,
      "learning_rate": 1.8786538461538462e-05,
      "loss": 1.8223,
      "step": 632
    },
    {
      "epoch": 0.6086538461538461,
      "grad_norm": 6.388813495635986,
      "learning_rate": 1.8784615384615385e-05,
      "loss": 1.8276,
      "step": 633
    },
    {
      "epoch": 0.6096153846153847,
      "grad_norm": 8.263483047485352,
      "learning_rate": 1.8782692307692308e-05,
      "loss": 1.8015,
      "step": 634
    },
    {
      "epoch": 0.6105769230769231,
      "grad_norm": 9.114771842956543,
      "learning_rate": 1.878076923076923e-05,
      "loss": 1.3283,
      "step": 635
    },
    {
      "epoch": 0.6115384615384616,
      "grad_norm": 7.471295356750488,
      "learning_rate": 1.8778846153846157e-05,
      "loss": 1.6021,
      "step": 636
    },
    {
      "epoch": 0.6125,
      "grad_norm": 5.633342742919922,
      "learning_rate": 1.877692307692308e-05,
      "loss": 1.052,
      "step": 637
    },
    {
      "epoch": 0.6134615384615385,
      "grad_norm": 7.0075483322143555,
      "learning_rate": 1.8775000000000002e-05,
      "loss": 1.6527,
      "step": 638
    },
    {
      "epoch": 0.614423076923077,
      "grad_norm": 5.855994701385498,
      "learning_rate": 1.8773076923076925e-05,
      "loss": 1.0569,
      "step": 639
    },
    {
      "epoch": 0.6153846153846154,
      "grad_norm": 7.950446605682373,
      "learning_rate": 1.8771153846153848e-05,
      "loss": 0.8393,
      "step": 640
    },
    {
      "epoch": 0.6163461538461539,
      "grad_norm": 6.0388898849487305,
      "learning_rate": 1.876923076923077e-05,
      "loss": 1.5614,
      "step": 641
    },
    {
      "epoch": 0.6173076923076923,
      "grad_norm": 7.35818338394165,
      "learning_rate": 1.8767307692307693e-05,
      "loss": 1.5011,
      "step": 642
    },
    {
      "epoch": 0.6182692307692308,
      "grad_norm": 6.3972249031066895,
      "learning_rate": 1.8765384615384616e-05,
      "loss": 1.6622,
      "step": 643
    },
    {
      "epoch": 0.6192307692307693,
      "grad_norm": 6.6761908531188965,
      "learning_rate": 1.876346153846154e-05,
      "loss": 1.3955,
      "step": 644
    },
    {
      "epoch": 0.6201923076923077,
      "grad_norm": 8.067420959472656,
      "learning_rate": 1.8761538461538462e-05,
      "loss": 1.7102,
      "step": 645
    },
    {
      "epoch": 0.6211538461538462,
      "grad_norm": 5.278964519500732,
      "learning_rate": 1.8759615384615385e-05,
      "loss": 1.8614,
      "step": 646
    },
    {
      "epoch": 0.6221153846153846,
      "grad_norm": 10.334246635437012,
      "learning_rate": 1.875769230769231e-05,
      "loss": 2.4036,
      "step": 647
    },
    {
      "epoch": 0.6230769230769231,
      "grad_norm": 7.663354873657227,
      "learning_rate": 1.8755769230769233e-05,
      "loss": 2.0297,
      "step": 648
    },
    {
      "epoch": 0.6240384615384615,
      "grad_norm": 6.963376522064209,
      "learning_rate": 1.8753846153846156e-05,
      "loss": 1.2412,
      "step": 649
    },
    {
      "epoch": 0.625,
      "grad_norm": 5.794271469116211,
      "learning_rate": 1.875192307692308e-05,
      "loss": 1.8384,
      "step": 650
    },
    {
      "epoch": 0.6259615384615385,
      "grad_norm": 7.594797611236572,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 1.3439,
      "step": 651
    },
    {
      "epoch": 0.6269230769230769,
      "grad_norm": 6.985005855560303,
      "learning_rate": 1.8748076923076925e-05,
      "loss": 0.7462,
      "step": 652
    },
    {
      "epoch": 0.6278846153846154,
      "grad_norm": 7.765188694000244,
      "learning_rate": 1.8746153846153847e-05,
      "loss": 0.2915,
      "step": 653
    },
    {
      "epoch": 0.6288461538461538,
      "grad_norm": 6.000957012176514,
      "learning_rate": 1.874423076923077e-05,
      "loss": 1.6616,
      "step": 654
    },
    {
      "epoch": 0.6298076923076923,
      "grad_norm": 7.8326311111450195,
      "learning_rate": 1.8742307692307693e-05,
      "loss": 1.0101,
      "step": 655
    },
    {
      "epoch": 0.6307692307692307,
      "grad_norm": 7.6645426750183105,
      "learning_rate": 1.8740384615384616e-05,
      "loss": 1.1892,
      "step": 656
    },
    {
      "epoch": 0.6317307692307692,
      "grad_norm": 6.468803882598877,
      "learning_rate": 1.8738461538461542e-05,
      "loss": 1.7722,
      "step": 657
    },
    {
      "epoch": 0.6326923076923077,
      "grad_norm": 8.985167503356934,
      "learning_rate": 1.873653846153846e-05,
      "loss": 0.8705,
      "step": 658
    },
    {
      "epoch": 0.6336538461538461,
      "grad_norm": 5.752403259277344,
      "learning_rate": 1.8734615384615387e-05,
      "loss": 1.5951,
      "step": 659
    },
    {
      "epoch": 0.6346153846153846,
      "grad_norm": 5.733902454376221,
      "learning_rate": 1.873269230769231e-05,
      "loss": 1.0841,
      "step": 660
    },
    {
      "epoch": 0.635576923076923,
      "grad_norm": 7.977508544921875,
      "learning_rate": 1.8730769230769233e-05,
      "loss": 2.304,
      "step": 661
    },
    {
      "epoch": 0.6365384615384615,
      "grad_norm": 9.226703643798828,
      "learning_rate": 1.8728846153846156e-05,
      "loss": 1.3129,
      "step": 662
    },
    {
      "epoch": 0.6375,
      "grad_norm": 4.507994651794434,
      "learning_rate": 1.872692307692308e-05,
      "loss": 0.1571,
      "step": 663
    },
    {
      "epoch": 0.6384615384615384,
      "grad_norm": 8.678627014160156,
      "learning_rate": 1.8725e-05,
      "loss": 1.076,
      "step": 664
    },
    {
      "epoch": 0.6394230769230769,
      "grad_norm": 10.763982772827148,
      "learning_rate": 1.8723076923076924e-05,
      "loss": 0.3711,
      "step": 665
    },
    {
      "epoch": 0.6403846153846153,
      "grad_norm": 8.09221076965332,
      "learning_rate": 1.8721153846153847e-05,
      "loss": 2.1309,
      "step": 666
    },
    {
      "epoch": 0.6413461538461539,
      "grad_norm": 6.655214786529541,
      "learning_rate": 1.8719230769230773e-05,
      "loss": 1.0792,
      "step": 667
    },
    {
      "epoch": 0.6423076923076924,
      "grad_norm": 10.103608131408691,
      "learning_rate": 1.8717307692307692e-05,
      "loss": 1.3943,
      "step": 668
    },
    {
      "epoch": 0.6432692307692308,
      "grad_norm": 5.648003101348877,
      "learning_rate": 1.871538461538462e-05,
      "loss": 1.9904,
      "step": 669
    },
    {
      "epoch": 0.6442307692307693,
      "grad_norm": 9.37877368927002,
      "learning_rate": 1.8713461538461538e-05,
      "loss": 1.9208,
      "step": 670
    },
    {
      "epoch": 0.6451923076923077,
      "grad_norm": 7.350439071655273,
      "learning_rate": 1.8711538461538464e-05,
      "loss": 1.3566,
      "step": 671
    },
    {
      "epoch": 0.6461538461538462,
      "grad_norm": 10.073530197143555,
      "learning_rate": 1.8709615384615387e-05,
      "loss": 1.7306,
      "step": 672
    },
    {
      "epoch": 0.6471153846153846,
      "grad_norm": 17.464054107666016,
      "learning_rate": 1.870769230769231e-05,
      "loss": 0.3625,
      "step": 673
    },
    {
      "epoch": 0.6480769230769231,
      "grad_norm": 5.875633239746094,
      "learning_rate": 1.8705769230769232e-05,
      "loss": 1.4299,
      "step": 674
    },
    {
      "epoch": 0.6490384615384616,
      "grad_norm": 6.158904552459717,
      "learning_rate": 1.8703846153846155e-05,
      "loss": 1.3999,
      "step": 675
    },
    {
      "epoch": 0.65,
      "grad_norm": 12.796852111816406,
      "learning_rate": 1.8701923076923078e-05,
      "loss": 2.6588,
      "step": 676
    },
    {
      "epoch": 0.6509615384615385,
      "grad_norm": 7.189528942108154,
      "learning_rate": 1.8700000000000004e-05,
      "loss": 1.4324,
      "step": 677
    },
    {
      "epoch": 0.6519230769230769,
      "grad_norm": 9.084332466125488,
      "learning_rate": 1.8698076923076923e-05,
      "loss": 0.9373,
      "step": 678
    },
    {
      "epoch": 0.6528846153846154,
      "grad_norm": 6.731773853302002,
      "learning_rate": 1.869615384615385e-05,
      "loss": 2.2894,
      "step": 679
    },
    {
      "epoch": 0.6538461538461539,
      "grad_norm": 9.013501167297363,
      "learning_rate": 1.869423076923077e-05,
      "loss": 1.6621,
      "step": 680
    },
    {
      "epoch": 0.6548076923076923,
      "grad_norm": 8.883675575256348,
      "learning_rate": 1.8692307692307695e-05,
      "loss": 2.0719,
      "step": 681
    },
    {
      "epoch": 0.6557692307692308,
      "grad_norm": 16.245777130126953,
      "learning_rate": 1.8690384615384618e-05,
      "loss": 1.5625,
      "step": 682
    },
    {
      "epoch": 0.6567307692307692,
      "grad_norm": 8.614927291870117,
      "learning_rate": 1.868846153846154e-05,
      "loss": 2.2241,
      "step": 683
    },
    {
      "epoch": 0.6576923076923077,
      "grad_norm": 10.538252830505371,
      "learning_rate": 1.8686538461538463e-05,
      "loss": 1.8524,
      "step": 684
    },
    {
      "epoch": 0.6586538461538461,
      "grad_norm": 5.749369144439697,
      "learning_rate": 1.8684615384615386e-05,
      "loss": 2.1724,
      "step": 685
    },
    {
      "epoch": 0.6596153846153846,
      "grad_norm": 6.020257949829102,
      "learning_rate": 1.868269230769231e-05,
      "loss": 2.0694,
      "step": 686
    },
    {
      "epoch": 0.6605769230769231,
      "grad_norm": 5.89334774017334,
      "learning_rate": 1.868076923076923e-05,
      "loss": 1.657,
      "step": 687
    },
    {
      "epoch": 0.6615384615384615,
      "grad_norm": 7.405440330505371,
      "learning_rate": 1.8678846153846154e-05,
      "loss": 1.9245,
      "step": 688
    },
    {
      "epoch": 0.6625,
      "grad_norm": 6.2725605964660645,
      "learning_rate": 1.867692307692308e-05,
      "loss": 1.8321,
      "step": 689
    },
    {
      "epoch": 0.6634615384615384,
      "grad_norm": 7.977604389190674,
      "learning_rate": 1.8675e-05,
      "loss": 0.6999,
      "step": 690
    },
    {
      "epoch": 0.6644230769230769,
      "grad_norm": 11.380573272705078,
      "learning_rate": 1.8673076923076926e-05,
      "loss": 1.5302,
      "step": 691
    },
    {
      "epoch": 0.6653846153846154,
      "grad_norm": 8.192191123962402,
      "learning_rate": 1.8671153846153845e-05,
      "loss": 1.7535,
      "step": 692
    },
    {
      "epoch": 0.6663461538461538,
      "grad_norm": 7.227672576904297,
      "learning_rate": 1.866923076923077e-05,
      "loss": 2.4642,
      "step": 693
    },
    {
      "epoch": 0.6673076923076923,
      "grad_norm": 6.43698263168335,
      "learning_rate": 1.8667307692307694e-05,
      "loss": 1.3781,
      "step": 694
    },
    {
      "epoch": 0.6682692307692307,
      "grad_norm": 5.29248571395874,
      "learning_rate": 1.8665384615384617e-05,
      "loss": 1.6355,
      "step": 695
    },
    {
      "epoch": 0.6692307692307692,
      "grad_norm": 6.459606647491455,
      "learning_rate": 1.866346153846154e-05,
      "loss": 1.0336,
      "step": 696
    },
    {
      "epoch": 0.6701923076923076,
      "grad_norm": 11.09585189819336,
      "learning_rate": 1.8661538461538463e-05,
      "loss": 2.1734,
      "step": 697
    },
    {
      "epoch": 0.6711538461538461,
      "grad_norm": 5.64484977722168,
      "learning_rate": 1.8659615384615385e-05,
      "loss": 1.558,
      "step": 698
    },
    {
      "epoch": 0.6721153846153847,
      "grad_norm": 9.875425338745117,
      "learning_rate": 1.865769230769231e-05,
      "loss": 1.2173,
      "step": 699
    },
    {
      "epoch": 0.6730769230769231,
      "grad_norm": 8.822303771972656,
      "learning_rate": 1.865576923076923e-05,
      "loss": 1.3347,
      "step": 700
    },
    {
      "epoch": 0.6740384615384616,
      "grad_norm": 6.297491073608398,
      "learning_rate": 1.8653846153846157e-05,
      "loss": 1.1615,
      "step": 701
    },
    {
      "epoch": 0.675,
      "grad_norm": 9.418291091918945,
      "learning_rate": 1.8651923076923077e-05,
      "loss": 1.951,
      "step": 702
    },
    {
      "epoch": 0.6759615384615385,
      "grad_norm": 6.980666637420654,
      "learning_rate": 1.8650000000000003e-05,
      "loss": 1.5832,
      "step": 703
    },
    {
      "epoch": 0.676923076923077,
      "grad_norm": 22.25563621520996,
      "learning_rate": 1.8648076923076925e-05,
      "loss": 1.9846,
      "step": 704
    },
    {
      "epoch": 0.6778846153846154,
      "grad_norm": 7.2314252853393555,
      "learning_rate": 1.8646153846153848e-05,
      "loss": 1.8094,
      "step": 705
    },
    {
      "epoch": 0.6788461538461539,
      "grad_norm": 5.405145645141602,
      "learning_rate": 1.864423076923077e-05,
      "loss": 1.4698,
      "step": 706
    },
    {
      "epoch": 0.6798076923076923,
      "grad_norm": 8.119194984436035,
      "learning_rate": 1.8642307692307694e-05,
      "loss": 1.2231,
      "step": 707
    },
    {
      "epoch": 0.6807692307692308,
      "grad_norm": 8.585567474365234,
      "learning_rate": 1.8640384615384617e-05,
      "loss": 1.7173,
      "step": 708
    },
    {
      "epoch": 0.6817307692307693,
      "grad_norm": 7.5378265380859375,
      "learning_rate": 1.863846153846154e-05,
      "loss": 1.6381,
      "step": 709
    },
    {
      "epoch": 0.6826923076923077,
      "grad_norm": 6.741039752960205,
      "learning_rate": 1.8636538461538462e-05,
      "loss": 1.5371,
      "step": 710
    },
    {
      "epoch": 0.6836538461538462,
      "grad_norm": 13.07659912109375,
      "learning_rate": 1.8634615384615385e-05,
      "loss": 0.9395,
      "step": 711
    },
    {
      "epoch": 0.6846153846153846,
      "grad_norm": 8.932109832763672,
      "learning_rate": 1.8632692307692308e-05,
      "loss": 1.9824,
      "step": 712
    },
    {
      "epoch": 0.6855769230769231,
      "grad_norm": 10.604705810546875,
      "learning_rate": 1.8630769230769234e-05,
      "loss": 0.8756,
      "step": 713
    },
    {
      "epoch": 0.6865384615384615,
      "grad_norm": 11.661755561828613,
      "learning_rate": 1.8628846153846157e-05,
      "loss": 2.1691,
      "step": 714
    },
    {
      "epoch": 0.6875,
      "grad_norm": 11.037986755371094,
      "learning_rate": 1.862692307692308e-05,
      "loss": 2.6611,
      "step": 715
    },
    {
      "epoch": 0.6884615384615385,
      "grad_norm": 9.458392143249512,
      "learning_rate": 1.8625000000000002e-05,
      "loss": 1.7696,
      "step": 716
    },
    {
      "epoch": 0.6894230769230769,
      "grad_norm": 12.983380317687988,
      "learning_rate": 1.8623076923076925e-05,
      "loss": 0.4537,
      "step": 717
    },
    {
      "epoch": 0.6903846153846154,
      "grad_norm": 6.860255241394043,
      "learning_rate": 1.8621153846153848e-05,
      "loss": 2.5898,
      "step": 718
    },
    {
      "epoch": 0.6913461538461538,
      "grad_norm": 8.869921684265137,
      "learning_rate": 1.861923076923077e-05,
      "loss": 1.1549,
      "step": 719
    },
    {
      "epoch": 0.6923076923076923,
      "grad_norm": 6.613960266113281,
      "learning_rate": 1.8617307692307693e-05,
      "loss": 1.5037,
      "step": 720
    },
    {
      "epoch": 0.6932692307692307,
      "grad_norm": 11.794038772583008,
      "learning_rate": 1.8615384615384616e-05,
      "loss": 0.2554,
      "step": 721
    },
    {
      "epoch": 0.6942307692307692,
      "grad_norm": 5.841821670532227,
      "learning_rate": 1.861346153846154e-05,
      "loss": 2.1765,
      "step": 722
    },
    {
      "epoch": 0.6951923076923077,
      "grad_norm": 14.003434181213379,
      "learning_rate": 1.861153846153846e-05,
      "loss": 0.7202,
      "step": 723
    },
    {
      "epoch": 0.6961538461538461,
      "grad_norm": 7.015161991119385,
      "learning_rate": 1.8609615384615388e-05,
      "loss": 2.0263,
      "step": 724
    },
    {
      "epoch": 0.6971153846153846,
      "grad_norm": 6.3685784339904785,
      "learning_rate": 1.860769230769231e-05,
      "loss": 2.667,
      "step": 725
    },
    {
      "epoch": 0.698076923076923,
      "grad_norm": 5.814029693603516,
      "learning_rate": 1.8605769230769233e-05,
      "loss": 1.8984,
      "step": 726
    },
    {
      "epoch": 0.6990384615384615,
      "grad_norm": 7.584077835083008,
      "learning_rate": 1.8603846153846156e-05,
      "loss": 1.0058,
      "step": 727
    },
    {
      "epoch": 0.7,
      "grad_norm": 6.927216053009033,
      "learning_rate": 1.860192307692308e-05,
      "loss": 0.2066,
      "step": 728
    },
    {
      "epoch": 0.7009615384615384,
      "grad_norm": 7.098221302032471,
      "learning_rate": 1.86e-05,
      "loss": 0.7749,
      "step": 729
    },
    {
      "epoch": 0.7019230769230769,
      "grad_norm": 9.471940994262695,
      "learning_rate": 1.8598076923076924e-05,
      "loss": 0.4067,
      "step": 730
    },
    {
      "epoch": 0.7028846153846153,
      "grad_norm": 7.247340202331543,
      "learning_rate": 1.8596153846153847e-05,
      "loss": 1.8623,
      "step": 731
    },
    {
      "epoch": 0.7038461538461539,
      "grad_norm": 8.27721881866455,
      "learning_rate": 1.859423076923077e-05,
      "loss": 1.4318,
      "step": 732
    },
    {
      "epoch": 0.7048076923076924,
      "grad_norm": 9.744200706481934,
      "learning_rate": 1.8592307692307693e-05,
      "loss": 1.6389,
      "step": 733
    },
    {
      "epoch": 0.7057692307692308,
      "grad_norm": 5.985347747802734,
      "learning_rate": 1.859038461538462e-05,
      "loss": 1.121,
      "step": 734
    },
    {
      "epoch": 0.7067307692307693,
      "grad_norm": 6.505265235900879,
      "learning_rate": 1.8588461538461538e-05,
      "loss": 0.931,
      "step": 735
    },
    {
      "epoch": 0.7076923076923077,
      "grad_norm": 4.967981338500977,
      "learning_rate": 1.8586538461538464e-05,
      "loss": 1.5428,
      "step": 736
    },
    {
      "epoch": 0.7086538461538462,
      "grad_norm": 5.621415138244629,
      "learning_rate": 1.8584615384615387e-05,
      "loss": 2.1993,
      "step": 737
    },
    {
      "epoch": 0.7096153846153846,
      "grad_norm": 5.768062591552734,
      "learning_rate": 1.858269230769231e-05,
      "loss": 1.5588,
      "step": 738
    },
    {
      "epoch": 0.7105769230769231,
      "grad_norm": 11.648355484008789,
      "learning_rate": 1.8580769230769233e-05,
      "loss": 1.3395,
      "step": 739
    },
    {
      "epoch": 0.7115384615384616,
      "grad_norm": 7.67816162109375,
      "learning_rate": 1.8578846153846155e-05,
      "loss": 0.555,
      "step": 740
    },
    {
      "epoch": 0.7125,
      "grad_norm": 8.442037582397461,
      "learning_rate": 1.8576923076923078e-05,
      "loss": 2.1123,
      "step": 741
    },
    {
      "epoch": 0.7134615384615385,
      "grad_norm": 9.024529457092285,
      "learning_rate": 1.8575e-05,
      "loss": 1.3271,
      "step": 742
    },
    {
      "epoch": 0.7144230769230769,
      "grad_norm": 5.3744659423828125,
      "learning_rate": 1.8573076923076924e-05,
      "loss": 1.5036,
      "step": 743
    },
    {
      "epoch": 0.7153846153846154,
      "grad_norm": 9.213717460632324,
      "learning_rate": 1.857115384615385e-05,
      "loss": 1.6869,
      "step": 744
    },
    {
      "epoch": 0.7163461538461539,
      "grad_norm": 7.798869609832764,
      "learning_rate": 1.856923076923077e-05,
      "loss": 1.2998,
      "step": 745
    },
    {
      "epoch": 0.7173076923076923,
      "grad_norm": 8.315536499023438,
      "learning_rate": 1.8567307692307695e-05,
      "loss": 2.1209,
      "step": 746
    },
    {
      "epoch": 0.7182692307692308,
      "grad_norm": 6.136494159698486,
      "learning_rate": 1.8565384615384615e-05,
      "loss": 2.1834,
      "step": 747
    },
    {
      "epoch": 0.7192307692307692,
      "grad_norm": 7.762100696563721,
      "learning_rate": 1.856346153846154e-05,
      "loss": 1.106,
      "step": 748
    },
    {
      "epoch": 0.7201923076923077,
      "grad_norm": 7.476059913635254,
      "learning_rate": 1.856153846153846e-05,
      "loss": 0.124,
      "step": 749
    },
    {
      "epoch": 0.7211538461538461,
      "grad_norm": 14.628934860229492,
      "learning_rate": 1.8559615384615386e-05,
      "loss": 2.3296,
      "step": 750
    },
    {
      "epoch": 0.7221153846153846,
      "grad_norm": 7.959118366241455,
      "learning_rate": 1.855769230769231e-05,
      "loss": 1.6141,
      "step": 751
    },
    {
      "epoch": 0.7230769230769231,
      "grad_norm": 5.613009929656982,
      "learning_rate": 1.8555769230769232e-05,
      "loss": 1.6644,
      "step": 752
    },
    {
      "epoch": 0.7240384615384615,
      "grad_norm": 9.63479232788086,
      "learning_rate": 1.8553846153846155e-05,
      "loss": 1.3203,
      "step": 753
    },
    {
      "epoch": 0.725,
      "grad_norm": 8.840660095214844,
      "learning_rate": 1.8551923076923078e-05,
      "loss": 0.9918,
      "step": 754
    },
    {
      "epoch": 0.7259615384615384,
      "grad_norm": 8.775951385498047,
      "learning_rate": 1.855e-05,
      "loss": 1.2112,
      "step": 755
    },
    {
      "epoch": 0.7269230769230769,
      "grad_norm": 8.04185962677002,
      "learning_rate": 1.8548076923076926e-05,
      "loss": 1.0563,
      "step": 756
    },
    {
      "epoch": 0.7278846153846154,
      "grad_norm": 6.441699028015137,
      "learning_rate": 1.8546153846153846e-05,
      "loss": 1.415,
      "step": 757
    },
    {
      "epoch": 0.7288461538461538,
      "grad_norm": 7.380186557769775,
      "learning_rate": 1.8544230769230772e-05,
      "loss": 1.2957,
      "step": 758
    },
    {
      "epoch": 0.7298076923076923,
      "grad_norm": 6.839478492736816,
      "learning_rate": 1.854230769230769e-05,
      "loss": 0.0576,
      "step": 759
    },
    {
      "epoch": 0.7307692307692307,
      "grad_norm": 5.919545650482178,
      "learning_rate": 1.8540384615384618e-05,
      "loss": 1.8273,
      "step": 760
    },
    {
      "epoch": 0.7317307692307692,
      "grad_norm": 6.031734943389893,
      "learning_rate": 1.853846153846154e-05,
      "loss": 2.0583,
      "step": 761
    },
    {
      "epoch": 0.7326923076923076,
      "grad_norm": 16.725557327270508,
      "learning_rate": 1.8536538461538463e-05,
      "loss": 0.4888,
      "step": 762
    },
    {
      "epoch": 0.7336538461538461,
      "grad_norm": 9.849857330322266,
      "learning_rate": 1.8534615384615386e-05,
      "loss": 1.5844,
      "step": 763
    },
    {
      "epoch": 0.7346153846153847,
      "grad_norm": 7.580043315887451,
      "learning_rate": 1.853269230769231e-05,
      "loss": 1.5344,
      "step": 764
    },
    {
      "epoch": 0.7355769230769231,
      "grad_norm": 14.608004570007324,
      "learning_rate": 1.853076923076923e-05,
      "loss": 0.5943,
      "step": 765
    },
    {
      "epoch": 0.7365384615384616,
      "grad_norm": 9.874677658081055,
      "learning_rate": 1.8528846153846158e-05,
      "loss": 2.0118,
      "step": 766
    },
    {
      "epoch": 0.7375,
      "grad_norm": 8.443711280822754,
      "learning_rate": 1.8526923076923077e-05,
      "loss": 0.2649,
      "step": 767
    },
    {
      "epoch": 0.7384615384615385,
      "grad_norm": 7.408234119415283,
      "learning_rate": 1.8525000000000003e-05,
      "loss": 2.4989,
      "step": 768
    },
    {
      "epoch": 0.739423076923077,
      "grad_norm": 10.328468322753906,
      "learning_rate": 1.8523076923076922e-05,
      "loss": 2.4401,
      "step": 769
    },
    {
      "epoch": 0.7403846153846154,
      "grad_norm": 7.788931369781494,
      "learning_rate": 1.852115384615385e-05,
      "loss": 1.1023,
      "step": 770
    },
    {
      "epoch": 0.7413461538461539,
      "grad_norm": 6.9727373123168945,
      "learning_rate": 1.851923076923077e-05,
      "loss": 1.8139,
      "step": 771
    },
    {
      "epoch": 0.7423076923076923,
      "grad_norm": 5.38435697555542,
      "learning_rate": 1.8517307692307694e-05,
      "loss": 0.7687,
      "step": 772
    },
    {
      "epoch": 0.7432692307692308,
      "grad_norm": 9.320330619812012,
      "learning_rate": 1.8515384615384617e-05,
      "loss": 0.9145,
      "step": 773
    },
    {
      "epoch": 0.7442307692307693,
      "grad_norm": 5.896482944488525,
      "learning_rate": 1.851346153846154e-05,
      "loss": 1.5619,
      "step": 774
    },
    {
      "epoch": 0.7451923076923077,
      "grad_norm": 6.953587532043457,
      "learning_rate": 1.8511538461538462e-05,
      "loss": 1.4661,
      "step": 775
    },
    {
      "epoch": 0.7461538461538462,
      "grad_norm": 7.435813903808594,
      "learning_rate": 1.850961538461539e-05,
      "loss": 0.9679,
      "step": 776
    },
    {
      "epoch": 0.7471153846153846,
      "grad_norm": 3.3995041847229004,
      "learning_rate": 1.8507692307692308e-05,
      "loss": 0.1115,
      "step": 777
    },
    {
      "epoch": 0.7480769230769231,
      "grad_norm": 5.805627822875977,
      "learning_rate": 1.8505769230769234e-05,
      "loss": 1.5869,
      "step": 778
    },
    {
      "epoch": 0.7490384615384615,
      "grad_norm": 8.444680213928223,
      "learning_rate": 1.8503846153846154e-05,
      "loss": 0.5425,
      "step": 779
    },
    {
      "epoch": 0.75,
      "grad_norm": 12.76225471496582,
      "learning_rate": 1.850192307692308e-05,
      "loss": 0.964,
      "step": 780
    },
    {
      "epoch": 0.7509615384615385,
      "grad_norm": 8.889962196350098,
      "learning_rate": 1.8500000000000002e-05,
      "loss": 0.9456,
      "step": 781
    },
    {
      "epoch": 0.7519230769230769,
      "grad_norm": 10.733272552490234,
      "learning_rate": 1.8498076923076925e-05,
      "loss": 0.6734,
      "step": 782
    },
    {
      "epoch": 0.7528846153846154,
      "grad_norm": 10.116504669189453,
      "learning_rate": 1.8496153846153848e-05,
      "loss": 1.094,
      "step": 783
    },
    {
      "epoch": 0.7538461538461538,
      "grad_norm": 9.083056449890137,
      "learning_rate": 1.849423076923077e-05,
      "loss": 1.5326,
      "step": 784
    },
    {
      "epoch": 0.7548076923076923,
      "grad_norm": 15.09214973449707,
      "learning_rate": 1.8492307692307694e-05,
      "loss": 0.9928,
      "step": 785
    },
    {
      "epoch": 0.7557692307692307,
      "grad_norm": 8.774499893188477,
      "learning_rate": 1.8490384615384616e-05,
      "loss": 1.5556,
      "step": 786
    },
    {
      "epoch": 0.7567307692307692,
      "grad_norm": 7.619232654571533,
      "learning_rate": 1.848846153846154e-05,
      "loss": 1.5914,
      "step": 787
    },
    {
      "epoch": 0.7576923076923077,
      "grad_norm": 8.23576545715332,
      "learning_rate": 1.8486538461538465e-05,
      "loss": 0.2058,
      "step": 788
    },
    {
      "epoch": 0.7586538461538461,
      "grad_norm": 15.099931716918945,
      "learning_rate": 1.8484615384615385e-05,
      "loss": 0.9576,
      "step": 789
    },
    {
      "epoch": 0.7596153846153846,
      "grad_norm": 7.208157062530518,
      "learning_rate": 1.848269230769231e-05,
      "loss": 1.7184,
      "step": 790
    },
    {
      "epoch": 0.760576923076923,
      "grad_norm": 6.479497909545898,
      "learning_rate": 1.8480769230769234e-05,
      "loss": 0.5822,
      "step": 791
    },
    {
      "epoch": 0.7615384615384615,
      "grad_norm": 6.822669982910156,
      "learning_rate": 1.8478846153846156e-05,
      "loss": 1.9241,
      "step": 792
    },
    {
      "epoch": 0.7625,
      "grad_norm": 4.4648118019104,
      "learning_rate": 1.847692307692308e-05,
      "loss": 1.1475,
      "step": 793
    },
    {
      "epoch": 0.7634615384615384,
      "grad_norm": 10.82160472869873,
      "learning_rate": 1.8475000000000002e-05,
      "loss": 0.4348,
      "step": 794
    },
    {
      "epoch": 0.7644230769230769,
      "grad_norm": 8.334981918334961,
      "learning_rate": 1.8473076923076925e-05,
      "loss": 1.8452,
      "step": 795
    },
    {
      "epoch": 0.7653846153846153,
      "grad_norm": 9.427668571472168,
      "learning_rate": 1.8471153846153847e-05,
      "loss": 1.5333,
      "step": 796
    },
    {
      "epoch": 0.7663461538461539,
      "grad_norm": 11.876938819885254,
      "learning_rate": 1.846923076923077e-05,
      "loss": 1.2147,
      "step": 797
    },
    {
      "epoch": 0.7673076923076924,
      "grad_norm": 11.250529289245605,
      "learning_rate": 1.8467307692307693e-05,
      "loss": 1.762,
      "step": 798
    },
    {
      "epoch": 0.7682692307692308,
      "grad_norm": 5.490035533905029,
      "learning_rate": 1.8465384615384616e-05,
      "loss": 1.8779,
      "step": 799
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 10.36661148071289,
      "learning_rate": 1.846346153846154e-05,
      "loss": 0.7827,
      "step": 800
    },
    {
      "epoch": 0.7701923076923077,
      "grad_norm": 7.982383728027344,
      "learning_rate": 1.8461538461538465e-05,
      "loss": 0.3713,
      "step": 801
    },
    {
      "epoch": 0.7711538461538462,
      "grad_norm": 7.292619705200195,
      "learning_rate": 1.8459615384615387e-05,
      "loss": 1.9174,
      "step": 802
    },
    {
      "epoch": 0.7721153846153846,
      "grad_norm": 6.495660781860352,
      "learning_rate": 1.845769230769231e-05,
      "loss": 1.4031,
      "step": 803
    },
    {
      "epoch": 0.7730769230769231,
      "grad_norm": 14.343474388122559,
      "learning_rate": 1.8455769230769233e-05,
      "loss": 1.3923,
      "step": 804
    },
    {
      "epoch": 0.7740384615384616,
      "grad_norm": 6.646790504455566,
      "learning_rate": 1.8453846153846156e-05,
      "loss": 1.8904,
      "step": 805
    },
    {
      "epoch": 0.775,
      "grad_norm": 11.332518577575684,
      "learning_rate": 1.845192307692308e-05,
      "loss": 1.0809,
      "step": 806
    },
    {
      "epoch": 0.7759615384615385,
      "grad_norm": 6.88009786605835,
      "learning_rate": 1.845e-05,
      "loss": 1.5282,
      "step": 807
    },
    {
      "epoch": 0.7769230769230769,
      "grad_norm": 6.957644939422607,
      "learning_rate": 1.8448076923076924e-05,
      "loss": 0.7768,
      "step": 808
    },
    {
      "epoch": 0.7778846153846154,
      "grad_norm": 8.201460838317871,
      "learning_rate": 1.8446153846153847e-05,
      "loss": 0.2195,
      "step": 809
    },
    {
      "epoch": 0.7788461538461539,
      "grad_norm": 6.355077266693115,
      "learning_rate": 1.844423076923077e-05,
      "loss": 1.6515,
      "step": 810
    },
    {
      "epoch": 0.7798076923076923,
      "grad_norm": 7.51664400100708,
      "learning_rate": 1.8442307692307696e-05,
      "loss": 1.1667,
      "step": 811
    },
    {
      "epoch": 0.7807692307692308,
      "grad_norm": 10.927873611450195,
      "learning_rate": 1.8440384615384615e-05,
      "loss": 0.4066,
      "step": 812
    },
    {
      "epoch": 0.7817307692307692,
      "grad_norm": 7.271788597106934,
      "learning_rate": 1.843846153846154e-05,
      "loss": 1.8594,
      "step": 813
    },
    {
      "epoch": 0.7826923076923077,
      "grad_norm": 5.823859691619873,
      "learning_rate": 1.8436538461538464e-05,
      "loss": 1.6834,
      "step": 814
    },
    {
      "epoch": 0.7836538461538461,
      "grad_norm": 4.981279373168945,
      "learning_rate": 1.8434615384615387e-05,
      "loss": 2.1902,
      "step": 815
    },
    {
      "epoch": 0.7846153846153846,
      "grad_norm": 4.845020294189453,
      "learning_rate": 1.843269230769231e-05,
      "loss": 0.4574,
      "step": 816
    },
    {
      "epoch": 0.7855769230769231,
      "grad_norm": 7.722294807434082,
      "learning_rate": 1.8430769230769232e-05,
      "loss": 1.5639,
      "step": 817
    },
    {
      "epoch": 0.7865384615384615,
      "grad_norm": 7.862182140350342,
      "learning_rate": 1.8428846153846155e-05,
      "loss": 1.2243,
      "step": 818
    },
    {
      "epoch": 0.7875,
      "grad_norm": 7.911808490753174,
      "learning_rate": 1.8426923076923078e-05,
      "loss": 1.9189,
      "step": 819
    },
    {
      "epoch": 0.7884615384615384,
      "grad_norm": 7.461679458618164,
      "learning_rate": 1.8425e-05,
      "loss": 1.164,
      "step": 820
    },
    {
      "epoch": 0.7894230769230769,
      "grad_norm": 8.109272003173828,
      "learning_rate": 1.8423076923076923e-05,
      "loss": 1.553,
      "step": 821
    },
    {
      "epoch": 0.7903846153846154,
      "grad_norm": 3.989435911178589,
      "learning_rate": 1.8421153846153846e-05,
      "loss": 0.0475,
      "step": 822
    },
    {
      "epoch": 0.7913461538461538,
      "grad_norm": 8.35849380493164,
      "learning_rate": 1.8419230769230772e-05,
      "loss": 1.5019,
      "step": 823
    },
    {
      "epoch": 0.7923076923076923,
      "grad_norm": 12.632129669189453,
      "learning_rate": 1.8417307692307692e-05,
      "loss": 1.7882,
      "step": 824
    },
    {
      "epoch": 0.7932692307692307,
      "grad_norm": 10.152111053466797,
      "learning_rate": 1.8415384615384618e-05,
      "loss": 0.7542,
      "step": 825
    },
    {
      "epoch": 0.7942307692307692,
      "grad_norm": 7.831778526306152,
      "learning_rate": 1.841346153846154e-05,
      "loss": 2.2549,
      "step": 826
    },
    {
      "epoch": 0.7951923076923076,
      "grad_norm": 10.238259315490723,
      "learning_rate": 1.8411538461538463e-05,
      "loss": 0.5232,
      "step": 827
    },
    {
      "epoch": 0.7961538461538461,
      "grad_norm": 6.798902988433838,
      "learning_rate": 1.8409615384615386e-05,
      "loss": 0.8248,
      "step": 828
    },
    {
      "epoch": 0.7971153846153847,
      "grad_norm": 24.908674240112305,
      "learning_rate": 1.840769230769231e-05,
      "loss": 2.5728,
      "step": 829
    },
    {
      "epoch": 0.7980769230769231,
      "grad_norm": 6.624409198760986,
      "learning_rate": 1.8405769230769232e-05,
      "loss": 1.6705,
      "step": 830
    },
    {
      "epoch": 0.7990384615384616,
      "grad_norm": 5.745303153991699,
      "learning_rate": 1.8403846153846154e-05,
      "loss": 1.6854,
      "step": 831
    },
    {
      "epoch": 0.8,
      "grad_norm": 6.204941749572754,
      "learning_rate": 1.8401923076923077e-05,
      "loss": 1.2915,
      "step": 832
    },
    {
      "epoch": 0.8009615384615385,
      "grad_norm": 9.925548553466797,
      "learning_rate": 1.8400000000000003e-05,
      "loss": 1.7617,
      "step": 833
    },
    {
      "epoch": 0.801923076923077,
      "grad_norm": 7.165386199951172,
      "learning_rate": 1.8398076923076923e-05,
      "loss": 2.2852,
      "step": 834
    },
    {
      "epoch": 0.8028846153846154,
      "grad_norm": 6.186108589172363,
      "learning_rate": 1.839615384615385e-05,
      "loss": 1.1437,
      "step": 835
    },
    {
      "epoch": 0.8038461538461539,
      "grad_norm": 10.593451499938965,
      "learning_rate": 1.839423076923077e-05,
      "loss": 1.3741,
      "step": 836
    },
    {
      "epoch": 0.8048076923076923,
      "grad_norm": 5.768534183502197,
      "learning_rate": 1.8392307692307694e-05,
      "loss": 2.2778,
      "step": 837
    },
    {
      "epoch": 0.8057692307692308,
      "grad_norm": 9.582779884338379,
      "learning_rate": 1.8390384615384617e-05,
      "loss": 2.1598,
      "step": 838
    },
    {
      "epoch": 0.8067307692307693,
      "grad_norm": 8.749763488769531,
      "learning_rate": 1.838846153846154e-05,
      "loss": 1.3888,
      "step": 839
    },
    {
      "epoch": 0.8076923076923077,
      "grad_norm": 5.4535746574401855,
      "learning_rate": 1.8386538461538463e-05,
      "loss": 1.6192,
      "step": 840
    },
    {
      "epoch": 0.8086538461538462,
      "grad_norm": 8.475619316101074,
      "learning_rate": 1.8384615384615386e-05,
      "loss": 1.9799,
      "step": 841
    },
    {
      "epoch": 0.8096153846153846,
      "grad_norm": 10.804218292236328,
      "learning_rate": 1.838269230769231e-05,
      "loss": 0.7246,
      "step": 842
    },
    {
      "epoch": 0.8105769230769231,
      "grad_norm": 10.748221397399902,
      "learning_rate": 1.8380769230769234e-05,
      "loss": 2.4359,
      "step": 843
    },
    {
      "epoch": 0.8115384615384615,
      "grad_norm": 10.03240966796875,
      "learning_rate": 1.8378846153846154e-05,
      "loss": 0.6946,
      "step": 844
    },
    {
      "epoch": 0.8125,
      "grad_norm": 6.594086647033691,
      "learning_rate": 1.837692307692308e-05,
      "loss": 2.0248,
      "step": 845
    },
    {
      "epoch": 0.8134615384615385,
      "grad_norm": 6.819063663482666,
      "learning_rate": 1.8375e-05,
      "loss": 1.8938,
      "step": 846
    },
    {
      "epoch": 0.8144230769230769,
      "grad_norm": 7.417344093322754,
      "learning_rate": 1.8373076923076926e-05,
      "loss": 1.4795,
      "step": 847
    },
    {
      "epoch": 0.8153846153846154,
      "grad_norm": 22.81149673461914,
      "learning_rate": 1.837115384615385e-05,
      "loss": 1.9801,
      "step": 848
    },
    {
      "epoch": 0.8163461538461538,
      "grad_norm": 7.882476329803467,
      "learning_rate": 1.836923076923077e-05,
      "loss": 1.6663,
      "step": 849
    },
    {
      "epoch": 0.8173076923076923,
      "grad_norm": 8.067553520202637,
      "learning_rate": 1.8367307692307694e-05,
      "loss": 1.284,
      "step": 850
    },
    {
      "epoch": 0.8182692307692307,
      "grad_norm": 8.644529342651367,
      "learning_rate": 1.8365384615384617e-05,
      "loss": 1.997,
      "step": 851
    },
    {
      "epoch": 0.8192307692307692,
      "grad_norm": 8.288276672363281,
      "learning_rate": 1.836346153846154e-05,
      "loss": 2.1529,
      "step": 852
    },
    {
      "epoch": 0.8201923076923077,
      "grad_norm": 7.19643497467041,
      "learning_rate": 1.8361538461538466e-05,
      "loss": 0.7646,
      "step": 853
    },
    {
      "epoch": 0.8211538461538461,
      "grad_norm": 8.844974517822266,
      "learning_rate": 1.8359615384615385e-05,
      "loss": 1.2217,
      "step": 854
    },
    {
      "epoch": 0.8221153846153846,
      "grad_norm": 6.9952073097229,
      "learning_rate": 1.835769230769231e-05,
      "loss": 1.6881,
      "step": 855
    },
    {
      "epoch": 0.823076923076923,
      "grad_norm": 9.705456733703613,
      "learning_rate": 1.835576923076923e-05,
      "loss": 1.3765,
      "step": 856
    },
    {
      "epoch": 0.8240384615384615,
      "grad_norm": 7.2074079513549805,
      "learning_rate": 1.8353846153846157e-05,
      "loss": 1.4912,
      "step": 857
    },
    {
      "epoch": 0.825,
      "grad_norm": 8.163466453552246,
      "learning_rate": 1.835192307692308e-05,
      "loss": 1.8092,
      "step": 858
    },
    {
      "epoch": 0.8259615384615384,
      "grad_norm": 11.799599647521973,
      "learning_rate": 1.8350000000000002e-05,
      "loss": 1.6962,
      "step": 859
    },
    {
      "epoch": 0.8269230769230769,
      "grad_norm": 5.6378865242004395,
      "learning_rate": 1.8348076923076925e-05,
      "loss": 1.492,
      "step": 860
    },
    {
      "epoch": 0.8278846153846153,
      "grad_norm": 6.660347938537598,
      "learning_rate": 1.8346153846153848e-05,
      "loss": 1.3979,
      "step": 861
    },
    {
      "epoch": 0.8288461538461539,
      "grad_norm": 9.521269798278809,
      "learning_rate": 1.834423076923077e-05,
      "loss": 1.9128,
      "step": 862
    },
    {
      "epoch": 0.8298076923076924,
      "grad_norm": 5.4103617668151855,
      "learning_rate": 1.8342307692307693e-05,
      "loss": 0.2477,
      "step": 863
    },
    {
      "epoch": 0.8307692307692308,
      "grad_norm": 5.783692836761475,
      "learning_rate": 1.8340384615384616e-05,
      "loss": 1.3707,
      "step": 864
    },
    {
      "epoch": 0.8317307692307693,
      "grad_norm": 8.330965042114258,
      "learning_rate": 1.8338461538461542e-05,
      "loss": 0.9093,
      "step": 865
    },
    {
      "epoch": 0.8326923076923077,
      "grad_norm": 6.201071739196777,
      "learning_rate": 1.833653846153846e-05,
      "loss": 1.5267,
      "step": 866
    },
    {
      "epoch": 0.8336538461538462,
      "grad_norm": 5.564773082733154,
      "learning_rate": 1.8334615384615388e-05,
      "loss": 0.9837,
      "step": 867
    },
    {
      "epoch": 0.8346153846153846,
      "grad_norm": 7.216233730316162,
      "learning_rate": 1.833269230769231e-05,
      "loss": 1.5396,
      "step": 868
    },
    {
      "epoch": 0.8355769230769231,
      "grad_norm": 7.975352764129639,
      "learning_rate": 1.8330769230769233e-05,
      "loss": 1.5219,
      "step": 869
    },
    {
      "epoch": 0.8365384615384616,
      "grad_norm": 6.785965919494629,
      "learning_rate": 1.8328846153846156e-05,
      "loss": 0.8153,
      "step": 870
    },
    {
      "epoch": 0.8375,
      "grad_norm": 7.429891109466553,
      "learning_rate": 1.832692307692308e-05,
      "loss": 1.1762,
      "step": 871
    },
    {
      "epoch": 0.8384615384615385,
      "grad_norm": 8.645564079284668,
      "learning_rate": 1.8325e-05,
      "loss": 2.111,
      "step": 872
    },
    {
      "epoch": 0.8394230769230769,
      "grad_norm": 7.424429893493652,
      "learning_rate": 1.8323076923076924e-05,
      "loss": 1.0875,
      "step": 873
    },
    {
      "epoch": 0.8403846153846154,
      "grad_norm": 7.567651271820068,
      "learning_rate": 1.8321153846153847e-05,
      "loss": 1.7482,
      "step": 874
    },
    {
      "epoch": 0.8413461538461539,
      "grad_norm": 5.246922016143799,
      "learning_rate": 1.831923076923077e-05,
      "loss": 0.9923,
      "step": 875
    },
    {
      "epoch": 0.8423076923076923,
      "grad_norm": 9.720109939575195,
      "learning_rate": 1.8317307692307693e-05,
      "loss": 1.7934,
      "step": 876
    },
    {
      "epoch": 0.8432692307692308,
      "grad_norm": 9.572181701660156,
      "learning_rate": 1.8315384615384615e-05,
      "loss": 1.5922,
      "step": 877
    },
    {
      "epoch": 0.8442307692307692,
      "grad_norm": 7.068564414978027,
      "learning_rate": 1.831346153846154e-05,
      "loss": 2.2755,
      "step": 878
    },
    {
      "epoch": 0.8451923076923077,
      "grad_norm": 13.551358222961426,
      "learning_rate": 1.8311538461538464e-05,
      "loss": 0.2974,
      "step": 879
    },
    {
      "epoch": 0.8461538461538461,
      "grad_norm": 9.028843879699707,
      "learning_rate": 1.8309615384615387e-05,
      "loss": 0.8704,
      "step": 880
    },
    {
      "epoch": 0.8471153846153846,
      "grad_norm": 9.241432189941406,
      "learning_rate": 1.830769230769231e-05,
      "loss": 1.2698,
      "step": 881
    },
    {
      "epoch": 0.8480769230769231,
      "grad_norm": 5.56260871887207,
      "learning_rate": 1.8305769230769233e-05,
      "loss": 1.9395,
      "step": 882
    },
    {
      "epoch": 0.8490384615384615,
      "grad_norm": 8.47996997833252,
      "learning_rate": 1.8303846153846155e-05,
      "loss": 0.148,
      "step": 883
    },
    {
      "epoch": 0.85,
      "grad_norm": 7.547031402587891,
      "learning_rate": 1.8301923076923078e-05,
      "loss": 1.4601,
      "step": 884
    },
    {
      "epoch": 0.8509615384615384,
      "grad_norm": 14.568679809570312,
      "learning_rate": 1.83e-05,
      "loss": 0.5503,
      "step": 885
    },
    {
      "epoch": 0.8519230769230769,
      "grad_norm": 8.305447578430176,
      "learning_rate": 1.8298076923076924e-05,
      "loss": 1.2955,
      "step": 886
    },
    {
      "epoch": 0.8528846153846154,
      "grad_norm": 6.904132843017578,
      "learning_rate": 1.8296153846153847e-05,
      "loss": 1.3041,
      "step": 887
    },
    {
      "epoch": 0.8538461538461538,
      "grad_norm": 6.699832916259766,
      "learning_rate": 1.829423076923077e-05,
      "loss": 0.8239,
      "step": 888
    },
    {
      "epoch": 0.8548076923076923,
      "grad_norm": 8.501626968383789,
      "learning_rate": 1.8292307692307692e-05,
      "loss": 1.2784,
      "step": 889
    },
    {
      "epoch": 0.8557692307692307,
      "grad_norm": 7.181022644042969,
      "learning_rate": 1.8290384615384618e-05,
      "loss": 1.547,
      "step": 890
    },
    {
      "epoch": 0.8567307692307692,
      "grad_norm": 7.471024036407471,
      "learning_rate": 1.828846153846154e-05,
      "loss": 0.9541,
      "step": 891
    },
    {
      "epoch": 0.8576923076923076,
      "grad_norm": 9.770421028137207,
      "learning_rate": 1.8286538461538464e-05,
      "loss": 1.331,
      "step": 892
    },
    {
      "epoch": 0.8586538461538461,
      "grad_norm": 7.1986985206604,
      "learning_rate": 1.8284615384615387e-05,
      "loss": 1.4289,
      "step": 893
    },
    {
      "epoch": 0.8596153846153847,
      "grad_norm": 15.416725158691406,
      "learning_rate": 1.828269230769231e-05,
      "loss": 1.8333,
      "step": 894
    },
    {
      "epoch": 0.8605769230769231,
      "grad_norm": 64.5702133178711,
      "learning_rate": 1.8280769230769232e-05,
      "loss": 1.4773,
      "step": 895
    },
    {
      "epoch": 0.8615384615384616,
      "grad_norm": 8.685745239257812,
      "learning_rate": 1.8278846153846155e-05,
      "loss": 1.673,
      "step": 896
    },
    {
      "epoch": 0.8625,
      "grad_norm": 7.819024085998535,
      "learning_rate": 1.8276923076923078e-05,
      "loss": 0.3927,
      "step": 897
    },
    {
      "epoch": 0.8634615384615385,
      "grad_norm": 5.013309478759766,
      "learning_rate": 1.8275e-05,
      "loss": 1.7543,
      "step": 898
    },
    {
      "epoch": 0.864423076923077,
      "grad_norm": 108.04153442382812,
      "learning_rate": 1.8273076923076923e-05,
      "loss": 0.8898,
      "step": 899
    },
    {
      "epoch": 0.8653846153846154,
      "grad_norm": 9.790885925292969,
      "learning_rate": 1.827115384615385e-05,
      "loss": 1.9394,
      "step": 900
    },
    {
      "epoch": 0.8663461538461539,
      "grad_norm": 10.681418418884277,
      "learning_rate": 1.826923076923077e-05,
      "loss": 1.1944,
      "step": 901
    },
    {
      "epoch": 0.8673076923076923,
      "grad_norm": 10.723258018493652,
      "learning_rate": 1.8267307692307695e-05,
      "loss": 1.2213,
      "step": 902
    },
    {
      "epoch": 0.8682692307692308,
      "grad_norm": 9.300004005432129,
      "learning_rate": 1.8265384615384618e-05,
      "loss": 1.2137,
      "step": 903
    },
    {
      "epoch": 0.8692307692307693,
      "grad_norm": 5.13023567199707,
      "learning_rate": 1.826346153846154e-05,
      "loss": 2.209,
      "step": 904
    },
    {
      "epoch": 0.8701923076923077,
      "grad_norm": 9.736746788024902,
      "learning_rate": 1.8261538461538463e-05,
      "loss": 1.4667,
      "step": 905
    },
    {
      "epoch": 0.8711538461538462,
      "grad_norm": 7.236647129058838,
      "learning_rate": 1.8259615384615386e-05,
      "loss": 0.9428,
      "step": 906
    },
    {
      "epoch": 0.8721153846153846,
      "grad_norm": 8.842143058776855,
      "learning_rate": 1.825769230769231e-05,
      "loss": 1.5799,
      "step": 907
    },
    {
      "epoch": 0.8730769230769231,
      "grad_norm": 7.131547451019287,
      "learning_rate": 1.825576923076923e-05,
      "loss": 1.5387,
      "step": 908
    },
    {
      "epoch": 0.8740384615384615,
      "grad_norm": 5.701018810272217,
      "learning_rate": 1.8253846153846154e-05,
      "loss": 1.7172,
      "step": 909
    },
    {
      "epoch": 0.875,
      "grad_norm": 6.606546401977539,
      "learning_rate": 1.825192307692308e-05,
      "loss": 0.7898,
      "step": 910
    },
    {
      "epoch": 0.8759615384615385,
      "grad_norm": 5.928542137145996,
      "learning_rate": 1.825e-05,
      "loss": 2.0232,
      "step": 911
    },
    {
      "epoch": 0.8769230769230769,
      "grad_norm": 7.438861846923828,
      "learning_rate": 1.8248076923076926e-05,
      "loss": 1.6424,
      "step": 912
    },
    {
      "epoch": 0.8778846153846154,
      "grad_norm": 7.354311466217041,
      "learning_rate": 1.8246153846153845e-05,
      "loss": 1.6743,
      "step": 913
    },
    {
      "epoch": 0.8788461538461538,
      "grad_norm": 8.690329551696777,
      "learning_rate": 1.824423076923077e-05,
      "loss": 1.408,
      "step": 914
    },
    {
      "epoch": 0.8798076923076923,
      "grad_norm": 9.906427383422852,
      "learning_rate": 1.8242307692307694e-05,
      "loss": 1.1474,
      "step": 915
    },
    {
      "epoch": 0.8807692307692307,
      "grad_norm": 33.674400329589844,
      "learning_rate": 1.8240384615384617e-05,
      "loss": 2.035,
      "step": 916
    },
    {
      "epoch": 0.8817307692307692,
      "grad_norm": 8.041065216064453,
      "learning_rate": 1.823846153846154e-05,
      "loss": 1.0676,
      "step": 917
    },
    {
      "epoch": 0.8826923076923077,
      "grad_norm": 6.950108051300049,
      "learning_rate": 1.8236538461538463e-05,
      "loss": 1.3404,
      "step": 918
    },
    {
      "epoch": 0.8836538461538461,
      "grad_norm": 8.690134048461914,
      "learning_rate": 1.8234615384615385e-05,
      "loss": 1.8061,
      "step": 919
    },
    {
      "epoch": 0.8846153846153846,
      "grad_norm": 6.800519943237305,
      "learning_rate": 1.823269230769231e-05,
      "loss": 1.3689,
      "step": 920
    },
    {
      "epoch": 0.885576923076923,
      "grad_norm": 6.500398635864258,
      "learning_rate": 1.823076923076923e-05,
      "loss": 2.2722,
      "step": 921
    },
    {
      "epoch": 0.8865384615384615,
      "grad_norm": 30.965045928955078,
      "learning_rate": 1.8228846153846157e-05,
      "loss": 2.0091,
      "step": 922
    },
    {
      "epoch": 0.8875,
      "grad_norm": 9.808333396911621,
      "learning_rate": 1.8226923076923076e-05,
      "loss": 1.125,
      "step": 923
    },
    {
      "epoch": 0.8884615384615384,
      "grad_norm": 10.56915283203125,
      "learning_rate": 1.8225000000000003e-05,
      "loss": 0.466,
      "step": 924
    },
    {
      "epoch": 0.8894230769230769,
      "grad_norm": 6.919031620025635,
      "learning_rate": 1.8223076923076925e-05,
      "loss": 1.8826,
      "step": 925
    },
    {
      "epoch": 0.8903846153846153,
      "grad_norm": 5.597850322723389,
      "learning_rate": 1.8221153846153848e-05,
      "loss": 1.5004,
      "step": 926
    },
    {
      "epoch": 0.8913461538461539,
      "grad_norm": 6.809344291687012,
      "learning_rate": 1.821923076923077e-05,
      "loss": 0.1525,
      "step": 927
    },
    {
      "epoch": 0.8923076923076924,
      "grad_norm": 5.979877948760986,
      "learning_rate": 1.8217307692307694e-05,
      "loss": 1.8923,
      "step": 928
    },
    {
      "epoch": 0.8932692307692308,
      "grad_norm": 14.947184562683105,
      "learning_rate": 1.8215384615384616e-05,
      "loss": 1.2311,
      "step": 929
    },
    {
      "epoch": 0.8942307692307693,
      "grad_norm": 6.384877681732178,
      "learning_rate": 1.8213461538461543e-05,
      "loss": 0.7311,
      "step": 930
    },
    {
      "epoch": 0.8951923076923077,
      "grad_norm": 4.157481670379639,
      "learning_rate": 1.8211538461538462e-05,
      "loss": 1.6924,
      "step": 931
    },
    {
      "epoch": 0.8961538461538462,
      "grad_norm": 8.59062385559082,
      "learning_rate": 1.8209615384615388e-05,
      "loss": 1.0145,
      "step": 932
    },
    {
      "epoch": 0.8971153846153846,
      "grad_norm": 8.496294975280762,
      "learning_rate": 1.8207692307692307e-05,
      "loss": 1.4241,
      "step": 933
    },
    {
      "epoch": 0.8980769230769231,
      "grad_norm": 8.282783508300781,
      "learning_rate": 1.8205769230769234e-05,
      "loss": 0.787,
      "step": 934
    },
    {
      "epoch": 0.8990384615384616,
      "grad_norm": 6.177614688873291,
      "learning_rate": 1.8203846153846156e-05,
      "loss": 1.9416,
      "step": 935
    },
    {
      "epoch": 0.9,
      "grad_norm": 4.421360969543457,
      "learning_rate": 1.820192307692308e-05,
      "loss": 0.9458,
      "step": 936
    },
    {
      "epoch": 0.9009615384615385,
      "grad_norm": 7.881058216094971,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 1.8259,
      "step": 937
    },
    {
      "epoch": 0.9019230769230769,
      "grad_norm": 5.540872097015381,
      "learning_rate": 1.8198076923076925e-05,
      "loss": 1.0754,
      "step": 938
    },
    {
      "epoch": 0.9028846153846154,
      "grad_norm": 7.501441478729248,
      "learning_rate": 1.8196153846153847e-05,
      "loss": 2.308,
      "step": 939
    },
    {
      "epoch": 0.9038461538461539,
      "grad_norm": 5.777532577514648,
      "learning_rate": 1.819423076923077e-05,
      "loss": 1.8744,
      "step": 940
    },
    {
      "epoch": 0.9048076923076923,
      "grad_norm": 10.047444343566895,
      "learning_rate": 1.8192307692307693e-05,
      "loss": 2.148,
      "step": 941
    },
    {
      "epoch": 0.9057692307692308,
      "grad_norm": 10.982952117919922,
      "learning_rate": 1.819038461538462e-05,
      "loss": 1.8279,
      "step": 942
    },
    {
      "epoch": 0.9067307692307692,
      "grad_norm": 6.618565559387207,
      "learning_rate": 1.818846153846154e-05,
      "loss": 1.0789,
      "step": 943
    },
    {
      "epoch": 0.9076923076923077,
      "grad_norm": 22.259618759155273,
      "learning_rate": 1.8186538461538465e-05,
      "loss": 0.3879,
      "step": 944
    },
    {
      "epoch": 0.9086538461538461,
      "grad_norm": 10.0850248336792,
      "learning_rate": 1.8184615384615384e-05,
      "loss": 1.128,
      "step": 945
    },
    {
      "epoch": 0.9096153846153846,
      "grad_norm": 12.969038009643555,
      "learning_rate": 1.818269230769231e-05,
      "loss": 1.2482,
      "step": 946
    },
    {
      "epoch": 0.9105769230769231,
      "grad_norm": 2.4486093521118164,
      "learning_rate": 1.8180769230769233e-05,
      "loss": 0.0332,
      "step": 947
    },
    {
      "epoch": 0.9115384615384615,
      "grad_norm": 4.451214790344238,
      "learning_rate": 1.8178846153846156e-05,
      "loss": 1.6635,
      "step": 948
    },
    {
      "epoch": 0.9125,
      "grad_norm": 7.087826251983643,
      "learning_rate": 1.817692307692308e-05,
      "loss": 0.7364,
      "step": 949
    },
    {
      "epoch": 0.9134615384615384,
      "grad_norm": 8.76205825805664,
      "learning_rate": 1.8175e-05,
      "loss": 1.5851,
      "step": 950
    },
    {
      "epoch": 0.9144230769230769,
      "grad_norm": 18.394638061523438,
      "learning_rate": 1.8173076923076924e-05,
      "loss": 1.4666,
      "step": 951
    },
    {
      "epoch": 0.9153846153846154,
      "grad_norm": 4.972236633300781,
      "learning_rate": 1.8171153846153847e-05,
      "loss": 1.2495,
      "step": 952
    },
    {
      "epoch": 0.9163461538461538,
      "grad_norm": 17.653953552246094,
      "learning_rate": 1.816923076923077e-05,
      "loss": 1.6382,
      "step": 953
    },
    {
      "epoch": 0.9173076923076923,
      "grad_norm": 6.114094257354736,
      "learning_rate": 1.8167307692307692e-05,
      "loss": 2.0023,
      "step": 954
    },
    {
      "epoch": 0.9182692307692307,
      "grad_norm": 6.019101142883301,
      "learning_rate": 1.8165384615384615e-05,
      "loss": 1.5583,
      "step": 955
    },
    {
      "epoch": 0.9192307692307692,
      "grad_norm": 6.34493350982666,
      "learning_rate": 1.816346153846154e-05,
      "loss": 1.8288,
      "step": 956
    },
    {
      "epoch": 0.9201923076923076,
      "grad_norm": 5.577966690063477,
      "learning_rate": 1.8161538461538464e-05,
      "loss": 1.6588,
      "step": 957
    },
    {
      "epoch": 0.9211538461538461,
      "grad_norm": 8.636378288269043,
      "learning_rate": 1.8159615384615387e-05,
      "loss": 0.9811,
      "step": 958
    },
    {
      "epoch": 0.9221153846153847,
      "grad_norm": 5.967293739318848,
      "learning_rate": 1.815769230769231e-05,
      "loss": 0.1094,
      "step": 959
    },
    {
      "epoch": 0.9230769230769231,
      "grad_norm": 4.402787208557129,
      "learning_rate": 1.8155769230769232e-05,
      "loss": 1.7333,
      "step": 960
    },
    {
      "epoch": 0.9240384615384616,
      "grad_norm": 9.000326156616211,
      "learning_rate": 1.8153846153846155e-05,
      "loss": 1.6835,
      "step": 961
    },
    {
      "epoch": 0.925,
      "grad_norm": 6.16838264465332,
      "learning_rate": 1.8151923076923078e-05,
      "loss": 0.1992,
      "step": 962
    },
    {
      "epoch": 0.9259615384615385,
      "grad_norm": 6.453162670135498,
      "learning_rate": 1.815e-05,
      "loss": 0.9107,
      "step": 963
    },
    {
      "epoch": 0.926923076923077,
      "grad_norm": 5.56924295425415,
      "learning_rate": 1.8148076923076923e-05,
      "loss": 1.7952,
      "step": 964
    },
    {
      "epoch": 0.9278846153846154,
      "grad_norm": 5.071963787078857,
      "learning_rate": 1.8146153846153846e-05,
      "loss": 1.5408,
      "step": 965
    },
    {
      "epoch": 0.9288461538461539,
      "grad_norm": 7.441102504730225,
      "learning_rate": 1.814423076923077e-05,
      "loss": 0.6062,
      "step": 966
    },
    {
      "epoch": 0.9298076923076923,
      "grad_norm": 7.121806621551514,
      "learning_rate": 1.8142307692307695e-05,
      "loss": 1.0288,
      "step": 967
    },
    {
      "epoch": 0.9307692307692308,
      "grad_norm": 5.00275182723999,
      "learning_rate": 1.8140384615384618e-05,
      "loss": 1.6586,
      "step": 968
    },
    {
      "epoch": 0.9317307692307693,
      "grad_norm": 7.107159614562988,
      "learning_rate": 1.813846153846154e-05,
      "loss": 1.7188,
      "step": 969
    },
    {
      "epoch": 0.9326923076923077,
      "grad_norm": 10.709102630615234,
      "learning_rate": 1.8136538461538463e-05,
      "loss": 0.5664,
      "step": 970
    },
    {
      "epoch": 0.9336538461538462,
      "grad_norm": 7.639601230621338,
      "learning_rate": 1.8134615384615386e-05,
      "loss": 1.5023,
      "step": 971
    },
    {
      "epoch": 0.9346153846153846,
      "grad_norm": 5.089292526245117,
      "learning_rate": 1.813269230769231e-05,
      "loss": 0.1014,
      "step": 972
    },
    {
      "epoch": 0.9355769230769231,
      "grad_norm": 6.797979354858398,
      "learning_rate": 1.8130769230769232e-05,
      "loss": 1.5364,
      "step": 973
    },
    {
      "epoch": 0.9365384615384615,
      "grad_norm": 7.465998649597168,
      "learning_rate": 1.8128846153846155e-05,
      "loss": 1.4742,
      "step": 974
    },
    {
      "epoch": 0.9375,
      "grad_norm": 8.770926475524902,
      "learning_rate": 1.8126923076923077e-05,
      "loss": 1.3856,
      "step": 975
    },
    {
      "epoch": 0.9384615384615385,
      "grad_norm": 5.407885551452637,
      "learning_rate": 1.8125e-05,
      "loss": 1.2249,
      "step": 976
    },
    {
      "epoch": 0.9394230769230769,
      "grad_norm": 11.710466384887695,
      "learning_rate": 1.8123076923076926e-05,
      "loss": 0.1969,
      "step": 977
    },
    {
      "epoch": 0.9403846153846154,
      "grad_norm": 7.5702033042907715,
      "learning_rate": 1.8121153846153846e-05,
      "loss": 1.3858,
      "step": 978
    },
    {
      "epoch": 0.9413461538461538,
      "grad_norm": 6.76817512512207,
      "learning_rate": 1.8119230769230772e-05,
      "loss": 0.5596,
      "step": 979
    },
    {
      "epoch": 0.9423076923076923,
      "grad_norm": 10.44013786315918,
      "learning_rate": 1.8117307692307695e-05,
      "loss": 1.4672,
      "step": 980
    },
    {
      "epoch": 0.9432692307692307,
      "grad_norm": 9.187131881713867,
      "learning_rate": 1.8115384615384617e-05,
      "loss": 1.0459,
      "step": 981
    },
    {
      "epoch": 0.9442307692307692,
      "grad_norm": 6.0683794021606445,
      "learning_rate": 1.811346153846154e-05,
      "loss": 0.9119,
      "step": 982
    },
    {
      "epoch": 0.9451923076923077,
      "grad_norm": 6.995528697967529,
      "learning_rate": 1.8111538461538463e-05,
      "loss": 1.8613,
      "step": 983
    },
    {
      "epoch": 0.9461538461538461,
      "grad_norm": 7.6237664222717285,
      "learning_rate": 1.8109615384615386e-05,
      "loss": 2.1007,
      "step": 984
    },
    {
      "epoch": 0.9471153846153846,
      "grad_norm": 18.48293685913086,
      "learning_rate": 1.810769230769231e-05,
      "loss": 1.1143,
      "step": 985
    },
    {
      "epoch": 0.948076923076923,
      "grad_norm": 9.91415786743164,
      "learning_rate": 1.810576923076923e-05,
      "loss": 1.3299,
      "step": 986
    },
    {
      "epoch": 0.9490384615384615,
      "grad_norm": 7.936708927154541,
      "learning_rate": 1.8103846153846157e-05,
      "loss": 1.4816,
      "step": 987
    },
    {
      "epoch": 0.95,
      "grad_norm": 11.92570686340332,
      "learning_rate": 1.8101923076923077e-05,
      "loss": 0.7611,
      "step": 988
    },
    {
      "epoch": 0.9509615384615384,
      "grad_norm": 7.073308944702148,
      "learning_rate": 1.8100000000000003e-05,
      "loss": 1.2769,
      "step": 989
    },
    {
      "epoch": 0.9519230769230769,
      "grad_norm": 12.916284561157227,
      "learning_rate": 1.8098076923076922e-05,
      "loss": 1.6348,
      "step": 990
    },
    {
      "epoch": 0.9528846153846153,
      "grad_norm": 10.656071662902832,
      "learning_rate": 1.809615384615385e-05,
      "loss": 1.4795,
      "step": 991
    },
    {
      "epoch": 0.9538461538461539,
      "grad_norm": 6.848259925842285,
      "learning_rate": 1.809423076923077e-05,
      "loss": 1.722,
      "step": 992
    },
    {
      "epoch": 0.9548076923076924,
      "grad_norm": 11.148494720458984,
      "learning_rate": 1.8092307692307694e-05,
      "loss": 1.5706,
      "step": 993
    },
    {
      "epoch": 0.9557692307692308,
      "grad_norm": 5.996086120605469,
      "learning_rate": 1.8090384615384617e-05,
      "loss": 1.3901,
      "step": 994
    },
    {
      "epoch": 0.9567307692307693,
      "grad_norm": 8.350056648254395,
      "learning_rate": 1.808846153846154e-05,
      "loss": 1.7767,
      "step": 995
    },
    {
      "epoch": 0.9576923076923077,
      "grad_norm": 11.208409309387207,
      "learning_rate": 1.8086538461538462e-05,
      "loss": 1.2193,
      "step": 996
    },
    {
      "epoch": 0.9586538461538462,
      "grad_norm": 7.527879238128662,
      "learning_rate": 1.808461538461539e-05,
      "loss": 1.8168,
      "step": 997
    },
    {
      "epoch": 0.9596153846153846,
      "grad_norm": 17.0919189453125,
      "learning_rate": 1.8082692307692308e-05,
      "loss": 0.9246,
      "step": 998
    },
    {
      "epoch": 0.9605769230769231,
      "grad_norm": 6.053404808044434,
      "learning_rate": 1.8080769230769234e-05,
      "loss": 2.1207,
      "step": 999
    },
    {
      "epoch": 0.9615384615384616,
      "grad_norm": 6.528761386871338,
      "learning_rate": 1.8078846153846153e-05,
      "loss": 0.2419,
      "step": 1000
    },
    {
      "epoch": 0.9625,
      "grad_norm": 12.0369291305542,
      "learning_rate": 1.807692307692308e-05,
      "loss": 1.2168,
      "step": 1001
    },
    {
      "epoch": 0.9634615384615385,
      "grad_norm": 5.5797624588012695,
      "learning_rate": 1.8075000000000002e-05,
      "loss": 1.811,
      "step": 1002
    },
    {
      "epoch": 0.9644230769230769,
      "grad_norm": 10.348000526428223,
      "learning_rate": 1.8073076923076925e-05,
      "loss": 0.1433,
      "step": 1003
    },
    {
      "epoch": 0.9653846153846154,
      "grad_norm": 10.275176048278809,
      "learning_rate": 1.8071153846153848e-05,
      "loss": 1.3007,
      "step": 1004
    },
    {
      "epoch": 0.9663461538461539,
      "grad_norm": 11.194792747497559,
      "learning_rate": 1.806923076923077e-05,
      "loss": 2.0246,
      "step": 1005
    },
    {
      "epoch": 0.9673076923076923,
      "grad_norm": 19.29010772705078,
      "learning_rate": 1.8067307692307693e-05,
      "loss": 1.4399,
      "step": 1006
    },
    {
      "epoch": 0.9682692307692308,
      "grad_norm": 2.1890761852264404,
      "learning_rate": 1.806538461538462e-05,
      "loss": 0.0495,
      "step": 1007
    },
    {
      "epoch": 0.9692307692307692,
      "grad_norm": 5.7311811447143555,
      "learning_rate": 1.806346153846154e-05,
      "loss": 1.7509,
      "step": 1008
    },
    {
      "epoch": 0.9701923076923077,
      "grad_norm": 9.347997665405273,
      "learning_rate": 1.8061538461538465e-05,
      "loss": 1.8129,
      "step": 1009
    },
    {
      "epoch": 0.9711538461538461,
      "grad_norm": 12.404438972473145,
      "learning_rate": 1.8059615384615384e-05,
      "loss": 0.3376,
      "step": 1010
    },
    {
      "epoch": 0.9721153846153846,
      "grad_norm": 6.694463729858398,
      "learning_rate": 1.805769230769231e-05,
      "loss": 1.7288,
      "step": 1011
    },
    {
      "epoch": 0.9730769230769231,
      "grad_norm": 7.7801055908203125,
      "learning_rate": 1.805576923076923e-05,
      "loss": 1.1692,
      "step": 1012
    },
    {
      "epoch": 0.9740384615384615,
      "grad_norm": 6.551296710968018,
      "learning_rate": 1.8053846153846156e-05,
      "loss": 1.6396,
      "step": 1013
    },
    {
      "epoch": 0.975,
      "grad_norm": 7.464986801147461,
      "learning_rate": 1.805192307692308e-05,
      "loss": 1.645,
      "step": 1014
    },
    {
      "epoch": 0.9759615384615384,
      "grad_norm": 8.146910667419434,
      "learning_rate": 1.805e-05,
      "loss": 1.0142,
      "step": 1015
    },
    {
      "epoch": 0.9769230769230769,
      "grad_norm": 5.434017658233643,
      "learning_rate": 1.8048076923076924e-05,
      "loss": 0.9151,
      "step": 1016
    },
    {
      "epoch": 0.9778846153846154,
      "grad_norm": 11.459763526916504,
      "learning_rate": 1.8046153846153847e-05,
      "loss": 2.114,
      "step": 1017
    },
    {
      "epoch": 0.9788461538461538,
      "grad_norm": 7.757121562957764,
      "learning_rate": 1.804423076923077e-05,
      "loss": 1.8052,
      "step": 1018
    },
    {
      "epoch": 0.9798076923076923,
      "grad_norm": 3.2602412700653076,
      "learning_rate": 1.8042307692307696e-05,
      "loss": 0.0492,
      "step": 1019
    },
    {
      "epoch": 0.9807692307692307,
      "grad_norm": 4.995337963104248,
      "learning_rate": 1.8040384615384616e-05,
      "loss": 1.6445,
      "step": 1020
    },
    {
      "epoch": 0.9817307692307692,
      "grad_norm": 5.777514457702637,
      "learning_rate": 1.803846153846154e-05,
      "loss": 1.9534,
      "step": 1021
    },
    {
      "epoch": 0.9826923076923076,
      "grad_norm": 13.863417625427246,
      "learning_rate": 1.803653846153846e-05,
      "loss": 1.3411,
      "step": 1022
    },
    {
      "epoch": 0.9836538461538461,
      "grad_norm": 11.277283668518066,
      "learning_rate": 1.8034615384615387e-05,
      "loss": 1.1312,
      "step": 1023
    },
    {
      "epoch": 0.9846153846153847,
      "grad_norm": 7.7258687019348145,
      "learning_rate": 1.803269230769231e-05,
      "loss": 1.9724,
      "step": 1024
    },
    {
      "epoch": 0.9855769230769231,
      "grad_norm": 6.2039666175842285,
      "learning_rate": 1.8030769230769233e-05,
      "loss": 1.673,
      "step": 1025
    },
    {
      "epoch": 0.9865384615384616,
      "grad_norm": 8.336957931518555,
      "learning_rate": 1.8028846153846156e-05,
      "loss": 2.1082,
      "step": 1026
    },
    {
      "epoch": 0.9875,
      "grad_norm": 9.627035140991211,
      "learning_rate": 1.8026923076923078e-05,
      "loss": 1.1725,
      "step": 1027
    },
    {
      "epoch": 0.9884615384615385,
      "grad_norm": 8.148213386535645,
      "learning_rate": 1.8025e-05,
      "loss": 1.3195,
      "step": 1028
    },
    {
      "epoch": 0.989423076923077,
      "grad_norm": 7.224534511566162,
      "learning_rate": 1.8023076923076924e-05,
      "loss": 1.655,
      "step": 1029
    },
    {
      "epoch": 0.9903846153846154,
      "grad_norm": 10.319259643554688,
      "learning_rate": 1.8021153846153847e-05,
      "loss": 1.1289,
      "step": 1030
    },
    {
      "epoch": 0.9913461538461539,
      "grad_norm": 6.582923889160156,
      "learning_rate": 1.8019230769230773e-05,
      "loss": 1.7454,
      "step": 1031
    },
    {
      "epoch": 0.9923076923076923,
      "grad_norm": 8.887337684631348,
      "learning_rate": 1.8017307692307692e-05,
      "loss": 1.7758,
      "step": 1032
    },
    {
      "epoch": 0.9932692307692308,
      "grad_norm": 6.776817798614502,
      "learning_rate": 1.8015384615384618e-05,
      "loss": 1.231,
      "step": 1033
    },
    {
      "epoch": 0.9942307692307693,
      "grad_norm": 5.53033447265625,
      "learning_rate": 1.801346153846154e-05,
      "loss": 1.5387,
      "step": 1034
    },
    {
      "epoch": 0.9951923076923077,
      "grad_norm": 9.368098258972168,
      "learning_rate": 1.8011538461538464e-05,
      "loss": 0.8982,
      "step": 1035
    },
    {
      "epoch": 0.9961538461538462,
      "grad_norm": 7.4292988777160645,
      "learning_rate": 1.8009615384615387e-05,
      "loss": 2.2221,
      "step": 1036
    },
    {
      "epoch": 0.9971153846153846,
      "grad_norm": 17.681730270385742,
      "learning_rate": 1.800769230769231e-05,
      "loss": 1.1118,
      "step": 1037
    },
    {
      "epoch": 0.9980769230769231,
      "grad_norm": 7.568525314331055,
      "learning_rate": 1.8005769230769232e-05,
      "loss": 1.2366,
      "step": 1038
    },
    {
      "epoch": 0.9990384615384615,
      "grad_norm": 7.917413711547852,
      "learning_rate": 1.8003846153846155e-05,
      "loss": 1.7911,
      "step": 1039
    },
    {
      "epoch": 1.0,
      "grad_norm": 11.230875968933105,
      "learning_rate": 1.8001923076923078e-05,
      "loss": 1.5678,
      "step": 1040
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.1389212608337402,
      "eval_runtime": 22.7995,
      "eval_samples_per_second": 10.132,
      "eval_steps_per_second": 10.132,
      "step": 1040
    },
    {
      "epoch": 1.0009615384615385,
      "grad_norm": 7.083648204803467,
      "learning_rate": 1.8e-05,
      "loss": 0.8644,
      "step": 1041
    },
    {
      "epoch": 1.001923076923077,
      "grad_norm": 5.951417446136475,
      "learning_rate": 1.7998076923076923e-05,
      "loss": 1.449,
      "step": 1042
    },
    {
      "epoch": 1.0028846153846154,
      "grad_norm": 5.38124418258667,
      "learning_rate": 1.7996153846153846e-05,
      "loss": 1.3895,
      "step": 1043
    },
    {
      "epoch": 1.0038461538461538,
      "grad_norm": 5.544604301452637,
      "learning_rate": 1.7994230769230772e-05,
      "loss": 1.2206,
      "step": 1044
    },
    {
      "epoch": 1.0048076923076923,
      "grad_norm": 6.783005237579346,
      "learning_rate": 1.7992307692307695e-05,
      "loss": 2.107,
      "step": 1045
    },
    {
      "epoch": 1.0057692307692307,
      "grad_norm": 4.641291618347168,
      "learning_rate": 1.7990384615384618e-05,
      "loss": 1.0985,
      "step": 1046
    },
    {
      "epoch": 1.0067307692307692,
      "grad_norm": 0.3570435047149658,
      "learning_rate": 1.798846153846154e-05,
      "loss": 0.0057,
      "step": 1047
    },
    {
      "epoch": 1.0076923076923077,
      "grad_norm": 7.651994228363037,
      "learning_rate": 1.7986538461538463e-05,
      "loss": 1.477,
      "step": 1048
    },
    {
      "epoch": 1.0086538461538461,
      "grad_norm": 5.571289539337158,
      "learning_rate": 1.7984615384615386e-05,
      "loss": 1.9018,
      "step": 1049
    },
    {
      "epoch": 1.0096153846153846,
      "grad_norm": 6.136453151702881,
      "learning_rate": 1.798269230769231e-05,
      "loss": 1.5909,
      "step": 1050
    },
    {
      "epoch": 1.010576923076923,
      "grad_norm": 5.901826858520508,
      "learning_rate": 1.798076923076923e-05,
      "loss": 0.7901,
      "step": 1051
    },
    {
      "epoch": 1.0115384615384615,
      "grad_norm": 10.526835441589355,
      "learning_rate": 1.7978846153846154e-05,
      "loss": 1.1543,
      "step": 1052
    },
    {
      "epoch": 1.0125,
      "grad_norm": 6.229038715362549,
      "learning_rate": 1.7976923076923077e-05,
      "loss": 1.9282,
      "step": 1053
    },
    {
      "epoch": 1.0134615384615384,
      "grad_norm": 4.857829570770264,
      "learning_rate": 1.7975000000000003e-05,
      "loss": 0.7653,
      "step": 1054
    },
    {
      "epoch": 1.0144230769230769,
      "grad_norm": 5.000843048095703,
      "learning_rate": 1.7973076923076923e-05,
      "loss": 1.2944,
      "step": 1055
    },
    {
      "epoch": 1.0153846153846153,
      "grad_norm": 5.880601406097412,
      "learning_rate": 1.797115384615385e-05,
      "loss": 1.6663,
      "step": 1056
    },
    {
      "epoch": 1.0163461538461538,
      "grad_norm": 45.35662841796875,
      "learning_rate": 1.796923076923077e-05,
      "loss": 1.7182,
      "step": 1057
    },
    {
      "epoch": 1.0173076923076922,
      "grad_norm": 10.006474494934082,
      "learning_rate": 1.7967307692307694e-05,
      "loss": 1.5144,
      "step": 1058
    },
    {
      "epoch": 1.0182692307692307,
      "grad_norm": 12.224428176879883,
      "learning_rate": 1.7965384615384617e-05,
      "loss": 0.129,
      "step": 1059
    },
    {
      "epoch": 1.0192307692307692,
      "grad_norm": 7.437097072601318,
      "learning_rate": 1.796346153846154e-05,
      "loss": 0.6245,
      "step": 1060
    },
    {
      "epoch": 1.0201923076923076,
      "grad_norm": 7.137476444244385,
      "learning_rate": 1.7961538461538463e-05,
      "loss": 1.5762,
      "step": 1061
    },
    {
      "epoch": 1.021153846153846,
      "grad_norm": 6.571751594543457,
      "learning_rate": 1.7959615384615385e-05,
      "loss": 0.1475,
      "step": 1062
    },
    {
      "epoch": 1.0221153846153845,
      "grad_norm": 7.358322620391846,
      "learning_rate": 1.7957692307692308e-05,
      "loss": 0.9681,
      "step": 1063
    },
    {
      "epoch": 1.023076923076923,
      "grad_norm": 8.484598159790039,
      "learning_rate": 1.7955769230769234e-05,
      "loss": 1.3185,
      "step": 1064
    },
    {
      "epoch": 1.0240384615384615,
      "grad_norm": 9.439947128295898,
      "learning_rate": 1.7953846153846154e-05,
      "loss": 1.4478,
      "step": 1065
    },
    {
      "epoch": 1.025,
      "grad_norm": 10.634114265441895,
      "learning_rate": 1.795192307692308e-05,
      "loss": 1.0662,
      "step": 1066
    },
    {
      "epoch": 1.0259615384615384,
      "grad_norm": 7.8901190757751465,
      "learning_rate": 1.795e-05,
      "loss": 0.6127,
      "step": 1067
    },
    {
      "epoch": 1.0269230769230768,
      "grad_norm": 9.786438941955566,
      "learning_rate": 1.7948076923076925e-05,
      "loss": 1.5948,
      "step": 1068
    },
    {
      "epoch": 1.0278846153846153,
      "grad_norm": 7.627085208892822,
      "learning_rate": 1.7946153846153848e-05,
      "loss": 1.8809,
      "step": 1069
    },
    {
      "epoch": 1.0288461538461537,
      "grad_norm": 7.710484981536865,
      "learning_rate": 1.794423076923077e-05,
      "loss": 1.3942,
      "step": 1070
    },
    {
      "epoch": 1.0298076923076922,
      "grad_norm": 5.711822032928467,
      "learning_rate": 1.7942307692307694e-05,
      "loss": 1.0561,
      "step": 1071
    },
    {
      "epoch": 1.0307692307692307,
      "grad_norm": 9.101350784301758,
      "learning_rate": 1.7940384615384616e-05,
      "loss": 2.3025,
      "step": 1072
    },
    {
      "epoch": 1.0317307692307693,
      "grad_norm": 6.168198585510254,
      "learning_rate": 1.793846153846154e-05,
      "loss": 0.8852,
      "step": 1073
    },
    {
      "epoch": 1.0326923076923078,
      "grad_norm": 6.005435943603516,
      "learning_rate": 1.7936538461538462e-05,
      "loss": 1.4758,
      "step": 1074
    },
    {
      "epoch": 1.0336538461538463,
      "grad_norm": 8.189194679260254,
      "learning_rate": 1.7934615384615385e-05,
      "loss": 1.7938,
      "step": 1075
    },
    {
      "epoch": 1.0346153846153847,
      "grad_norm": 5.910791873931885,
      "learning_rate": 1.793269230769231e-05,
      "loss": 0.9941,
      "step": 1076
    },
    {
      "epoch": 1.0355769230769232,
      "grad_norm": 5.19244384765625,
      "learning_rate": 1.793076923076923e-05,
      "loss": 1.2745,
      "step": 1077
    },
    {
      "epoch": 1.0365384615384616,
      "grad_norm": 5.950710296630859,
      "learning_rate": 1.7928846153846156e-05,
      "loss": 1.7543,
      "step": 1078
    },
    {
      "epoch": 1.0375,
      "grad_norm": 5.607781410217285,
      "learning_rate": 1.7926923076923076e-05,
      "loss": 1.0501,
      "step": 1079
    },
    {
      "epoch": 1.0384615384615385,
      "grad_norm": 4.794894218444824,
      "learning_rate": 1.7925000000000002e-05,
      "loss": 0.6504,
      "step": 1080
    },
    {
      "epoch": 1.039423076923077,
      "grad_norm": 6.875726222991943,
      "learning_rate": 1.7923076923076925e-05,
      "loss": 1.091,
      "step": 1081
    },
    {
      "epoch": 1.0403846153846155,
      "grad_norm": 7.644294261932373,
      "learning_rate": 1.7921153846153848e-05,
      "loss": 0.9368,
      "step": 1082
    },
    {
      "epoch": 1.041346153846154,
      "grad_norm": 7.204314708709717,
      "learning_rate": 1.791923076923077e-05,
      "loss": 1.1485,
      "step": 1083
    },
    {
      "epoch": 1.0423076923076924,
      "grad_norm": 7.476468563079834,
      "learning_rate": 1.7917307692307693e-05,
      "loss": 1.3831,
      "step": 1084
    },
    {
      "epoch": 1.0432692307692308,
      "grad_norm": 8.285502433776855,
      "learning_rate": 1.7915384615384616e-05,
      "loss": 2.2784,
      "step": 1085
    },
    {
      "epoch": 1.0442307692307693,
      "grad_norm": 6.373882293701172,
      "learning_rate": 1.7913461538461542e-05,
      "loss": 1.577,
      "step": 1086
    },
    {
      "epoch": 1.0451923076923078,
      "grad_norm": 5.228020191192627,
      "learning_rate": 1.791153846153846e-05,
      "loss": 0.831,
      "step": 1087
    },
    {
      "epoch": 1.0461538461538462,
      "grad_norm": 6.721907615661621,
      "learning_rate": 1.7909615384615388e-05,
      "loss": 1.3477,
      "step": 1088
    },
    {
      "epoch": 1.0471153846153847,
      "grad_norm": 6.020115375518799,
      "learning_rate": 1.7907692307692307e-05,
      "loss": 1.6282,
      "step": 1089
    },
    {
      "epoch": 1.0480769230769231,
      "grad_norm": 9.187471389770508,
      "learning_rate": 1.7905769230769233e-05,
      "loss": 0.3414,
      "step": 1090
    },
    {
      "epoch": 1.0490384615384616,
      "grad_norm": 6.395987033843994,
      "learning_rate": 1.7903846153846156e-05,
      "loss": 1.3068,
      "step": 1091
    },
    {
      "epoch": 1.05,
      "grad_norm": 9.02353572845459,
      "learning_rate": 1.790192307692308e-05,
      "loss": 1.1826,
      "step": 1092
    },
    {
      "epoch": 1.0509615384615385,
      "grad_norm": 7.792054176330566,
      "learning_rate": 1.79e-05,
      "loss": 1.4994,
      "step": 1093
    },
    {
      "epoch": 1.051923076923077,
      "grad_norm": 5.857453346252441,
      "learning_rate": 1.7898076923076924e-05,
      "loss": 1.3845,
      "step": 1094
    },
    {
      "epoch": 1.0528846153846154,
      "grad_norm": 12.49850845336914,
      "learning_rate": 1.7896153846153847e-05,
      "loss": 1.5812,
      "step": 1095
    },
    {
      "epoch": 1.0538461538461539,
      "grad_norm": 6.893025875091553,
      "learning_rate": 1.7894230769230773e-05,
      "loss": 1.6626,
      "step": 1096
    },
    {
      "epoch": 1.0548076923076923,
      "grad_norm": 7.307709693908691,
      "learning_rate": 1.7892307692307692e-05,
      "loss": 1.681,
      "step": 1097
    },
    {
      "epoch": 1.0557692307692308,
      "grad_norm": 40.667999267578125,
      "learning_rate": 1.789038461538462e-05,
      "loss": 1.0416,
      "step": 1098
    },
    {
      "epoch": 1.0567307692307693,
      "grad_norm": 13.575267791748047,
      "learning_rate": 1.7888461538461538e-05,
      "loss": 0.4656,
      "step": 1099
    },
    {
      "epoch": 1.0576923076923077,
      "grad_norm": 7.294530391693115,
      "learning_rate": 1.7886538461538464e-05,
      "loss": 1.4219,
      "step": 1100
    },
    {
      "epoch": 1.0586538461538462,
      "grad_norm": 10.998120307922363,
      "learning_rate": 1.7884615384615387e-05,
      "loss": 1.5282,
      "step": 1101
    },
    {
      "epoch": 1.0596153846153846,
      "grad_norm": 5.104761123657227,
      "learning_rate": 1.788269230769231e-05,
      "loss": 0.5999,
      "step": 1102
    },
    {
      "epoch": 1.060576923076923,
      "grad_norm": 6.572075366973877,
      "learning_rate": 1.7880769230769232e-05,
      "loss": 1.2746,
      "step": 1103
    },
    {
      "epoch": 1.0615384615384615,
      "grad_norm": 6.829624652862549,
      "learning_rate": 1.7878846153846155e-05,
      "loss": 1.4574,
      "step": 1104
    },
    {
      "epoch": 1.0625,
      "grad_norm": 7.5128092765808105,
      "learning_rate": 1.7876923076923078e-05,
      "loss": 1.4435,
      "step": 1105
    },
    {
      "epoch": 1.0634615384615385,
      "grad_norm": 5.649197101593018,
      "learning_rate": 1.7875e-05,
      "loss": 1.0044,
      "step": 1106
    },
    {
      "epoch": 1.064423076923077,
      "grad_norm": 10.788466453552246,
      "learning_rate": 1.7873076923076924e-05,
      "loss": 0.6445,
      "step": 1107
    },
    {
      "epoch": 1.0653846153846154,
      "grad_norm": 10.886199951171875,
      "learning_rate": 1.787115384615385e-05,
      "loss": 0.7978,
      "step": 1108
    },
    {
      "epoch": 1.0663461538461538,
      "grad_norm": 7.013319492340088,
      "learning_rate": 1.786923076923077e-05,
      "loss": 0.8254,
      "step": 1109
    },
    {
      "epoch": 1.0673076923076923,
      "grad_norm": 5.3484039306640625,
      "learning_rate": 1.7867307692307695e-05,
      "loss": 1.7628,
      "step": 1110
    },
    {
      "epoch": 1.0682692307692307,
      "grad_norm": 3.304379940032959,
      "learning_rate": 1.7865384615384618e-05,
      "loss": 0.053,
      "step": 1111
    },
    {
      "epoch": 1.0692307692307692,
      "grad_norm": 7.389278888702393,
      "learning_rate": 1.786346153846154e-05,
      "loss": 0.7842,
      "step": 1112
    },
    {
      "epoch": 1.0701923076923077,
      "grad_norm": 6.85060977935791,
      "learning_rate": 1.7861538461538464e-05,
      "loss": 1.7197,
      "step": 1113
    },
    {
      "epoch": 1.0711538461538461,
      "grad_norm": 7.190740585327148,
      "learning_rate": 1.7859615384615386e-05,
      "loss": 1.4392,
      "step": 1114
    },
    {
      "epoch": 1.0721153846153846,
      "grad_norm": 4.807882308959961,
      "learning_rate": 1.785769230769231e-05,
      "loss": 1.391,
      "step": 1115
    },
    {
      "epoch": 1.073076923076923,
      "grad_norm": 6.85056209564209,
      "learning_rate": 1.7855769230769232e-05,
      "loss": 0.8915,
      "step": 1116
    },
    {
      "epoch": 1.0740384615384615,
      "grad_norm": 7.010766506195068,
      "learning_rate": 1.7853846153846155e-05,
      "loss": 1.0827,
      "step": 1117
    },
    {
      "epoch": 1.075,
      "grad_norm": 7.936859130859375,
      "learning_rate": 1.7851923076923077e-05,
      "loss": 0.5473,
      "step": 1118
    },
    {
      "epoch": 1.0759615384615384,
      "grad_norm": 5.992210865020752,
      "learning_rate": 1.785e-05,
      "loss": 1.9178,
      "step": 1119
    },
    {
      "epoch": 1.0769230769230769,
      "grad_norm": 8.201196670532227,
      "learning_rate": 1.7848076923076923e-05,
      "loss": 1.4769,
      "step": 1120
    },
    {
      "epoch": 1.0778846153846153,
      "grad_norm": 5.90748405456543,
      "learning_rate": 1.784615384615385e-05,
      "loss": 1.2703,
      "step": 1121
    },
    {
      "epoch": 1.0788461538461538,
      "grad_norm": 9.028183937072754,
      "learning_rate": 1.7844230769230772e-05,
      "loss": 1.3762,
      "step": 1122
    },
    {
      "epoch": 1.0798076923076922,
      "grad_norm": 6.441413879394531,
      "learning_rate": 1.7842307692307695e-05,
      "loss": 1.6347,
      "step": 1123
    },
    {
      "epoch": 1.0807692307692307,
      "grad_norm": 7.1501641273498535,
      "learning_rate": 1.7840384615384617e-05,
      "loss": 0.7742,
      "step": 1124
    },
    {
      "epoch": 1.0817307692307692,
      "grad_norm": 8.938201904296875,
      "learning_rate": 1.783846153846154e-05,
      "loss": 1.2682,
      "step": 1125
    },
    {
      "epoch": 1.0826923076923076,
      "grad_norm": 6.526290416717529,
      "learning_rate": 1.7836538461538463e-05,
      "loss": 0.5033,
      "step": 1126
    },
    {
      "epoch": 1.083653846153846,
      "grad_norm": 8.216402053833008,
      "learning_rate": 1.7834615384615386e-05,
      "loss": 1.1444,
      "step": 1127
    },
    {
      "epoch": 1.0846153846153845,
      "grad_norm": 8.089051246643066,
      "learning_rate": 1.783269230769231e-05,
      "loss": 1.539,
      "step": 1128
    },
    {
      "epoch": 1.085576923076923,
      "grad_norm": 7.717517375946045,
      "learning_rate": 1.783076923076923e-05,
      "loss": 1.0888,
      "step": 1129
    },
    {
      "epoch": 1.0865384615384615,
      "grad_norm": 5.843758583068848,
      "learning_rate": 1.7828846153846154e-05,
      "loss": 1.2298,
      "step": 1130
    },
    {
      "epoch": 1.0875,
      "grad_norm": 7.009703636169434,
      "learning_rate": 1.782692307692308e-05,
      "loss": 1.2906,
      "step": 1131
    },
    {
      "epoch": 1.0884615384615384,
      "grad_norm": 10.207712173461914,
      "learning_rate": 1.7825e-05,
      "loss": 0.4596,
      "step": 1132
    },
    {
      "epoch": 1.0894230769230768,
      "grad_norm": 7.812712669372559,
      "learning_rate": 1.7823076923076926e-05,
      "loss": 0.3683,
      "step": 1133
    },
    {
      "epoch": 1.0903846153846153,
      "grad_norm": 7.36474084854126,
      "learning_rate": 1.782115384615385e-05,
      "loss": 0.1821,
      "step": 1134
    },
    {
      "epoch": 1.0913461538461537,
      "grad_norm": 7.766171932220459,
      "learning_rate": 1.781923076923077e-05,
      "loss": 1.0996,
      "step": 1135
    },
    {
      "epoch": 1.0923076923076924,
      "grad_norm": 5.909807205200195,
      "learning_rate": 1.7817307692307694e-05,
      "loss": 1.6743,
      "step": 1136
    },
    {
      "epoch": 1.0932692307692307,
      "grad_norm": 6.480469226837158,
      "learning_rate": 1.7815384615384617e-05,
      "loss": 1.3425,
      "step": 1137
    },
    {
      "epoch": 1.0942307692307693,
      "grad_norm": 7.276418209075928,
      "learning_rate": 1.781346153846154e-05,
      "loss": 1.1979,
      "step": 1138
    },
    {
      "epoch": 1.0951923076923076,
      "grad_norm": 12.523333549499512,
      "learning_rate": 1.7811538461538462e-05,
      "loss": 1.3426,
      "step": 1139
    },
    {
      "epoch": 1.0961538461538463,
      "grad_norm": 13.619963645935059,
      "learning_rate": 1.7809615384615385e-05,
      "loss": 1.5076,
      "step": 1140
    },
    {
      "epoch": 1.0971153846153847,
      "grad_norm": 18.336793899536133,
      "learning_rate": 1.7807692307692308e-05,
      "loss": 1.5585,
      "step": 1141
    },
    {
      "epoch": 1.0980769230769232,
      "grad_norm": 7.086999893188477,
      "learning_rate": 1.780576923076923e-05,
      "loss": 1.6126,
      "step": 1142
    },
    {
      "epoch": 1.0990384615384616,
      "grad_norm": 7.41251277923584,
      "learning_rate": 1.7803846153846157e-05,
      "loss": 1.6191,
      "step": 1143
    },
    {
      "epoch": 1.1,
      "grad_norm": 11.375904083251953,
      "learning_rate": 1.7801923076923076e-05,
      "loss": 0.7317,
      "step": 1144
    },
    {
      "epoch": 1.1009615384615385,
      "grad_norm": 13.496355056762695,
      "learning_rate": 1.7800000000000002e-05,
      "loss": 0.2461,
      "step": 1145
    },
    {
      "epoch": 1.101923076923077,
      "grad_norm": 10.533581733703613,
      "learning_rate": 1.7798076923076925e-05,
      "loss": 1.2664,
      "step": 1146
    },
    {
      "epoch": 1.1028846153846155,
      "grad_norm": 7.595422744750977,
      "learning_rate": 1.7796153846153848e-05,
      "loss": 1.318,
      "step": 1147
    },
    {
      "epoch": 1.103846153846154,
      "grad_norm": 7.738350868225098,
      "learning_rate": 1.779423076923077e-05,
      "loss": 1.4184,
      "step": 1148
    },
    {
      "epoch": 1.1048076923076924,
      "grad_norm": 9.587121963500977,
      "learning_rate": 1.7792307692307693e-05,
      "loss": 1.0478,
      "step": 1149
    },
    {
      "epoch": 1.1057692307692308,
      "grad_norm": 7.042441368103027,
      "learning_rate": 1.7790384615384616e-05,
      "loss": 1.1396,
      "step": 1150
    },
    {
      "epoch": 1.1067307692307693,
      "grad_norm": 7.837393283843994,
      "learning_rate": 1.778846153846154e-05,
      "loss": 1.2986,
      "step": 1151
    },
    {
      "epoch": 1.1076923076923078,
      "grad_norm": 7.158184051513672,
      "learning_rate": 1.7786538461538462e-05,
      "loss": 0.3267,
      "step": 1152
    },
    {
      "epoch": 1.1086538461538462,
      "grad_norm": 8.221412658691406,
      "learning_rate": 1.7784615384615388e-05,
      "loss": 0.27,
      "step": 1153
    },
    {
      "epoch": 1.1096153846153847,
      "grad_norm": 8.951529502868652,
      "learning_rate": 1.7782692307692307e-05,
      "loss": 1.3585,
      "step": 1154
    },
    {
      "epoch": 1.1105769230769231,
      "grad_norm": 8.775079727172852,
      "learning_rate": 1.7780769230769233e-05,
      "loss": 1.1014,
      "step": 1155
    },
    {
      "epoch": 1.1115384615384616,
      "grad_norm": 7.518828392028809,
      "learning_rate": 1.7778846153846153e-05,
      "loss": 1.4707,
      "step": 1156
    },
    {
      "epoch": 1.1125,
      "grad_norm": 7.595402240753174,
      "learning_rate": 1.777692307692308e-05,
      "loss": 1.1444,
      "step": 1157
    },
    {
      "epoch": 1.1134615384615385,
      "grad_norm": 12.20345401763916,
      "learning_rate": 1.7775000000000002e-05,
      "loss": 0.4382,
      "step": 1158
    },
    {
      "epoch": 1.114423076923077,
      "grad_norm": 15.47634220123291,
      "learning_rate": 1.7773076923076925e-05,
      "loss": 0.3117,
      "step": 1159
    },
    {
      "epoch": 1.1153846153846154,
      "grad_norm": 8.488587379455566,
      "learning_rate": 1.7771153846153847e-05,
      "loss": 1.791,
      "step": 1160
    },
    {
      "epoch": 1.1163461538461539,
      "grad_norm": 9.016586303710938,
      "learning_rate": 1.776923076923077e-05,
      "loss": 1.642,
      "step": 1161
    },
    {
      "epoch": 1.1173076923076923,
      "grad_norm": 6.903302192687988,
      "learning_rate": 1.7767307692307693e-05,
      "loss": 0.9667,
      "step": 1162
    },
    {
      "epoch": 1.1182692307692308,
      "grad_norm": 6.519628524780273,
      "learning_rate": 1.776538461538462e-05,
      "loss": 1.4315,
      "step": 1163
    },
    {
      "epoch": 1.1192307692307693,
      "grad_norm": 7.281775951385498,
      "learning_rate": 1.776346153846154e-05,
      "loss": 1.1371,
      "step": 1164
    },
    {
      "epoch": 1.1201923076923077,
      "grad_norm": 6.477919578552246,
      "learning_rate": 1.7761538461538465e-05,
      "loss": 1.0696,
      "step": 1165
    },
    {
      "epoch": 1.1211538461538462,
      "grad_norm": 5.345682144165039,
      "learning_rate": 1.7759615384615384e-05,
      "loss": 0.3895,
      "step": 1166
    },
    {
      "epoch": 1.1221153846153846,
      "grad_norm": 8.0874662399292,
      "learning_rate": 1.775769230769231e-05,
      "loss": 1.3588,
      "step": 1167
    },
    {
      "epoch": 1.123076923076923,
      "grad_norm": 5.9735517501831055,
      "learning_rate": 1.7755769230769233e-05,
      "loss": 1.6282,
      "step": 1168
    },
    {
      "epoch": 1.1240384615384615,
      "grad_norm": 6.465163230895996,
      "learning_rate": 1.7753846153846156e-05,
      "loss": 0.8072,
      "step": 1169
    },
    {
      "epoch": 1.125,
      "grad_norm": 6.74644136428833,
      "learning_rate": 1.775192307692308e-05,
      "loss": 1.6981,
      "step": 1170
    },
    {
      "epoch": 1.1259615384615385,
      "grad_norm": 9.228351593017578,
      "learning_rate": 1.775e-05,
      "loss": 1.0207,
      "step": 1171
    },
    {
      "epoch": 1.126923076923077,
      "grad_norm": 6.8191609382629395,
      "learning_rate": 1.7748076923076924e-05,
      "loss": 1.1176,
      "step": 1172
    },
    {
      "epoch": 1.1278846153846154,
      "grad_norm": 6.814632415771484,
      "learning_rate": 1.774615384615385e-05,
      "loss": 0.7041,
      "step": 1173
    },
    {
      "epoch": 1.1288461538461538,
      "grad_norm": 24.445682525634766,
      "learning_rate": 1.774423076923077e-05,
      "loss": 1.3487,
      "step": 1174
    },
    {
      "epoch": 1.1298076923076923,
      "grad_norm": 8.044562339782715,
      "learning_rate": 1.7742307692307696e-05,
      "loss": 1.0693,
      "step": 1175
    },
    {
      "epoch": 1.1307692307692307,
      "grad_norm": 7.2650909423828125,
      "learning_rate": 1.7740384615384615e-05,
      "loss": 0.7851,
      "step": 1176
    },
    {
      "epoch": 1.1317307692307692,
      "grad_norm": 10.097061157226562,
      "learning_rate": 1.773846153846154e-05,
      "loss": 2.0863,
      "step": 1177
    },
    {
      "epoch": 1.1326923076923077,
      "grad_norm": 10.698511123657227,
      "learning_rate": 1.7736538461538464e-05,
      "loss": 0.7589,
      "step": 1178
    },
    {
      "epoch": 1.1336538461538461,
      "grad_norm": 11.166792869567871,
      "learning_rate": 1.7734615384615387e-05,
      "loss": 1.9359,
      "step": 1179
    },
    {
      "epoch": 1.1346153846153846,
      "grad_norm": 6.754636764526367,
      "learning_rate": 1.773269230769231e-05,
      "loss": 1.1857,
      "step": 1180
    },
    {
      "epoch": 1.135576923076923,
      "grad_norm": 7.915428638458252,
      "learning_rate": 1.7730769230769232e-05,
      "loss": 1.8867,
      "step": 1181
    },
    {
      "epoch": 1.1365384615384615,
      "grad_norm": 4.895561695098877,
      "learning_rate": 1.7728846153846155e-05,
      "loss": 1.5504,
      "step": 1182
    },
    {
      "epoch": 1.1375,
      "grad_norm": 7.734982967376709,
      "learning_rate": 1.7726923076923078e-05,
      "loss": 1.351,
      "step": 1183
    },
    {
      "epoch": 1.1384615384615384,
      "grad_norm": 10.35969066619873,
      "learning_rate": 1.7725e-05,
      "loss": 1.183,
      "step": 1184
    },
    {
      "epoch": 1.1394230769230769,
      "grad_norm": 6.642940044403076,
      "learning_rate": 1.7723076923076927e-05,
      "loss": 1.5582,
      "step": 1185
    },
    {
      "epoch": 1.1403846153846153,
      "grad_norm": 5.480701923370361,
      "learning_rate": 1.7721153846153846e-05,
      "loss": 0.3273,
      "step": 1186
    },
    {
      "epoch": 1.1413461538461538,
      "grad_norm": 6.56153678894043,
      "learning_rate": 1.7719230769230772e-05,
      "loss": 1.7856,
      "step": 1187
    },
    {
      "epoch": 1.1423076923076922,
      "grad_norm": 6.992345333099365,
      "learning_rate": 1.7717307692307695e-05,
      "loss": 1.2192,
      "step": 1188
    },
    {
      "epoch": 1.1432692307692307,
      "grad_norm": 10.5452880859375,
      "learning_rate": 1.7715384615384618e-05,
      "loss": 1.9886,
      "step": 1189
    },
    {
      "epoch": 1.1442307692307692,
      "grad_norm": 8.775300025939941,
      "learning_rate": 1.771346153846154e-05,
      "loss": 1.1021,
      "step": 1190
    },
    {
      "epoch": 1.1451923076923076,
      "grad_norm": 6.520989418029785,
      "learning_rate": 1.7711538461538463e-05,
      "loss": 1.0005,
      "step": 1191
    },
    {
      "epoch": 1.146153846153846,
      "grad_norm": 6.972891330718994,
      "learning_rate": 1.7709615384615386e-05,
      "loss": 0.5552,
      "step": 1192
    },
    {
      "epoch": 1.1471153846153845,
      "grad_norm": 9.834553718566895,
      "learning_rate": 1.770769230769231e-05,
      "loss": 0.4745,
      "step": 1193
    },
    {
      "epoch": 1.148076923076923,
      "grad_norm": 12.670267105102539,
      "learning_rate": 1.770576923076923e-05,
      "loss": 2.1531,
      "step": 1194
    },
    {
      "epoch": 1.1490384615384615,
      "grad_norm": 15.445599555969238,
      "learning_rate": 1.7703846153846154e-05,
      "loss": 0.5627,
      "step": 1195
    },
    {
      "epoch": 1.15,
      "grad_norm": 6.31038761138916,
      "learning_rate": 1.7701923076923077e-05,
      "loss": 2.0979,
      "step": 1196
    },
    {
      "epoch": 1.1509615384615384,
      "grad_norm": 5.402377128601074,
      "learning_rate": 1.77e-05,
      "loss": 0.5295,
      "step": 1197
    },
    {
      "epoch": 1.1519230769230768,
      "grad_norm": 9.236397743225098,
      "learning_rate": 1.7698076923076926e-05,
      "loss": 1.4879,
      "step": 1198
    },
    {
      "epoch": 1.1528846153846155,
      "grad_norm": 5.380141735076904,
      "learning_rate": 1.769615384615385e-05,
      "loss": 1.0418,
      "step": 1199
    },
    {
      "epoch": 1.1538461538461537,
      "grad_norm": 7.4860520362854,
      "learning_rate": 1.769423076923077e-05,
      "loss": 1.105,
      "step": 1200
    },
    {
      "epoch": 1.1548076923076924,
      "grad_norm": 8.087043762207031,
      "learning_rate": 1.7692307692307694e-05,
      "loss": 2.0062,
      "step": 1201
    },
    {
      "epoch": 1.1557692307692307,
      "grad_norm": 5.5410475730896,
      "learning_rate": 1.7690384615384617e-05,
      "loss": 2.1542,
      "step": 1202
    },
    {
      "epoch": 1.1567307692307693,
      "grad_norm": 14.880151748657227,
      "learning_rate": 1.768846153846154e-05,
      "loss": 1.049,
      "step": 1203
    },
    {
      "epoch": 1.1576923076923076,
      "grad_norm": 6.374655246734619,
      "learning_rate": 1.7686538461538463e-05,
      "loss": 1.647,
      "step": 1204
    },
    {
      "epoch": 1.1586538461538463,
      "grad_norm": 6.748347759246826,
      "learning_rate": 1.7684615384615385e-05,
      "loss": 1.6708,
      "step": 1205
    },
    {
      "epoch": 1.1596153846153845,
      "grad_norm": 7.667849063873291,
      "learning_rate": 1.7682692307692308e-05,
      "loss": 1.3726,
      "step": 1206
    },
    {
      "epoch": 1.1605769230769232,
      "grad_norm": 7.353489875793457,
      "learning_rate": 1.768076923076923e-05,
      "loss": 1.8408,
      "step": 1207
    },
    {
      "epoch": 1.1615384615384616,
      "grad_norm": 6.070751190185547,
      "learning_rate": 1.7678846153846154e-05,
      "loss": 1.2116,
      "step": 1208
    },
    {
      "epoch": 1.1625,
      "grad_norm": 5.9082441329956055,
      "learning_rate": 1.7676923076923077e-05,
      "loss": 0.6393,
      "step": 1209
    },
    {
      "epoch": 1.1634615384615385,
      "grad_norm": 5.68604850769043,
      "learning_rate": 1.7675000000000003e-05,
      "loss": 0.9524,
      "step": 1210
    },
    {
      "epoch": 1.164423076923077,
      "grad_norm": 8.206642150878906,
      "learning_rate": 1.7673076923076925e-05,
      "loss": 1.4324,
      "step": 1211
    },
    {
      "epoch": 1.1653846153846155,
      "grad_norm": 10.558384895324707,
      "learning_rate": 1.7671153846153848e-05,
      "loss": 0.9252,
      "step": 1212
    },
    {
      "epoch": 1.166346153846154,
      "grad_norm": 7.130324840545654,
      "learning_rate": 1.766923076923077e-05,
      "loss": 1.1502,
      "step": 1213
    },
    {
      "epoch": 1.1673076923076924,
      "grad_norm": 5.840915679931641,
      "learning_rate": 1.7667307692307694e-05,
      "loss": 0.8339,
      "step": 1214
    },
    {
      "epoch": 1.1682692307692308,
      "grad_norm": 6.5192060470581055,
      "learning_rate": 1.7665384615384617e-05,
      "loss": 2.0066,
      "step": 1215
    },
    {
      "epoch": 1.1692307692307693,
      "grad_norm": 6.550567626953125,
      "learning_rate": 1.766346153846154e-05,
      "loss": 0.9117,
      "step": 1216
    },
    {
      "epoch": 1.1701923076923078,
      "grad_norm": 5.856640815734863,
      "learning_rate": 1.7661538461538462e-05,
      "loss": 2.0164,
      "step": 1217
    },
    {
      "epoch": 1.1711538461538462,
      "grad_norm": 10.999506950378418,
      "learning_rate": 1.7659615384615385e-05,
      "loss": 1.2153,
      "step": 1218
    },
    {
      "epoch": 1.1721153846153847,
      "grad_norm": 8.263845443725586,
      "learning_rate": 1.7657692307692308e-05,
      "loss": 1.1398,
      "step": 1219
    },
    {
      "epoch": 1.1730769230769231,
      "grad_norm": 8.381062507629395,
      "learning_rate": 1.7655769230769234e-05,
      "loss": 1.6818,
      "step": 1220
    },
    {
      "epoch": 1.1740384615384616,
      "grad_norm": 8.707709312438965,
      "learning_rate": 1.7653846153846153e-05,
      "loss": 1.3832,
      "step": 1221
    },
    {
      "epoch": 1.175,
      "grad_norm": 10.73078441619873,
      "learning_rate": 1.765192307692308e-05,
      "loss": 1.3224,
      "step": 1222
    },
    {
      "epoch": 1.1759615384615385,
      "grad_norm": 9.710039138793945,
      "learning_rate": 1.7650000000000002e-05,
      "loss": 0.8646,
      "step": 1223
    },
    {
      "epoch": 1.176923076923077,
      "grad_norm": 4.967927932739258,
      "learning_rate": 1.7648076923076925e-05,
      "loss": 0.9833,
      "step": 1224
    },
    {
      "epoch": 1.1778846153846154,
      "grad_norm": 6.998035430908203,
      "learning_rate": 1.7646153846153848e-05,
      "loss": 1.6086,
      "step": 1225
    },
    {
      "epoch": 1.1788461538461539,
      "grad_norm": 7.868578910827637,
      "learning_rate": 1.764423076923077e-05,
      "loss": 1.5566,
      "step": 1226
    },
    {
      "epoch": 1.1798076923076923,
      "grad_norm": 6.0776777267456055,
      "learning_rate": 1.7642307692307693e-05,
      "loss": 1.726,
      "step": 1227
    },
    {
      "epoch": 1.1807692307692308,
      "grad_norm": 7.006542205810547,
      "learning_rate": 1.7640384615384616e-05,
      "loss": 1.3118,
      "step": 1228
    },
    {
      "epoch": 1.1817307692307693,
      "grad_norm": 6.225818157196045,
      "learning_rate": 1.763846153846154e-05,
      "loss": 1.3967,
      "step": 1229
    },
    {
      "epoch": 1.1826923076923077,
      "grad_norm": 6.823882579803467,
      "learning_rate": 1.7636538461538465e-05,
      "loss": 1.5242,
      "step": 1230
    },
    {
      "epoch": 1.1836538461538462,
      "grad_norm": 5.656426906585693,
      "learning_rate": 1.7634615384615384e-05,
      "loss": 1.6172,
      "step": 1231
    },
    {
      "epoch": 1.1846153846153846,
      "grad_norm": 5.058721542358398,
      "learning_rate": 1.763269230769231e-05,
      "loss": 1.2841,
      "step": 1232
    },
    {
      "epoch": 1.185576923076923,
      "grad_norm": 7.207999229431152,
      "learning_rate": 1.763076923076923e-05,
      "loss": 1.8244,
      "step": 1233
    },
    {
      "epoch": 1.1865384615384615,
      "grad_norm": 8.696551322937012,
      "learning_rate": 1.7628846153846156e-05,
      "loss": 1.1094,
      "step": 1234
    },
    {
      "epoch": 1.1875,
      "grad_norm": 8.604615211486816,
      "learning_rate": 1.762692307692308e-05,
      "loss": 1.6293,
      "step": 1235
    },
    {
      "epoch": 1.1884615384615385,
      "grad_norm": 8.511164665222168,
      "learning_rate": 1.7625e-05,
      "loss": 0.6645,
      "step": 1236
    },
    {
      "epoch": 1.189423076923077,
      "grad_norm": 6.2846784591674805,
      "learning_rate": 1.7623076923076924e-05,
      "loss": 1.8411,
      "step": 1237
    },
    {
      "epoch": 1.1903846153846154,
      "grad_norm": 9.695560455322266,
      "learning_rate": 1.7621153846153847e-05,
      "loss": 0.9401,
      "step": 1238
    },
    {
      "epoch": 1.1913461538461538,
      "grad_norm": 5.965844631195068,
      "learning_rate": 1.761923076923077e-05,
      "loss": 1.9047,
      "step": 1239
    },
    {
      "epoch": 1.1923076923076923,
      "grad_norm": 6.520747661590576,
      "learning_rate": 1.7617307692307696e-05,
      "loss": 1.2077,
      "step": 1240
    },
    {
      "epoch": 1.1932692307692307,
      "grad_norm": 14.562752723693848,
      "learning_rate": 1.7615384615384615e-05,
      "loss": 0.7414,
      "step": 1241
    },
    {
      "epoch": 1.1942307692307692,
      "grad_norm": 6.759368419647217,
      "learning_rate": 1.761346153846154e-05,
      "loss": 1.1932,
      "step": 1242
    },
    {
      "epoch": 1.1951923076923077,
      "grad_norm": 6.073851585388184,
      "learning_rate": 1.761153846153846e-05,
      "loss": 1.5758,
      "step": 1243
    },
    {
      "epoch": 1.1961538461538461,
      "grad_norm": 5.637701988220215,
      "learning_rate": 1.7609615384615387e-05,
      "loss": 1.5385,
      "step": 1244
    },
    {
      "epoch": 1.1971153846153846,
      "grad_norm": 10.149378776550293,
      "learning_rate": 1.760769230769231e-05,
      "loss": 1.6372,
      "step": 1245
    },
    {
      "epoch": 1.198076923076923,
      "grad_norm": 12.582498550415039,
      "learning_rate": 1.7605769230769233e-05,
      "loss": 1.169,
      "step": 1246
    },
    {
      "epoch": 1.1990384615384615,
      "grad_norm": 4.995904445648193,
      "learning_rate": 1.7603846153846155e-05,
      "loss": 1.6791,
      "step": 1247
    },
    {
      "epoch": 1.2,
      "grad_norm": 5.496471881866455,
      "learning_rate": 1.7601923076923078e-05,
      "loss": 1.5862,
      "step": 1248
    },
    {
      "epoch": 1.2009615384615384,
      "grad_norm": 4.883450984954834,
      "learning_rate": 1.76e-05,
      "loss": 1.2531,
      "step": 1249
    },
    {
      "epoch": 1.2019230769230769,
      "grad_norm": 6.561707496643066,
      "learning_rate": 1.7598076923076927e-05,
      "loss": 0.5682,
      "step": 1250
    },
    {
      "epoch": 1.2028846153846153,
      "grad_norm": 8.482490539550781,
      "learning_rate": 1.7596153846153846e-05,
      "loss": 2.0505,
      "step": 1251
    },
    {
      "epoch": 1.2038461538461538,
      "grad_norm": 11.340316772460938,
      "learning_rate": 1.7594230769230773e-05,
      "loss": 0.0986,
      "step": 1252
    },
    {
      "epoch": 1.2048076923076922,
      "grad_norm": 6.353091239929199,
      "learning_rate": 1.7592307692307692e-05,
      "loss": 0.9736,
      "step": 1253
    },
    {
      "epoch": 1.2057692307692307,
      "grad_norm": 8.873287200927734,
      "learning_rate": 1.7590384615384618e-05,
      "loss": 1.0224,
      "step": 1254
    },
    {
      "epoch": 1.2067307692307692,
      "grad_norm": 5.373356819152832,
      "learning_rate": 1.758846153846154e-05,
      "loss": 1.3313,
      "step": 1255
    },
    {
      "epoch": 1.2076923076923076,
      "grad_norm": 6.183599948883057,
      "learning_rate": 1.7586538461538464e-05,
      "loss": 1.5088,
      "step": 1256
    },
    {
      "epoch": 1.208653846153846,
      "grad_norm": 7.528284072875977,
      "learning_rate": 1.7584615384615386e-05,
      "loss": 0.7051,
      "step": 1257
    },
    {
      "epoch": 1.2096153846153845,
      "grad_norm": 17.380849838256836,
      "learning_rate": 1.758269230769231e-05,
      "loss": 0.6474,
      "step": 1258
    },
    {
      "epoch": 1.210576923076923,
      "grad_norm": 5.751458168029785,
      "learning_rate": 1.7580769230769232e-05,
      "loss": 1.5426,
      "step": 1259
    },
    {
      "epoch": 1.2115384615384615,
      "grad_norm": 8.033707618713379,
      "learning_rate": 1.7578846153846155e-05,
      "loss": 1.4001,
      "step": 1260
    },
    {
      "epoch": 1.2125,
      "grad_norm": 6.666274070739746,
      "learning_rate": 1.7576923076923078e-05,
      "loss": 1.5372,
      "step": 1261
    },
    {
      "epoch": 1.2134615384615384,
      "grad_norm": 1.3326411247253418,
      "learning_rate": 1.7575000000000004e-05,
      "loss": 0.018,
      "step": 1262
    },
    {
      "epoch": 1.2144230769230768,
      "grad_norm": 6.088021755218506,
      "learning_rate": 1.7573076923076923e-05,
      "loss": 1.0049,
      "step": 1263
    },
    {
      "epoch": 1.2153846153846155,
      "grad_norm": 9.63624095916748,
      "learning_rate": 1.757115384615385e-05,
      "loss": 0.8893,
      "step": 1264
    },
    {
      "epoch": 1.2163461538461537,
      "grad_norm": 5.525622367858887,
      "learning_rate": 1.7569230769230772e-05,
      "loss": 0.1353,
      "step": 1265
    },
    {
      "epoch": 1.2173076923076924,
      "grad_norm": 5.357698440551758,
      "learning_rate": 1.7567307692307695e-05,
      "loss": 2.2721,
      "step": 1266
    },
    {
      "epoch": 1.2182692307692307,
      "grad_norm": 6.131730556488037,
      "learning_rate": 1.7565384615384618e-05,
      "loss": 1.4853,
      "step": 1267
    },
    {
      "epoch": 1.2192307692307693,
      "grad_norm": 7.73441743850708,
      "learning_rate": 1.756346153846154e-05,
      "loss": 1.8795,
      "step": 1268
    },
    {
      "epoch": 1.2201923076923076,
      "grad_norm": 22.83088493347168,
      "learning_rate": 1.7561538461538463e-05,
      "loss": 1.4592,
      "step": 1269
    },
    {
      "epoch": 1.2211538461538463,
      "grad_norm": 5.967719078063965,
      "learning_rate": 1.7559615384615386e-05,
      "loss": 1.729,
      "step": 1270
    },
    {
      "epoch": 1.2221153846153845,
      "grad_norm": 6.681755065917969,
      "learning_rate": 1.755769230769231e-05,
      "loss": 0.5937,
      "step": 1271
    },
    {
      "epoch": 1.2230769230769232,
      "grad_norm": 7.495696067810059,
      "learning_rate": 1.755576923076923e-05,
      "loss": 0.946,
      "step": 1272
    },
    {
      "epoch": 1.2240384615384616,
      "grad_norm": 6.113794803619385,
      "learning_rate": 1.7553846153846154e-05,
      "loss": 0.2953,
      "step": 1273
    },
    {
      "epoch": 1.225,
      "grad_norm": 3.262925863265991,
      "learning_rate": 1.755192307692308e-05,
      "loss": 0.0329,
      "step": 1274
    },
    {
      "epoch": 1.2259615384615385,
      "grad_norm": 9.380894660949707,
      "learning_rate": 1.755e-05,
      "loss": 1.7353,
      "step": 1275
    },
    {
      "epoch": 1.226923076923077,
      "grad_norm": 9.183545112609863,
      "learning_rate": 1.7548076923076926e-05,
      "loss": 1.0142,
      "step": 1276
    },
    {
      "epoch": 1.2278846153846155,
      "grad_norm": 5.596758842468262,
      "learning_rate": 1.754615384615385e-05,
      "loss": 1.4965,
      "step": 1277
    },
    {
      "epoch": 1.228846153846154,
      "grad_norm": 6.063991546630859,
      "learning_rate": 1.754423076923077e-05,
      "loss": 1.5985,
      "step": 1278
    },
    {
      "epoch": 1.2298076923076924,
      "grad_norm": 10.026253700256348,
      "learning_rate": 1.7542307692307694e-05,
      "loss": 0.2036,
      "step": 1279
    },
    {
      "epoch": 1.2307692307692308,
      "grad_norm": 5.821896553039551,
      "learning_rate": 1.7540384615384617e-05,
      "loss": 1.6698,
      "step": 1280
    },
    {
      "epoch": 1.2317307692307693,
      "grad_norm": 7.391417980194092,
      "learning_rate": 1.753846153846154e-05,
      "loss": 1.3458,
      "step": 1281
    },
    {
      "epoch": 1.2326923076923078,
      "grad_norm": 8.1218900680542,
      "learning_rate": 1.7536538461538462e-05,
      "loss": 1.1443,
      "step": 1282
    },
    {
      "epoch": 1.2336538461538462,
      "grad_norm": 8.010279655456543,
      "learning_rate": 1.7534615384615385e-05,
      "loss": 1.1759,
      "step": 1283
    },
    {
      "epoch": 1.2346153846153847,
      "grad_norm": 6.061884880065918,
      "learning_rate": 1.7532692307692308e-05,
      "loss": 1.8355,
      "step": 1284
    },
    {
      "epoch": 1.2355769230769231,
      "grad_norm": 8.732094764709473,
      "learning_rate": 1.753076923076923e-05,
      "loss": 1.1649,
      "step": 1285
    },
    {
      "epoch": 1.2365384615384616,
      "grad_norm": 6.3003997802734375,
      "learning_rate": 1.7528846153846154e-05,
      "loss": 1.6981,
      "step": 1286
    },
    {
      "epoch": 1.2375,
      "grad_norm": 2.3553740978240967,
      "learning_rate": 1.752692307692308e-05,
      "loss": 0.0235,
      "step": 1287
    },
    {
      "epoch": 1.2384615384615385,
      "grad_norm": 3.9904069900512695,
      "learning_rate": 1.7525000000000002e-05,
      "loss": 0.0336,
      "step": 1288
    },
    {
      "epoch": 1.239423076923077,
      "grad_norm": 7.041998863220215,
      "learning_rate": 1.7523076923076925e-05,
      "loss": 1.2463,
      "step": 1289
    },
    {
      "epoch": 1.2403846153846154,
      "grad_norm": 6.4926252365112305,
      "learning_rate": 1.7521153846153848e-05,
      "loss": 1.4232,
      "step": 1290
    },
    {
      "epoch": 1.2413461538461539,
      "grad_norm": 4.3106279373168945,
      "learning_rate": 1.751923076923077e-05,
      "loss": 0.1648,
      "step": 1291
    },
    {
      "epoch": 1.2423076923076923,
      "grad_norm": 13.92105770111084,
      "learning_rate": 1.7517307692307694e-05,
      "loss": 0.9598,
      "step": 1292
    },
    {
      "epoch": 1.2432692307692308,
      "grad_norm": 9.113999366760254,
      "learning_rate": 1.7515384615384616e-05,
      "loss": 1.0002,
      "step": 1293
    },
    {
      "epoch": 1.2442307692307693,
      "grad_norm": 7.1666741371154785,
      "learning_rate": 1.751346153846154e-05,
      "loss": 1.9922,
      "step": 1294
    },
    {
      "epoch": 1.2451923076923077,
      "grad_norm": 7.420692443847656,
      "learning_rate": 1.7511538461538462e-05,
      "loss": 1.3429,
      "step": 1295
    },
    {
      "epoch": 1.2461538461538462,
      "grad_norm": 6.913588047027588,
      "learning_rate": 1.7509615384615385e-05,
      "loss": 1.7824,
      "step": 1296
    },
    {
      "epoch": 1.2471153846153846,
      "grad_norm": 6.177270889282227,
      "learning_rate": 1.750769230769231e-05,
      "loss": 1.3288,
      "step": 1297
    },
    {
      "epoch": 1.248076923076923,
      "grad_norm": 5.478122711181641,
      "learning_rate": 1.750576923076923e-05,
      "loss": 1.6814,
      "step": 1298
    },
    {
      "epoch": 1.2490384615384615,
      "grad_norm": 6.273842811584473,
      "learning_rate": 1.7503846153846156e-05,
      "loss": 1.3417,
      "step": 1299
    },
    {
      "epoch": 1.25,
      "grad_norm": 5.127583026885986,
      "learning_rate": 1.750192307692308e-05,
      "loss": 1.2558,
      "step": 1300
    },
    {
      "epoch": 1.2509615384615385,
      "grad_norm": 5.2679829597473145,
      "learning_rate": 1.7500000000000002e-05,
      "loss": 1.1439,
      "step": 1301
    },
    {
      "epoch": 1.251923076923077,
      "grad_norm": 5.765327453613281,
      "learning_rate": 1.7498076923076925e-05,
      "loss": 1.2055,
      "step": 1302
    },
    {
      "epoch": 1.2528846153846154,
      "grad_norm": 6.620988368988037,
      "learning_rate": 1.7496153846153847e-05,
      "loss": 0.6104,
      "step": 1303
    },
    {
      "epoch": 1.2538461538461538,
      "grad_norm": 4.335615158081055,
      "learning_rate": 1.749423076923077e-05,
      "loss": 0.5696,
      "step": 1304
    },
    {
      "epoch": 1.2548076923076923,
      "grad_norm": 7.116262435913086,
      "learning_rate": 1.7492307692307693e-05,
      "loss": 1.5009,
      "step": 1305
    },
    {
      "epoch": 1.2557692307692307,
      "grad_norm": 7.736232757568359,
      "learning_rate": 1.7490384615384616e-05,
      "loss": 1.2684,
      "step": 1306
    },
    {
      "epoch": 1.2567307692307692,
      "grad_norm": 6.858211994171143,
      "learning_rate": 1.7488461538461542e-05,
      "loss": 1.38,
      "step": 1307
    },
    {
      "epoch": 1.2576923076923077,
      "grad_norm": 9.324773788452148,
      "learning_rate": 1.748653846153846e-05,
      "loss": 1.6799,
      "step": 1308
    },
    {
      "epoch": 1.2586538461538461,
      "grad_norm": 8.749091148376465,
      "learning_rate": 1.7484615384615387e-05,
      "loss": 1.4737,
      "step": 1309
    },
    {
      "epoch": 1.2596153846153846,
      "grad_norm": 7.822444915771484,
      "learning_rate": 1.7482692307692307e-05,
      "loss": 1.0939,
      "step": 1310
    },
    {
      "epoch": 1.260576923076923,
      "grad_norm": 8.548921585083008,
      "learning_rate": 1.7480769230769233e-05,
      "loss": 1.1286,
      "step": 1311
    },
    {
      "epoch": 1.2615384615384615,
      "grad_norm": 8.399435043334961,
      "learning_rate": 1.7478846153846156e-05,
      "loss": 1.1976,
      "step": 1312
    },
    {
      "epoch": 1.2625,
      "grad_norm": 6.678022384643555,
      "learning_rate": 1.747692307692308e-05,
      "loss": 1.8075,
      "step": 1313
    },
    {
      "epoch": 1.2634615384615384,
      "grad_norm": 7.796258449554443,
      "learning_rate": 1.7475e-05,
      "loss": 1.1483,
      "step": 1314
    },
    {
      "epoch": 1.2644230769230769,
      "grad_norm": 7.104834079742432,
      "learning_rate": 1.7473076923076924e-05,
      "loss": 0.0602,
      "step": 1315
    },
    {
      "epoch": 1.2653846153846153,
      "grad_norm": 10.365077018737793,
      "learning_rate": 1.7471153846153847e-05,
      "loss": 0.8168,
      "step": 1316
    },
    {
      "epoch": 1.2663461538461538,
      "grad_norm": 4.485780715942383,
      "learning_rate": 1.7469230769230773e-05,
      "loss": 1.0481,
      "step": 1317
    },
    {
      "epoch": 1.2673076923076922,
      "grad_norm": 14.188581466674805,
      "learning_rate": 1.7467307692307692e-05,
      "loss": 1.8905,
      "step": 1318
    },
    {
      "epoch": 1.2682692307692307,
      "grad_norm": 7.888566493988037,
      "learning_rate": 1.746538461538462e-05,
      "loss": 1.4907,
      "step": 1319
    },
    {
      "epoch": 1.2692307692307692,
      "grad_norm": 4.763081073760986,
      "learning_rate": 1.7463461538461538e-05,
      "loss": 1.1738,
      "step": 1320
    },
    {
      "epoch": 1.2701923076923076,
      "grad_norm": 6.852401256561279,
      "learning_rate": 1.7461538461538464e-05,
      "loss": 1.6942,
      "step": 1321
    },
    {
      "epoch": 1.271153846153846,
      "grad_norm": 6.484696388244629,
      "learning_rate": 1.7459615384615387e-05,
      "loss": 1.2284,
      "step": 1322
    },
    {
      "epoch": 1.2721153846153845,
      "grad_norm": 1.938886046409607,
      "learning_rate": 1.745769230769231e-05,
      "loss": 0.0159,
      "step": 1323
    },
    {
      "epoch": 1.273076923076923,
      "grad_norm": 7.896383762359619,
      "learning_rate": 1.7455769230769232e-05,
      "loss": 1.7231,
      "step": 1324
    },
    {
      "epoch": 1.2740384615384617,
      "grad_norm": 12.267745971679688,
      "learning_rate": 1.7453846153846155e-05,
      "loss": 0.3941,
      "step": 1325
    },
    {
      "epoch": 1.275,
      "grad_norm": 7.578659534454346,
      "learning_rate": 1.7451923076923078e-05,
      "loss": 0.2479,
      "step": 1326
    },
    {
      "epoch": 1.2759615384615386,
      "grad_norm": 6.251343250274658,
      "learning_rate": 1.7450000000000004e-05,
      "loss": 1.3311,
      "step": 1327
    },
    {
      "epoch": 1.2769230769230768,
      "grad_norm": 6.426092624664307,
      "learning_rate": 1.7448076923076923e-05,
      "loss": 1.2223,
      "step": 1328
    },
    {
      "epoch": 1.2778846153846155,
      "grad_norm": 13.70655632019043,
      "learning_rate": 1.744615384615385e-05,
      "loss": 1.1058,
      "step": 1329
    },
    {
      "epoch": 1.2788461538461537,
      "grad_norm": 10.25557804107666,
      "learning_rate": 1.744423076923077e-05,
      "loss": 1.1724,
      "step": 1330
    },
    {
      "epoch": 1.2798076923076924,
      "grad_norm": 10.616016387939453,
      "learning_rate": 1.7442307692307695e-05,
      "loss": 1.6823,
      "step": 1331
    },
    {
      "epoch": 1.2807692307692307,
      "grad_norm": 7.0986809730529785,
      "learning_rate": 1.7440384615384618e-05,
      "loss": 1.5988,
      "step": 1332
    },
    {
      "epoch": 1.2817307692307693,
      "grad_norm": 9.068717956542969,
      "learning_rate": 1.743846153846154e-05,
      "loss": 1.5558,
      "step": 1333
    },
    {
      "epoch": 1.2826923076923076,
      "grad_norm": 6.271995544433594,
      "learning_rate": 1.7436538461538463e-05,
      "loss": 1.4354,
      "step": 1334
    },
    {
      "epoch": 1.2836538461538463,
      "grad_norm": 5.954681873321533,
      "learning_rate": 1.7434615384615386e-05,
      "loss": 1.692,
      "step": 1335
    },
    {
      "epoch": 1.2846153846153845,
      "grad_norm": 3.9219229221343994,
      "learning_rate": 1.743269230769231e-05,
      "loss": 0.0288,
      "step": 1336
    },
    {
      "epoch": 1.2855769230769232,
      "grad_norm": 7.8491291999816895,
      "learning_rate": 1.7430769230769232e-05,
      "loss": 0.1162,
      "step": 1337
    },
    {
      "epoch": 1.2865384615384614,
      "grad_norm": 5.81074333190918,
      "learning_rate": 1.7428846153846154e-05,
      "loss": 1.6614,
      "step": 1338
    },
    {
      "epoch": 1.2875,
      "grad_norm": 9.289114952087402,
      "learning_rate": 1.742692307692308e-05,
      "loss": 1.7369,
      "step": 1339
    },
    {
      "epoch": 1.2884615384615383,
      "grad_norm": 8.436521530151367,
      "learning_rate": 1.7425e-05,
      "loss": 1.5447,
      "step": 1340
    },
    {
      "epoch": 1.289423076923077,
      "grad_norm": 7.9945807456970215,
      "learning_rate": 1.7423076923076926e-05,
      "loss": 1.8697,
      "step": 1341
    },
    {
      "epoch": 1.2903846153846155,
      "grad_norm": 9.015040397644043,
      "learning_rate": 1.7421153846153846e-05,
      "loss": 1.5212,
      "step": 1342
    },
    {
      "epoch": 1.291346153846154,
      "grad_norm": 7.192790508270264,
      "learning_rate": 1.7419230769230772e-05,
      "loss": 1.5472,
      "step": 1343
    },
    {
      "epoch": 1.2923076923076924,
      "grad_norm": 8.110513687133789,
      "learning_rate": 1.7417307692307694e-05,
      "loss": 0.9462,
      "step": 1344
    },
    {
      "epoch": 1.2932692307692308,
      "grad_norm": 5.672741889953613,
      "learning_rate": 1.7415384615384617e-05,
      "loss": 1.2318,
      "step": 1345
    },
    {
      "epoch": 1.2942307692307693,
      "grad_norm": 10.740699768066406,
      "learning_rate": 1.741346153846154e-05,
      "loss": 1.8361,
      "step": 1346
    },
    {
      "epoch": 1.2951923076923078,
      "grad_norm": 8.072315216064453,
      "learning_rate": 1.7411538461538463e-05,
      "loss": 0.963,
      "step": 1347
    },
    {
      "epoch": 1.2961538461538462,
      "grad_norm": 9.904940605163574,
      "learning_rate": 1.7409615384615386e-05,
      "loss": 0.7743,
      "step": 1348
    },
    {
      "epoch": 1.2971153846153847,
      "grad_norm": 6.650479316711426,
      "learning_rate": 1.740769230769231e-05,
      "loss": 0.2551,
      "step": 1349
    },
    {
      "epoch": 1.2980769230769231,
      "grad_norm": 8.194189071655273,
      "learning_rate": 1.740576923076923e-05,
      "loss": 1.0388,
      "step": 1350
    },
    {
      "epoch": 1.2990384615384616,
      "grad_norm": 8.990485191345215,
      "learning_rate": 1.7403846153846157e-05,
      "loss": 0.0976,
      "step": 1351
    },
    {
      "epoch": 1.3,
      "grad_norm": 12.92582893371582,
      "learning_rate": 1.7401923076923077e-05,
      "loss": 1.5848,
      "step": 1352
    },
    {
      "epoch": 1.3009615384615385,
      "grad_norm": 9.726490020751953,
      "learning_rate": 1.7400000000000003e-05,
      "loss": 0.8159,
      "step": 1353
    },
    {
      "epoch": 1.301923076923077,
      "grad_norm": 12.207145690917969,
      "learning_rate": 1.7398076923076926e-05,
      "loss": 0.2795,
      "step": 1354
    },
    {
      "epoch": 1.3028846153846154,
      "grad_norm": 7.688720703125,
      "learning_rate": 1.739615384615385e-05,
      "loss": 1.5132,
      "step": 1355
    },
    {
      "epoch": 1.3038461538461539,
      "grad_norm": 7.235333442687988,
      "learning_rate": 1.739423076923077e-05,
      "loss": 0.7136,
      "step": 1356
    },
    {
      "epoch": 1.3048076923076923,
      "grad_norm": 17.264623641967773,
      "learning_rate": 1.7392307692307694e-05,
      "loss": 0.248,
      "step": 1357
    },
    {
      "epoch": 1.3057692307692308,
      "grad_norm": 8.20783519744873,
      "learning_rate": 1.7390384615384617e-05,
      "loss": 0.6141,
      "step": 1358
    },
    {
      "epoch": 1.3067307692307693,
      "grad_norm": 11.677807807922363,
      "learning_rate": 1.738846153846154e-05,
      "loss": 1.0992,
      "step": 1359
    },
    {
      "epoch": 1.3076923076923077,
      "grad_norm": 6.331857681274414,
      "learning_rate": 1.7386538461538462e-05,
      "loss": 0.5923,
      "step": 1360
    },
    {
      "epoch": 1.3086538461538462,
      "grad_norm": 10.613923072814941,
      "learning_rate": 1.7384615384615385e-05,
      "loss": 1.74,
      "step": 1361
    },
    {
      "epoch": 1.3096153846153846,
      "grad_norm": 5.427294731140137,
      "learning_rate": 1.7382692307692308e-05,
      "loss": 1.4739,
      "step": 1362
    },
    {
      "epoch": 1.310576923076923,
      "grad_norm": 6.654175758361816,
      "learning_rate": 1.738076923076923e-05,
      "loss": 0.8066,
      "step": 1363
    },
    {
      "epoch": 1.3115384615384615,
      "grad_norm": 9.449716567993164,
      "learning_rate": 1.7378846153846157e-05,
      "loss": 0.6315,
      "step": 1364
    },
    {
      "epoch": 1.3125,
      "grad_norm": 6.365128993988037,
      "learning_rate": 1.737692307692308e-05,
      "loss": 0.8434,
      "step": 1365
    },
    {
      "epoch": 1.3134615384615385,
      "grad_norm": 9.45512866973877,
      "learning_rate": 1.7375000000000002e-05,
      "loss": 1.7479,
      "step": 1366
    },
    {
      "epoch": 1.314423076923077,
      "grad_norm": 8.040741920471191,
      "learning_rate": 1.7373076923076925e-05,
      "loss": 1.3761,
      "step": 1367
    },
    {
      "epoch": 1.3153846153846154,
      "grad_norm": 5.3983025550842285,
      "learning_rate": 1.7371153846153848e-05,
      "loss": 1.5438,
      "step": 1368
    },
    {
      "epoch": 1.3163461538461538,
      "grad_norm": 5.689659595489502,
      "learning_rate": 1.736923076923077e-05,
      "loss": 1.2083,
      "step": 1369
    },
    {
      "epoch": 1.3173076923076923,
      "grad_norm": 6.087823390960693,
      "learning_rate": 1.7367307692307693e-05,
      "loss": 1.4894,
      "step": 1370
    },
    {
      "epoch": 1.3182692307692307,
      "grad_norm": 8.104141235351562,
      "learning_rate": 1.7365384615384616e-05,
      "loss": 1.5441,
      "step": 1371
    },
    {
      "epoch": 1.3192307692307692,
      "grad_norm": 4.802700996398926,
      "learning_rate": 1.736346153846154e-05,
      "loss": 1.5896,
      "step": 1372
    },
    {
      "epoch": 1.3201923076923077,
      "grad_norm": 6.034360885620117,
      "learning_rate": 1.736153846153846e-05,
      "loss": 1.4653,
      "step": 1373
    },
    {
      "epoch": 1.3211538461538461,
      "grad_norm": 6.032492160797119,
      "learning_rate": 1.7359615384615388e-05,
      "loss": 1.3544,
      "step": 1374
    },
    {
      "epoch": 1.3221153846153846,
      "grad_norm": 7.635272026062012,
      "learning_rate": 1.7357692307692307e-05,
      "loss": 1.4093,
      "step": 1375
    },
    {
      "epoch": 1.323076923076923,
      "grad_norm": 7.761926651000977,
      "learning_rate": 1.7355769230769233e-05,
      "loss": 1.1095,
      "step": 1376
    },
    {
      "epoch": 1.3240384615384615,
      "grad_norm": 11.424717903137207,
      "learning_rate": 1.7353846153846156e-05,
      "loss": 1.8218,
      "step": 1377
    },
    {
      "epoch": 1.325,
      "grad_norm": 6.872424602508545,
      "learning_rate": 1.735192307692308e-05,
      "loss": 0.6246,
      "step": 1378
    },
    {
      "epoch": 1.3259615384615384,
      "grad_norm": 8.613903045654297,
      "learning_rate": 1.735e-05,
      "loss": 1.5493,
      "step": 1379
    },
    {
      "epoch": 1.3269230769230769,
      "grad_norm": 9.658441543579102,
      "learning_rate": 1.7348076923076924e-05,
      "loss": 1.4428,
      "step": 1380
    },
    {
      "epoch": 1.3278846153846153,
      "grad_norm": 5.825420379638672,
      "learning_rate": 1.7346153846153847e-05,
      "loss": 1.6206,
      "step": 1381
    },
    {
      "epoch": 1.3288461538461538,
      "grad_norm": 47.864044189453125,
      "learning_rate": 1.734423076923077e-05,
      "loss": 1.7422,
      "step": 1382
    },
    {
      "epoch": 1.3298076923076922,
      "grad_norm": 9.49191665649414,
      "learning_rate": 1.7342307692307693e-05,
      "loss": 1.4369,
      "step": 1383
    },
    {
      "epoch": 1.3307692307692307,
      "grad_norm": 10.578003883361816,
      "learning_rate": 1.734038461538462e-05,
      "loss": 1.6261,
      "step": 1384
    },
    {
      "epoch": 1.3317307692307692,
      "grad_norm": 7.554867744445801,
      "learning_rate": 1.7338461538461538e-05,
      "loss": 1.0679,
      "step": 1385
    },
    {
      "epoch": 1.3326923076923076,
      "grad_norm": 6.53312873840332,
      "learning_rate": 1.7336538461538464e-05,
      "loss": 0.769,
      "step": 1386
    },
    {
      "epoch": 1.333653846153846,
      "grad_norm": 9.805951118469238,
      "learning_rate": 1.7334615384615384e-05,
      "loss": 1.9625,
      "step": 1387
    },
    {
      "epoch": 1.3346153846153845,
      "grad_norm": 9.699800491333008,
      "learning_rate": 1.733269230769231e-05,
      "loss": 1.8711,
      "step": 1388
    },
    {
      "epoch": 1.335576923076923,
      "grad_norm": 7.738372802734375,
      "learning_rate": 1.7330769230769233e-05,
      "loss": 1.6635,
      "step": 1389
    },
    {
      "epoch": 1.3365384615384617,
      "grad_norm": 6.784471035003662,
      "learning_rate": 1.7328846153846155e-05,
      "loss": 0.0961,
      "step": 1390
    },
    {
      "epoch": 1.3375,
      "grad_norm": 8.36584186553955,
      "learning_rate": 1.7326923076923078e-05,
      "loss": 0.5968,
      "step": 1391
    },
    {
      "epoch": 1.3384615384615386,
      "grad_norm": 9.637786865234375,
      "learning_rate": 1.7325e-05,
      "loss": 1.7634,
      "step": 1392
    },
    {
      "epoch": 1.3394230769230768,
      "grad_norm": 5.519735336303711,
      "learning_rate": 1.7323076923076924e-05,
      "loss": 1.2731,
      "step": 1393
    },
    {
      "epoch": 1.3403846153846155,
      "grad_norm": 10.805088996887207,
      "learning_rate": 1.732115384615385e-05,
      "loss": 1.2186,
      "step": 1394
    },
    {
      "epoch": 1.3413461538461537,
      "grad_norm": 6.325198650360107,
      "learning_rate": 1.731923076923077e-05,
      "loss": 1.2005,
      "step": 1395
    },
    {
      "epoch": 1.3423076923076924,
      "grad_norm": 6.599369525909424,
      "learning_rate": 1.7317307692307695e-05,
      "loss": 1.4658,
      "step": 1396
    },
    {
      "epoch": 1.3432692307692307,
      "grad_norm": 7.914189338684082,
      "learning_rate": 1.7315384615384615e-05,
      "loss": 1.367,
      "step": 1397
    },
    {
      "epoch": 1.3442307692307693,
      "grad_norm": 5.872966289520264,
      "learning_rate": 1.731346153846154e-05,
      "loss": 1.5062,
      "step": 1398
    },
    {
      "epoch": 1.3451923076923076,
      "grad_norm": 4.215269088745117,
      "learning_rate": 1.731153846153846e-05,
      "loss": 1.6836,
      "step": 1399
    },
    {
      "epoch": 1.3461538461538463,
      "grad_norm": 6.971758842468262,
      "learning_rate": 1.7309615384615387e-05,
      "loss": 1.4075,
      "step": 1400
    },
    {
      "epoch": 1.3471153846153845,
      "grad_norm": 15.65677261352539,
      "learning_rate": 1.730769230769231e-05,
      "loss": 0.5079,
      "step": 1401
    },
    {
      "epoch": 1.3480769230769232,
      "grad_norm": 6.807415962219238,
      "learning_rate": 1.7305769230769232e-05,
      "loss": 1.4161,
      "step": 1402
    },
    {
      "epoch": 1.3490384615384614,
      "grad_norm": 4.903568267822266,
      "learning_rate": 1.7303846153846155e-05,
      "loss": 1.5806,
      "step": 1403
    },
    {
      "epoch": 1.35,
      "grad_norm": 8.300129890441895,
      "learning_rate": 1.7301923076923078e-05,
      "loss": 0.9933,
      "step": 1404
    },
    {
      "epoch": 1.3509615384615383,
      "grad_norm": 4.897279739379883,
      "learning_rate": 1.73e-05,
      "loss": 0.8955,
      "step": 1405
    },
    {
      "epoch": 1.351923076923077,
      "grad_norm": 5.362287521362305,
      "learning_rate": 1.7298076923076927e-05,
      "loss": 0.7076,
      "step": 1406
    },
    {
      "epoch": 1.3528846153846155,
      "grad_norm": 8.557710647583008,
      "learning_rate": 1.7296153846153846e-05,
      "loss": 0.9082,
      "step": 1407
    },
    {
      "epoch": 1.353846153846154,
      "grad_norm": 6.240246772766113,
      "learning_rate": 1.7294230769230772e-05,
      "loss": 0.9489,
      "step": 1408
    },
    {
      "epoch": 1.3548076923076924,
      "grad_norm": 7.9093523025512695,
      "learning_rate": 1.729230769230769e-05,
      "loss": 1.8645,
      "step": 1409
    },
    {
      "epoch": 1.3557692307692308,
      "grad_norm": 5.508688926696777,
      "learning_rate": 1.7290384615384618e-05,
      "loss": 0.8944,
      "step": 1410
    },
    {
      "epoch": 1.3567307692307693,
      "grad_norm": 6.1546759605407715,
      "learning_rate": 1.728846153846154e-05,
      "loss": 1.0583,
      "step": 1411
    },
    {
      "epoch": 1.3576923076923078,
      "grad_norm": 7.857935428619385,
      "learning_rate": 1.7286538461538463e-05,
      "loss": 2.0504,
      "step": 1412
    },
    {
      "epoch": 1.3586538461538462,
      "grad_norm": 7.251712322235107,
      "learning_rate": 1.7284615384615386e-05,
      "loss": 1.1343,
      "step": 1413
    },
    {
      "epoch": 1.3596153846153847,
      "grad_norm": 7.2329182624816895,
      "learning_rate": 1.728269230769231e-05,
      "loss": 1.1154,
      "step": 1414
    },
    {
      "epoch": 1.3605769230769231,
      "grad_norm": 8.829280853271484,
      "learning_rate": 1.728076923076923e-05,
      "loss": 0.7347,
      "step": 1415
    },
    {
      "epoch": 1.3615384615384616,
      "grad_norm": 9.499094009399414,
      "learning_rate": 1.7278846153846158e-05,
      "loss": 1.2308,
      "step": 1416
    },
    {
      "epoch": 1.3625,
      "grad_norm": 11.353334426879883,
      "learning_rate": 1.7276923076923077e-05,
      "loss": 1.0692,
      "step": 1417
    },
    {
      "epoch": 1.3634615384615385,
      "grad_norm": 6.668569087982178,
      "learning_rate": 1.7275000000000003e-05,
      "loss": 1.4104,
      "step": 1418
    },
    {
      "epoch": 1.364423076923077,
      "grad_norm": 8.95974349975586,
      "learning_rate": 1.7273076923076923e-05,
      "loss": 0.9071,
      "step": 1419
    },
    {
      "epoch": 1.3653846153846154,
      "grad_norm": 7.875505447387695,
      "learning_rate": 1.727115384615385e-05,
      "loss": 1.5513,
      "step": 1420
    },
    {
      "epoch": 1.3663461538461539,
      "grad_norm": 8.254481315612793,
      "learning_rate": 1.726923076923077e-05,
      "loss": 1.525,
      "step": 1421
    },
    {
      "epoch": 1.3673076923076923,
      "grad_norm": 12.253824234008789,
      "learning_rate": 1.7267307692307694e-05,
      "loss": 0.6716,
      "step": 1422
    },
    {
      "epoch": 1.3682692307692308,
      "grad_norm": 6.592871189117432,
      "learning_rate": 1.7265384615384617e-05,
      "loss": 0.8409,
      "step": 1423
    },
    {
      "epoch": 1.3692307692307693,
      "grad_norm": 9.980037689208984,
      "learning_rate": 1.726346153846154e-05,
      "loss": 1.5556,
      "step": 1424
    },
    {
      "epoch": 1.3701923076923077,
      "grad_norm": 12.377099990844727,
      "learning_rate": 1.7261538461538463e-05,
      "loss": 0.4673,
      "step": 1425
    },
    {
      "epoch": 1.3711538461538462,
      "grad_norm": 6.009430885314941,
      "learning_rate": 1.7259615384615385e-05,
      "loss": 1.3863,
      "step": 1426
    },
    {
      "epoch": 1.3721153846153846,
      "grad_norm": 7.2208404541015625,
      "learning_rate": 1.7257692307692308e-05,
      "loss": 1.3361,
      "step": 1427
    },
    {
      "epoch": 1.373076923076923,
      "grad_norm": 12.669790267944336,
      "learning_rate": 1.7255769230769234e-05,
      "loss": 0.1486,
      "step": 1428
    },
    {
      "epoch": 1.3740384615384615,
      "grad_norm": 7.9066691398620605,
      "learning_rate": 1.7253846153846154e-05,
      "loss": 1.2725,
      "step": 1429
    },
    {
      "epoch": 1.375,
      "grad_norm": 5.287196636199951,
      "learning_rate": 1.725192307692308e-05,
      "loss": 1.96,
      "step": 1430
    },
    {
      "epoch": 1.3759615384615385,
      "grad_norm": 9.302237510681152,
      "learning_rate": 1.7250000000000003e-05,
      "loss": 1.3938,
      "step": 1431
    },
    {
      "epoch": 1.376923076923077,
      "grad_norm": 7.928627967834473,
      "learning_rate": 1.7248076923076925e-05,
      "loss": 1.7253,
      "step": 1432
    },
    {
      "epoch": 1.3778846153846154,
      "grad_norm": 9.466836929321289,
      "learning_rate": 1.7246153846153848e-05,
      "loss": 1.857,
      "step": 1433
    },
    {
      "epoch": 1.3788461538461538,
      "grad_norm": 9.194687843322754,
      "learning_rate": 1.724423076923077e-05,
      "loss": 1.4322,
      "step": 1434
    },
    {
      "epoch": 1.3798076923076923,
      "grad_norm": 7.420426845550537,
      "learning_rate": 1.7242307692307694e-05,
      "loss": 1.3579,
      "step": 1435
    },
    {
      "epoch": 1.3807692307692307,
      "grad_norm": 13.619379997253418,
      "learning_rate": 1.7240384615384616e-05,
      "loss": 1.4794,
      "step": 1436
    },
    {
      "epoch": 1.3817307692307692,
      "grad_norm": 5.809659957885742,
      "learning_rate": 1.723846153846154e-05,
      "loss": 0.4681,
      "step": 1437
    },
    {
      "epoch": 1.3826923076923077,
      "grad_norm": 6.729851722717285,
      "learning_rate": 1.7236538461538462e-05,
      "loss": 1.5648,
      "step": 1438
    },
    {
      "epoch": 1.3836538461538461,
      "grad_norm": 5.105946063995361,
      "learning_rate": 1.7234615384615385e-05,
      "loss": 1.5903,
      "step": 1439
    },
    {
      "epoch": 1.3846153846153846,
      "grad_norm": 7.354580402374268,
      "learning_rate": 1.7232692307692307e-05,
      "loss": 1.9201,
      "step": 1440
    },
    {
      "epoch": 1.385576923076923,
      "grad_norm": 9.634486198425293,
      "learning_rate": 1.7230769230769234e-05,
      "loss": 0.9974,
      "step": 1441
    },
    {
      "epoch": 1.3865384615384615,
      "grad_norm": 1.9297858476638794,
      "learning_rate": 1.7228846153846156e-05,
      "loss": 0.0203,
      "step": 1442
    },
    {
      "epoch": 1.3875,
      "grad_norm": 6.4858198165893555,
      "learning_rate": 1.722692307692308e-05,
      "loss": 1.3044,
      "step": 1443
    },
    {
      "epoch": 1.3884615384615384,
      "grad_norm": 5.374757766723633,
      "learning_rate": 1.7225000000000002e-05,
      "loss": 1.7529,
      "step": 1444
    },
    {
      "epoch": 1.3894230769230769,
      "grad_norm": 13.971177101135254,
      "learning_rate": 1.7223076923076925e-05,
      "loss": 1.2176,
      "step": 1445
    },
    {
      "epoch": 1.3903846153846153,
      "grad_norm": 9.006380081176758,
      "learning_rate": 1.7221153846153847e-05,
      "loss": 1.37,
      "step": 1446
    },
    {
      "epoch": 1.3913461538461538,
      "grad_norm": 5.51453161239624,
      "learning_rate": 1.721923076923077e-05,
      "loss": 1.5719,
      "step": 1447
    },
    {
      "epoch": 1.3923076923076922,
      "grad_norm": 5.817105293273926,
      "learning_rate": 1.7217307692307693e-05,
      "loss": 1.3366,
      "step": 1448
    },
    {
      "epoch": 1.3932692307692307,
      "grad_norm": 20.5411319732666,
      "learning_rate": 1.7215384615384616e-05,
      "loss": 0.9466,
      "step": 1449
    },
    {
      "epoch": 1.3942307692307692,
      "grad_norm": 7.952486991882324,
      "learning_rate": 1.721346153846154e-05,
      "loss": 1.5915,
      "step": 1450
    },
    {
      "epoch": 1.3951923076923076,
      "grad_norm": 6.481051921844482,
      "learning_rate": 1.7211538461538465e-05,
      "loss": 0.8774,
      "step": 1451
    },
    {
      "epoch": 1.396153846153846,
      "grad_norm": 5.177177429199219,
      "learning_rate": 1.7209615384615384e-05,
      "loss": 0.15,
      "step": 1452
    },
    {
      "epoch": 1.3971153846153845,
      "grad_norm": 6.962122917175293,
      "learning_rate": 1.720769230769231e-05,
      "loss": 1.0009,
      "step": 1453
    },
    {
      "epoch": 1.398076923076923,
      "grad_norm": 6.455193519592285,
      "learning_rate": 1.7205769230769233e-05,
      "loss": 1.3991,
      "step": 1454
    },
    {
      "epoch": 1.3990384615384617,
      "grad_norm": 9.19315242767334,
      "learning_rate": 1.7203846153846156e-05,
      "loss": 1.7494,
      "step": 1455
    },
    {
      "epoch": 1.4,
      "grad_norm": 8.477821350097656,
      "learning_rate": 1.720192307692308e-05,
      "loss": 0.1993,
      "step": 1456
    },
    {
      "epoch": 1.4009615384615386,
      "grad_norm": 9.037496566772461,
      "learning_rate": 1.72e-05,
      "loss": 0.4951,
      "step": 1457
    },
    {
      "epoch": 1.4019230769230768,
      "grad_norm": 22.078493118286133,
      "learning_rate": 1.7198076923076924e-05,
      "loss": 0.2215,
      "step": 1458
    },
    {
      "epoch": 1.4028846153846155,
      "grad_norm": 8.282703399658203,
      "learning_rate": 1.7196153846153847e-05,
      "loss": 1.1303,
      "step": 1459
    },
    {
      "epoch": 1.4038461538461537,
      "grad_norm": 6.202552318572998,
      "learning_rate": 1.719423076923077e-05,
      "loss": 1.5611,
      "step": 1460
    },
    {
      "epoch": 1.4048076923076924,
      "grad_norm": 11.39491081237793,
      "learning_rate": 1.7192307692307696e-05,
      "loss": 0.199,
      "step": 1461
    },
    {
      "epoch": 1.4057692307692307,
      "grad_norm": 5.23662805557251,
      "learning_rate": 1.7190384615384615e-05,
      "loss": 1.8382,
      "step": 1462
    },
    {
      "epoch": 1.4067307692307693,
      "grad_norm": 6.642453670501709,
      "learning_rate": 1.718846153846154e-05,
      "loss": 1.1704,
      "step": 1463
    },
    {
      "epoch": 1.4076923076923076,
      "grad_norm": 11.128928184509277,
      "learning_rate": 1.718653846153846e-05,
      "loss": 1.1167,
      "step": 1464
    },
    {
      "epoch": 1.4086538461538463,
      "grad_norm": 8.589539527893066,
      "learning_rate": 1.7184615384615387e-05,
      "loss": 1.0183,
      "step": 1465
    },
    {
      "epoch": 1.4096153846153845,
      "grad_norm": 5.5317864418029785,
      "learning_rate": 1.718269230769231e-05,
      "loss": 1.3854,
      "step": 1466
    },
    {
      "epoch": 1.4105769230769232,
      "grad_norm": 6.435554504394531,
      "learning_rate": 1.7180769230769232e-05,
      "loss": 0.0612,
      "step": 1467
    },
    {
      "epoch": 1.4115384615384614,
      "grad_norm": 5.709028244018555,
      "learning_rate": 1.7178846153846155e-05,
      "loss": 2.0384,
      "step": 1468
    },
    {
      "epoch": 1.4125,
      "grad_norm": 10.096856117248535,
      "learning_rate": 1.7176923076923078e-05,
      "loss": 1.0603,
      "step": 1469
    },
    {
      "epoch": 1.4134615384615383,
      "grad_norm": 8.744699478149414,
      "learning_rate": 1.7175e-05,
      "loss": 1.1902,
      "step": 1470
    },
    {
      "epoch": 1.414423076923077,
      "grad_norm": 10.074049949645996,
      "learning_rate": 1.7173076923076924e-05,
      "loss": 1.3131,
      "step": 1471
    },
    {
      "epoch": 1.4153846153846155,
      "grad_norm": 6.445004940032959,
      "learning_rate": 1.7171153846153846e-05,
      "loss": 1.9977,
      "step": 1472
    },
    {
      "epoch": 1.416346153846154,
      "grad_norm": 5.802604675292969,
      "learning_rate": 1.7169230769230772e-05,
      "loss": 1.1078,
      "step": 1473
    },
    {
      "epoch": 1.4173076923076924,
      "grad_norm": 7.921639919281006,
      "learning_rate": 1.7167307692307692e-05,
      "loss": 1.7511,
      "step": 1474
    },
    {
      "epoch": 1.4182692307692308,
      "grad_norm": 7.115299701690674,
      "learning_rate": 1.7165384615384618e-05,
      "loss": 1.2804,
      "step": 1475
    },
    {
      "epoch": 1.4192307692307693,
      "grad_norm": 8.981890678405762,
      "learning_rate": 1.7163461538461537e-05,
      "loss": 1.1783,
      "step": 1476
    },
    {
      "epoch": 1.4201923076923078,
      "grad_norm": 7.47222900390625,
      "learning_rate": 1.7161538461538464e-05,
      "loss": 0.2795,
      "step": 1477
    },
    {
      "epoch": 1.4211538461538462,
      "grad_norm": 6.3866095542907715,
      "learning_rate": 1.7159615384615386e-05,
      "loss": 1.6588,
      "step": 1478
    },
    {
      "epoch": 1.4221153846153847,
      "grad_norm": 7.416135787963867,
      "learning_rate": 1.715769230769231e-05,
      "loss": 1.3297,
      "step": 1479
    },
    {
      "epoch": 1.4230769230769231,
      "grad_norm": 4.607891082763672,
      "learning_rate": 1.7155769230769232e-05,
      "loss": 1.3629,
      "step": 1480
    },
    {
      "epoch": 1.4240384615384616,
      "grad_norm": 7.986772060394287,
      "learning_rate": 1.7153846153846155e-05,
      "loss": 1.4531,
      "step": 1481
    },
    {
      "epoch": 1.425,
      "grad_norm": 6.062914848327637,
      "learning_rate": 1.7151923076923077e-05,
      "loss": 1.5768,
      "step": 1482
    },
    {
      "epoch": 1.4259615384615385,
      "grad_norm": 8.332566261291504,
      "learning_rate": 1.7150000000000004e-05,
      "loss": 0.8083,
      "step": 1483
    },
    {
      "epoch": 1.426923076923077,
      "grad_norm": 10.078559875488281,
      "learning_rate": 1.7148076923076923e-05,
      "loss": 0.5138,
      "step": 1484
    },
    {
      "epoch": 1.4278846153846154,
      "grad_norm": 11.07079029083252,
      "learning_rate": 1.714615384615385e-05,
      "loss": 1.1335,
      "step": 1485
    },
    {
      "epoch": 1.4288461538461539,
      "grad_norm": 6.791167736053467,
      "learning_rate": 1.714423076923077e-05,
      "loss": 1.11,
      "step": 1486
    },
    {
      "epoch": 1.4298076923076923,
      "grad_norm": 6.413769245147705,
      "learning_rate": 1.7142307692307695e-05,
      "loss": 1.2684,
      "step": 1487
    },
    {
      "epoch": 1.4307692307692308,
      "grad_norm": 7.967136383056641,
      "learning_rate": 1.7140384615384617e-05,
      "loss": 1.4111,
      "step": 1488
    },
    {
      "epoch": 1.4317307692307693,
      "grad_norm": 6.261098384857178,
      "learning_rate": 1.713846153846154e-05,
      "loss": 1.3604,
      "step": 1489
    },
    {
      "epoch": 1.4326923076923077,
      "grad_norm": 6.046470642089844,
      "learning_rate": 1.7136538461538463e-05,
      "loss": 1.3685,
      "step": 1490
    },
    {
      "epoch": 1.4336538461538462,
      "grad_norm": 9.4185791015625,
      "learning_rate": 1.7134615384615386e-05,
      "loss": 1.2282,
      "step": 1491
    },
    {
      "epoch": 1.4346153846153846,
      "grad_norm": 8.258381843566895,
      "learning_rate": 1.713269230769231e-05,
      "loss": 1.9168,
      "step": 1492
    },
    {
      "epoch": 1.435576923076923,
      "grad_norm": 5.6182684898376465,
      "learning_rate": 1.7130769230769235e-05,
      "loss": 1.3288,
      "step": 1493
    },
    {
      "epoch": 1.4365384615384615,
      "grad_norm": 6.27933406829834,
      "learning_rate": 1.7128846153846154e-05,
      "loss": 0.5033,
      "step": 1494
    },
    {
      "epoch": 1.4375,
      "grad_norm": 7.018559455871582,
      "learning_rate": 1.712692307692308e-05,
      "loss": 1.9603,
      "step": 1495
    },
    {
      "epoch": 1.4384615384615385,
      "grad_norm": 6.791259765625,
      "learning_rate": 1.7125e-05,
      "loss": 1.1719,
      "step": 1496
    },
    {
      "epoch": 1.439423076923077,
      "grad_norm": 7.697914123535156,
      "learning_rate": 1.7123076923076926e-05,
      "loss": 1.6102,
      "step": 1497
    },
    {
      "epoch": 1.4403846153846154,
      "grad_norm": 6.150912761688232,
      "learning_rate": 1.712115384615385e-05,
      "loss": 1.0822,
      "step": 1498
    },
    {
      "epoch": 1.4413461538461538,
      "grad_norm": 5.247795581817627,
      "learning_rate": 1.711923076923077e-05,
      "loss": 0.0643,
      "step": 1499
    },
    {
      "epoch": 1.4423076923076923,
      "grad_norm": 5.592288017272949,
      "learning_rate": 1.7117307692307694e-05,
      "loss": 1.5382,
      "step": 1500
    },
    {
      "epoch": 1.4432692307692307,
      "grad_norm": 5.884547710418701,
      "learning_rate": 1.7115384615384617e-05,
      "loss": 1.4518,
      "step": 1501
    },
    {
      "epoch": 1.4442307692307692,
      "grad_norm": 6.182272911071777,
      "learning_rate": 1.711346153846154e-05,
      "loss": 2.0537,
      "step": 1502
    },
    {
      "epoch": 1.4451923076923077,
      "grad_norm": 7.194971561431885,
      "learning_rate": 1.7111538461538462e-05,
      "loss": 1.2143,
      "step": 1503
    },
    {
      "epoch": 1.4461538461538461,
      "grad_norm": 11.91676139831543,
      "learning_rate": 1.7109615384615385e-05,
      "loss": 1.2316,
      "step": 1504
    },
    {
      "epoch": 1.4471153846153846,
      "grad_norm": 11.071151733398438,
      "learning_rate": 1.710769230769231e-05,
      "loss": 1.4705,
      "step": 1505
    },
    {
      "epoch": 1.448076923076923,
      "grad_norm": 9.969399452209473,
      "learning_rate": 1.710576923076923e-05,
      "loss": 0.6524,
      "step": 1506
    },
    {
      "epoch": 1.4490384615384615,
      "grad_norm": 7.860743522644043,
      "learning_rate": 1.7103846153846157e-05,
      "loss": 1.1682,
      "step": 1507
    },
    {
      "epoch": 1.45,
      "grad_norm": 11.222683906555176,
      "learning_rate": 1.710192307692308e-05,
      "loss": 1.4471,
      "step": 1508
    },
    {
      "epoch": 1.4509615384615384,
      "grad_norm": 8.595974922180176,
      "learning_rate": 1.7100000000000002e-05,
      "loss": 1.4445,
      "step": 1509
    },
    {
      "epoch": 1.4519230769230769,
      "grad_norm": 7.49821662902832,
      "learning_rate": 1.7098076923076925e-05,
      "loss": 1.0416,
      "step": 1510
    },
    {
      "epoch": 1.4528846153846153,
      "grad_norm": 5.923517227172852,
      "learning_rate": 1.7096153846153848e-05,
      "loss": 0.6648,
      "step": 1511
    },
    {
      "epoch": 1.4538461538461538,
      "grad_norm": 6.294576644897461,
      "learning_rate": 1.709423076923077e-05,
      "loss": 1.7139,
      "step": 1512
    },
    {
      "epoch": 1.4548076923076922,
      "grad_norm": 4.18669319152832,
      "learning_rate": 1.7092307692307693e-05,
      "loss": 0.0907,
      "step": 1513
    },
    {
      "epoch": 1.4557692307692307,
      "grad_norm": 11.99100112915039,
      "learning_rate": 1.7090384615384616e-05,
      "loss": 1.2078,
      "step": 1514
    },
    {
      "epoch": 1.4567307692307692,
      "grad_norm": 7.110445976257324,
      "learning_rate": 1.708846153846154e-05,
      "loss": 1.0005,
      "step": 1515
    },
    {
      "epoch": 1.4576923076923076,
      "grad_norm": 10.582849502563477,
      "learning_rate": 1.708653846153846e-05,
      "loss": 0.8994,
      "step": 1516
    },
    {
      "epoch": 1.458653846153846,
      "grad_norm": 13.399150848388672,
      "learning_rate": 1.7084615384615388e-05,
      "loss": 1.525,
      "step": 1517
    },
    {
      "epoch": 1.4596153846153845,
      "grad_norm": 6.67477560043335,
      "learning_rate": 1.708269230769231e-05,
      "loss": 1.4471,
      "step": 1518
    },
    {
      "epoch": 1.460576923076923,
      "grad_norm": 14.855596542358398,
      "learning_rate": 1.7080769230769233e-05,
      "loss": 1.7867,
      "step": 1519
    },
    {
      "epoch": 1.4615384615384617,
      "grad_norm": 6.660310745239258,
      "learning_rate": 1.7078846153846156e-05,
      "loss": 1.6514,
      "step": 1520
    },
    {
      "epoch": 1.4625,
      "grad_norm": 9.487617492675781,
      "learning_rate": 1.707692307692308e-05,
      "loss": 2.2177,
      "step": 1521
    },
    {
      "epoch": 1.4634615384615386,
      "grad_norm": 6.617308616638184,
      "learning_rate": 1.7075e-05,
      "loss": 1.697,
      "step": 1522
    },
    {
      "epoch": 1.4644230769230768,
      "grad_norm": 7.002877712249756,
      "learning_rate": 1.7073076923076924e-05,
      "loss": 1.3949,
      "step": 1523
    },
    {
      "epoch": 1.4653846153846155,
      "grad_norm": 9.964168548583984,
      "learning_rate": 1.7071153846153847e-05,
      "loss": 1.2992,
      "step": 1524
    },
    {
      "epoch": 1.4663461538461537,
      "grad_norm": 6.946832180023193,
      "learning_rate": 1.706923076923077e-05,
      "loss": 0.8314,
      "step": 1525
    },
    {
      "epoch": 1.4673076923076924,
      "grad_norm": 8.986841201782227,
      "learning_rate": 1.7067307692307693e-05,
      "loss": 1.213,
      "step": 1526
    },
    {
      "epoch": 1.4682692307692307,
      "grad_norm": 9.921128273010254,
      "learning_rate": 1.7065384615384616e-05,
      "loss": 0.7447,
      "step": 1527
    },
    {
      "epoch": 1.4692307692307693,
      "grad_norm": 8.5172119140625,
      "learning_rate": 1.7063461538461542e-05,
      "loss": 0.8037,
      "step": 1528
    },
    {
      "epoch": 1.4701923076923076,
      "grad_norm": 6.066137313842773,
      "learning_rate": 1.706153846153846e-05,
      "loss": 0.4215,
      "step": 1529
    },
    {
      "epoch": 1.4711538461538463,
      "grad_norm": 9.057004928588867,
      "learning_rate": 1.7059615384615387e-05,
      "loss": 1.8639,
      "step": 1530
    },
    {
      "epoch": 1.4721153846153845,
      "grad_norm": 7.7755126953125,
      "learning_rate": 1.705769230769231e-05,
      "loss": 0.6633,
      "step": 1531
    },
    {
      "epoch": 1.4730769230769232,
      "grad_norm": 12.189204216003418,
      "learning_rate": 1.7055769230769233e-05,
      "loss": 0.6818,
      "step": 1532
    },
    {
      "epoch": 1.4740384615384614,
      "grad_norm": 7.862501621246338,
      "learning_rate": 1.7053846153846156e-05,
      "loss": 1.6497,
      "step": 1533
    },
    {
      "epoch": 1.475,
      "grad_norm": 7.0235443115234375,
      "learning_rate": 1.705192307692308e-05,
      "loss": 1.2405,
      "step": 1534
    },
    {
      "epoch": 1.4759615384615383,
      "grad_norm": 17.76008415222168,
      "learning_rate": 1.705e-05,
      "loss": 1.8197,
      "step": 1535
    },
    {
      "epoch": 1.476923076923077,
      "grad_norm": 5.365699768066406,
      "learning_rate": 1.7048076923076924e-05,
      "loss": 0.2276,
      "step": 1536
    },
    {
      "epoch": 1.4778846153846155,
      "grad_norm": 5.592864036560059,
      "learning_rate": 1.7046153846153847e-05,
      "loss": 1.4613,
      "step": 1537
    },
    {
      "epoch": 1.478846153846154,
      "grad_norm": 6.280494689941406,
      "learning_rate": 1.704423076923077e-05,
      "loss": 1.2855,
      "step": 1538
    },
    {
      "epoch": 1.4798076923076924,
      "grad_norm": 8.441412925720215,
      "learning_rate": 1.7042307692307692e-05,
      "loss": 1.2529,
      "step": 1539
    },
    {
      "epoch": 1.4807692307692308,
      "grad_norm": 6.743664741516113,
      "learning_rate": 1.704038461538462e-05,
      "loss": 1.1979,
      "step": 1540
    },
    {
      "epoch": 1.4817307692307693,
      "grad_norm": 5.138129711151123,
      "learning_rate": 1.7038461538461538e-05,
      "loss": 1.2973,
      "step": 1541
    },
    {
      "epoch": 1.4826923076923078,
      "grad_norm": 16.193660736083984,
      "learning_rate": 1.7036538461538464e-05,
      "loss": 1.3754,
      "step": 1542
    },
    {
      "epoch": 1.4836538461538462,
      "grad_norm": 29.536542892456055,
      "learning_rate": 1.7034615384615387e-05,
      "loss": 1.8185,
      "step": 1543
    },
    {
      "epoch": 1.4846153846153847,
      "grad_norm": 9.325776100158691,
      "learning_rate": 1.703269230769231e-05,
      "loss": 0.7085,
      "step": 1544
    },
    {
      "epoch": 1.4855769230769231,
      "grad_norm": 6.636033058166504,
      "learning_rate": 1.7030769230769232e-05,
      "loss": 0.2577,
      "step": 1545
    },
    {
      "epoch": 1.4865384615384616,
      "grad_norm": 6.522423267364502,
      "learning_rate": 1.7028846153846155e-05,
      "loss": 1.8358,
      "step": 1546
    },
    {
      "epoch": 1.4875,
      "grad_norm": 25.21355628967285,
      "learning_rate": 1.7026923076923078e-05,
      "loss": 0.4098,
      "step": 1547
    },
    {
      "epoch": 1.4884615384615385,
      "grad_norm": 5.188690662384033,
      "learning_rate": 1.7025e-05,
      "loss": 1.4024,
      "step": 1548
    },
    {
      "epoch": 1.489423076923077,
      "grad_norm": 7.262436866760254,
      "learning_rate": 1.7023076923076923e-05,
      "loss": 1.8853,
      "step": 1549
    },
    {
      "epoch": 1.4903846153846154,
      "grad_norm": 12.50891399383545,
      "learning_rate": 1.702115384615385e-05,
      "loss": 1.7867,
      "step": 1550
    },
    {
      "epoch": 1.4913461538461539,
      "grad_norm": 12.107137680053711,
      "learning_rate": 1.701923076923077e-05,
      "loss": 1.2241,
      "step": 1551
    },
    {
      "epoch": 1.4923076923076923,
      "grad_norm": 6.716769695281982,
      "learning_rate": 1.7017307692307695e-05,
      "loss": 1.5407,
      "step": 1552
    },
    {
      "epoch": 1.4932692307692308,
      "grad_norm": 5.340530872344971,
      "learning_rate": 1.7015384615384614e-05,
      "loss": 0.0556,
      "step": 1553
    },
    {
      "epoch": 1.4942307692307693,
      "grad_norm": 8.068318367004395,
      "learning_rate": 1.701346153846154e-05,
      "loss": 1.5027,
      "step": 1554
    },
    {
      "epoch": 1.4951923076923077,
      "grad_norm": 7.32456636428833,
      "learning_rate": 1.7011538461538463e-05,
      "loss": 0.7076,
      "step": 1555
    },
    {
      "epoch": 1.4961538461538462,
      "grad_norm": 8.723642349243164,
      "learning_rate": 1.7009615384615386e-05,
      "loss": 1.7954,
      "step": 1556
    },
    {
      "epoch": 1.4971153846153846,
      "grad_norm": 5.577590465545654,
      "learning_rate": 1.700769230769231e-05,
      "loss": 1.2589,
      "step": 1557
    },
    {
      "epoch": 1.498076923076923,
      "grad_norm": 17.493938446044922,
      "learning_rate": 1.700576923076923e-05,
      "loss": 0.5511,
      "step": 1558
    },
    {
      "epoch": 1.4990384615384615,
      "grad_norm": 8.511760711669922,
      "learning_rate": 1.7003846153846154e-05,
      "loss": 1.4417,
      "step": 1559
    },
    {
      "epoch": 1.5,
      "grad_norm": 6.944801330566406,
      "learning_rate": 1.700192307692308e-05,
      "loss": 1.5448,
      "step": 1560
    },
    {
      "epoch": 1.5009615384615385,
      "grad_norm": 9.695072174072266,
      "learning_rate": 1.7e-05,
      "loss": 0.3732,
      "step": 1561
    },
    {
      "epoch": 1.501923076923077,
      "grad_norm": 5.893626689910889,
      "learning_rate": 1.6998076923076926e-05,
      "loss": 1.2906,
      "step": 1562
    },
    {
      "epoch": 1.5028846153846154,
      "grad_norm": 5.686310291290283,
      "learning_rate": 1.6996153846153845e-05,
      "loss": 1.1652,
      "step": 1563
    },
    {
      "epoch": 1.5038461538461538,
      "grad_norm": 102.9662857055664,
      "learning_rate": 1.699423076923077e-05,
      "loss": 0.853,
      "step": 1564
    },
    {
      "epoch": 1.5048076923076923,
      "grad_norm": 4.682920455932617,
      "learning_rate": 1.6992307692307694e-05,
      "loss": 1.7553,
      "step": 1565
    },
    {
      "epoch": 1.5057692307692307,
      "grad_norm": 11.430030822753906,
      "learning_rate": 1.6990384615384617e-05,
      "loss": 1.0943,
      "step": 1566
    },
    {
      "epoch": 1.5067307692307692,
      "grad_norm": 8.761764526367188,
      "learning_rate": 1.698846153846154e-05,
      "loss": 1.3264,
      "step": 1567
    },
    {
      "epoch": 1.5076923076923077,
      "grad_norm": 6.8854804039001465,
      "learning_rate": 1.6986538461538463e-05,
      "loss": 1.3156,
      "step": 1568
    },
    {
      "epoch": 1.5086538461538461,
      "grad_norm": 10.497163772583008,
      "learning_rate": 1.6984615384615385e-05,
      "loss": 0.9374,
      "step": 1569
    },
    {
      "epoch": 1.5096153846153846,
      "grad_norm": 9.948874473571777,
      "learning_rate": 1.698269230769231e-05,
      "loss": 1.0018,
      "step": 1570
    },
    {
      "epoch": 1.510576923076923,
      "grad_norm": 10.053153038024902,
      "learning_rate": 1.698076923076923e-05,
      "loss": 1.3641,
      "step": 1571
    },
    {
      "epoch": 1.5115384615384615,
      "grad_norm": 11.472087860107422,
      "learning_rate": 1.6978846153846157e-05,
      "loss": 0.3448,
      "step": 1572
    },
    {
      "epoch": 1.5125,
      "grad_norm": 14.613621711730957,
      "learning_rate": 1.6976923076923076e-05,
      "loss": 0.7481,
      "step": 1573
    },
    {
      "epoch": 1.5134615384615384,
      "grad_norm": 4.843701362609863,
      "learning_rate": 1.6975000000000003e-05,
      "loss": 0.6553,
      "step": 1574
    },
    {
      "epoch": 1.5144230769230769,
      "grad_norm": 23.37299919128418,
      "learning_rate": 1.6973076923076925e-05,
      "loss": 0.9161,
      "step": 1575
    },
    {
      "epoch": 1.5153846153846153,
      "grad_norm": 12.885642051696777,
      "learning_rate": 1.6971153846153848e-05,
      "loss": 1.7492,
      "step": 1576
    },
    {
      "epoch": 1.516346153846154,
      "grad_norm": 6.566417694091797,
      "learning_rate": 1.696923076923077e-05,
      "loss": 1.0166,
      "step": 1577
    },
    {
      "epoch": 1.5173076923076922,
      "grad_norm": 8.35442066192627,
      "learning_rate": 1.6967307692307694e-05,
      "loss": 1.7715,
      "step": 1578
    },
    {
      "epoch": 1.518269230769231,
      "grad_norm": 10.179828643798828,
      "learning_rate": 1.6965384615384616e-05,
      "loss": 1.1345,
      "step": 1579
    },
    {
      "epoch": 1.5192307692307692,
      "grad_norm": 4.6672587394714355,
      "learning_rate": 1.696346153846154e-05,
      "loss": 1.6968,
      "step": 1580
    },
    {
      "epoch": 1.5201923076923078,
      "grad_norm": 4.541895866394043,
      "learning_rate": 1.6961538461538462e-05,
      "loss": 0.571,
      "step": 1581
    },
    {
      "epoch": 1.521153846153846,
      "grad_norm": 7.32155179977417,
      "learning_rate": 1.6959615384615388e-05,
      "loss": 1.9098,
      "step": 1582
    },
    {
      "epoch": 1.5221153846153848,
      "grad_norm": 7.724240303039551,
      "learning_rate": 1.6957692307692308e-05,
      "loss": 1.7098,
      "step": 1583
    },
    {
      "epoch": 1.523076923076923,
      "grad_norm": 6.140933036804199,
      "learning_rate": 1.6955769230769234e-05,
      "loss": 1.1964,
      "step": 1584
    },
    {
      "epoch": 1.5240384615384617,
      "grad_norm": 8.21030044555664,
      "learning_rate": 1.6953846153846156e-05,
      "loss": 0.3198,
      "step": 1585
    },
    {
      "epoch": 1.525,
      "grad_norm": 12.197905540466309,
      "learning_rate": 1.695192307692308e-05,
      "loss": 1.5516,
      "step": 1586
    },
    {
      "epoch": 1.5259615384615386,
      "grad_norm": 5.929342746734619,
      "learning_rate": 1.6950000000000002e-05,
      "loss": 1.5187,
      "step": 1587
    },
    {
      "epoch": 1.5269230769230768,
      "grad_norm": 4.930322170257568,
      "learning_rate": 1.6948076923076925e-05,
      "loss": 0.9695,
      "step": 1588
    },
    {
      "epoch": 1.5278846153846155,
      "grad_norm": 11.508671760559082,
      "learning_rate": 1.6946153846153848e-05,
      "loss": 2.1224,
      "step": 1589
    },
    {
      "epoch": 1.5288461538461537,
      "grad_norm": 7.750954627990723,
      "learning_rate": 1.694423076923077e-05,
      "loss": 0.2788,
      "step": 1590
    },
    {
      "epoch": 1.5298076923076924,
      "grad_norm": 5.760194778442383,
      "learning_rate": 1.6942307692307693e-05,
      "loss": 1.7492,
      "step": 1591
    },
    {
      "epoch": 1.5307692307692307,
      "grad_norm": 9.115992546081543,
      "learning_rate": 1.6940384615384616e-05,
      "loss": 1.4287,
      "step": 1592
    },
    {
      "epoch": 1.5317307692307693,
      "grad_norm": 2.2192628383636475,
      "learning_rate": 1.693846153846154e-05,
      "loss": 0.0232,
      "step": 1593
    },
    {
      "epoch": 1.5326923076923076,
      "grad_norm": 24.323917388916016,
      "learning_rate": 1.6936538461538465e-05,
      "loss": 0.5111,
      "step": 1594
    },
    {
      "epoch": 1.5336538461538463,
      "grad_norm": 7.169609069824219,
      "learning_rate": 1.6934615384615384e-05,
      "loss": 1.7135,
      "step": 1595
    },
    {
      "epoch": 1.5346153846153845,
      "grad_norm": 10.779475212097168,
      "learning_rate": 1.693269230769231e-05,
      "loss": 1.5385,
      "step": 1596
    },
    {
      "epoch": 1.5355769230769232,
      "grad_norm": 9.430338859558105,
      "learning_rate": 1.6930769230769233e-05,
      "loss": 1.2867,
      "step": 1597
    },
    {
      "epoch": 1.5365384615384614,
      "grad_norm": 4.898049831390381,
      "learning_rate": 1.6928846153846156e-05,
      "loss": 0.8156,
      "step": 1598
    },
    {
      "epoch": 1.5375,
      "grad_norm": 8.543061256408691,
      "learning_rate": 1.692692307692308e-05,
      "loss": 0.8491,
      "step": 1599
    },
    {
      "epoch": 1.5384615384615383,
      "grad_norm": 6.818922996520996,
      "learning_rate": 1.6925e-05,
      "loss": 1.0423,
      "step": 1600
    },
    {
      "epoch": 1.539423076923077,
      "grad_norm": 7.227365970611572,
      "learning_rate": 1.6923076923076924e-05,
      "loss": 1.4233,
      "step": 1601
    },
    {
      "epoch": 1.5403846153846152,
      "grad_norm": 11.37682819366455,
      "learning_rate": 1.6921153846153847e-05,
      "loss": 1.0375,
      "step": 1602
    },
    {
      "epoch": 1.541346153846154,
      "grad_norm": 12.874191284179688,
      "learning_rate": 1.691923076923077e-05,
      "loss": 1.5111,
      "step": 1603
    },
    {
      "epoch": 1.5423076923076922,
      "grad_norm": 7.487081527709961,
      "learning_rate": 1.6917307692307693e-05,
      "loss": 1.9871,
      "step": 1604
    },
    {
      "epoch": 1.5432692307692308,
      "grad_norm": 13.58925724029541,
      "learning_rate": 1.6915384615384615e-05,
      "loss": 1.4606,
      "step": 1605
    },
    {
      "epoch": 1.544230769230769,
      "grad_norm": 7.438632011413574,
      "learning_rate": 1.6913461538461538e-05,
      "loss": 1.4687,
      "step": 1606
    },
    {
      "epoch": 1.5451923076923078,
      "grad_norm": 13.718082427978516,
      "learning_rate": 1.6911538461538464e-05,
      "loss": 0.9359,
      "step": 1607
    },
    {
      "epoch": 1.546153846153846,
      "grad_norm": 7.251060962677002,
      "learning_rate": 1.6909615384615387e-05,
      "loss": 1.2922,
      "step": 1608
    },
    {
      "epoch": 1.5471153846153847,
      "grad_norm": 25.463834762573242,
      "learning_rate": 1.690769230769231e-05,
      "loss": 1.9098,
      "step": 1609
    },
    {
      "epoch": 1.5480769230769231,
      "grad_norm": 7.821784496307373,
      "learning_rate": 1.6905769230769233e-05,
      "loss": 0.5099,
      "step": 1610
    },
    {
      "epoch": 1.5490384615384616,
      "grad_norm": 6.783939838409424,
      "learning_rate": 1.6903846153846155e-05,
      "loss": 0.9473,
      "step": 1611
    },
    {
      "epoch": 1.55,
      "grad_norm": 7.9067559242248535,
      "learning_rate": 1.6901923076923078e-05,
      "loss": 1.4696,
      "step": 1612
    },
    {
      "epoch": 1.5509615384615385,
      "grad_norm": 7.2419538497924805,
      "learning_rate": 1.69e-05,
      "loss": 1.3641,
      "step": 1613
    },
    {
      "epoch": 1.551923076923077,
      "grad_norm": 6.700682640075684,
      "learning_rate": 1.6898076923076924e-05,
      "loss": 1.4359,
      "step": 1614
    },
    {
      "epoch": 1.5528846153846154,
      "grad_norm": 6.135627746582031,
      "learning_rate": 1.6896153846153846e-05,
      "loss": 1.1971,
      "step": 1615
    },
    {
      "epoch": 1.5538461538461539,
      "grad_norm": 7.243745803833008,
      "learning_rate": 1.689423076923077e-05,
      "loss": 1.6584,
      "step": 1616
    },
    {
      "epoch": 1.5548076923076923,
      "grad_norm": 9.218127250671387,
      "learning_rate": 1.6892307692307695e-05,
      "loss": 1.4537,
      "step": 1617
    },
    {
      "epoch": 1.5557692307692308,
      "grad_norm": 8.439014434814453,
      "learning_rate": 1.6890384615384615e-05,
      "loss": 1.9998,
      "step": 1618
    },
    {
      "epoch": 1.5567307692307693,
      "grad_norm": 6.728560924530029,
      "learning_rate": 1.688846153846154e-05,
      "loss": 1.7056,
      "step": 1619
    },
    {
      "epoch": 1.5576923076923077,
      "grad_norm": 9.704581260681152,
      "learning_rate": 1.6886538461538464e-05,
      "loss": 0.7157,
      "step": 1620
    },
    {
      "epoch": 1.5586538461538462,
      "grad_norm": 9.281500816345215,
      "learning_rate": 1.6884615384615386e-05,
      "loss": 0.3983,
      "step": 1621
    },
    {
      "epoch": 1.5596153846153846,
      "grad_norm": 6.387157917022705,
      "learning_rate": 1.688269230769231e-05,
      "loss": 0.1755,
      "step": 1622
    },
    {
      "epoch": 1.560576923076923,
      "grad_norm": 6.236350059509277,
      "learning_rate": 1.6880769230769232e-05,
      "loss": 1.4131,
      "step": 1623
    },
    {
      "epoch": 1.5615384615384615,
      "grad_norm": 10.974696159362793,
      "learning_rate": 1.6878846153846155e-05,
      "loss": 1.436,
      "step": 1624
    },
    {
      "epoch": 1.5625,
      "grad_norm": 7.984853744506836,
      "learning_rate": 1.6876923076923077e-05,
      "loss": 1.493,
      "step": 1625
    },
    {
      "epoch": 1.5634615384615385,
      "grad_norm": 9.753996849060059,
      "learning_rate": 1.6875e-05,
      "loss": 1.7406,
      "step": 1626
    },
    {
      "epoch": 1.564423076923077,
      "grad_norm": 10.522035598754883,
      "learning_rate": 1.6873076923076926e-05,
      "loss": 1.3247,
      "step": 1627
    },
    {
      "epoch": 1.5653846153846154,
      "grad_norm": 6.712428569793701,
      "learning_rate": 1.6871153846153846e-05,
      "loss": 1.2629,
      "step": 1628
    },
    {
      "epoch": 1.5663461538461538,
      "grad_norm": 6.616179943084717,
      "learning_rate": 1.6869230769230772e-05,
      "loss": 1.4549,
      "step": 1629
    },
    {
      "epoch": 1.5673076923076923,
      "grad_norm": 5.437473773956299,
      "learning_rate": 1.686730769230769e-05,
      "loss": 1.6469,
      "step": 1630
    },
    {
      "epoch": 1.5682692307692307,
      "grad_norm": 4.136465549468994,
      "learning_rate": 1.6865384615384617e-05,
      "loss": 0.082,
      "step": 1631
    },
    {
      "epoch": 1.5692307692307692,
      "grad_norm": 5.42014217376709,
      "learning_rate": 1.686346153846154e-05,
      "loss": 1.6365,
      "step": 1632
    },
    {
      "epoch": 1.5701923076923077,
      "grad_norm": 6.39804220199585,
      "learning_rate": 1.6861538461538463e-05,
      "loss": 0.9979,
      "step": 1633
    },
    {
      "epoch": 1.5711538461538461,
      "grad_norm": 5.22722053527832,
      "learning_rate": 1.6859615384615386e-05,
      "loss": 1.8217,
      "step": 1634
    },
    {
      "epoch": 1.5721153846153846,
      "grad_norm": 10.76814079284668,
      "learning_rate": 1.685769230769231e-05,
      "loss": 0.9096,
      "step": 1635
    },
    {
      "epoch": 1.573076923076923,
      "grad_norm": 5.7767815589904785,
      "learning_rate": 1.685576923076923e-05,
      "loss": 1.6316,
      "step": 1636
    },
    {
      "epoch": 1.5740384615384615,
      "grad_norm": 5.639969348907471,
      "learning_rate": 1.6853846153846157e-05,
      "loss": 1.2968,
      "step": 1637
    },
    {
      "epoch": 1.575,
      "grad_norm": 7.569429397583008,
      "learning_rate": 1.6851923076923077e-05,
      "loss": 0.6624,
      "step": 1638
    },
    {
      "epoch": 1.5759615384615384,
      "grad_norm": 6.539488315582275,
      "learning_rate": 1.6850000000000003e-05,
      "loss": 1.4646,
      "step": 1639
    },
    {
      "epoch": 1.5769230769230769,
      "grad_norm": 3.237189531326294,
      "learning_rate": 1.6848076923076922e-05,
      "loss": 0.0395,
      "step": 1640
    },
    {
      "epoch": 1.5778846153846153,
      "grad_norm": 9.102067947387695,
      "learning_rate": 1.684615384615385e-05,
      "loss": 1.7266,
      "step": 1641
    },
    {
      "epoch": 1.578846153846154,
      "grad_norm": 6.907880783081055,
      "learning_rate": 1.684423076923077e-05,
      "loss": 1.6595,
      "step": 1642
    },
    {
      "epoch": 1.5798076923076922,
      "grad_norm": 7.320252418518066,
      "learning_rate": 1.6842307692307694e-05,
      "loss": 1.6957,
      "step": 1643
    },
    {
      "epoch": 1.580769230769231,
      "grad_norm": 5.29960823059082,
      "learning_rate": 1.6840384615384617e-05,
      "loss": 0.2958,
      "step": 1644
    },
    {
      "epoch": 1.5817307692307692,
      "grad_norm": 6.170762062072754,
      "learning_rate": 1.683846153846154e-05,
      "loss": 1.1639,
      "step": 1645
    },
    {
      "epoch": 1.5826923076923078,
      "grad_norm": 6.477640628814697,
      "learning_rate": 1.6836538461538462e-05,
      "loss": 1.31,
      "step": 1646
    },
    {
      "epoch": 1.583653846153846,
      "grad_norm": 11.556058883666992,
      "learning_rate": 1.683461538461539e-05,
      "loss": 0.7531,
      "step": 1647
    },
    {
      "epoch": 1.5846153846153848,
      "grad_norm": 6.632603168487549,
      "learning_rate": 1.6832692307692308e-05,
      "loss": 1.199,
      "step": 1648
    },
    {
      "epoch": 1.585576923076923,
      "grad_norm": 18.02686309814453,
      "learning_rate": 1.6830769230769234e-05,
      "loss": 1.5226,
      "step": 1649
    },
    {
      "epoch": 1.5865384615384617,
      "grad_norm": 9.993369102478027,
      "learning_rate": 1.6828846153846153e-05,
      "loss": 0.5451,
      "step": 1650
    },
    {
      "epoch": 1.5875,
      "grad_norm": 5.5819292068481445,
      "learning_rate": 1.682692307692308e-05,
      "loss": 1.6802,
      "step": 1651
    },
    {
      "epoch": 1.5884615384615386,
      "grad_norm": 8.293460845947266,
      "learning_rate": 1.6825000000000002e-05,
      "loss": 1.325,
      "step": 1652
    },
    {
      "epoch": 1.5894230769230768,
      "grad_norm": 6.454996109008789,
      "learning_rate": 1.6823076923076925e-05,
      "loss": 1.1584,
      "step": 1653
    },
    {
      "epoch": 1.5903846153846155,
      "grad_norm": 11.18399715423584,
      "learning_rate": 1.6821153846153848e-05,
      "loss": 0.4238,
      "step": 1654
    },
    {
      "epoch": 1.5913461538461537,
      "grad_norm": 13.685914039611816,
      "learning_rate": 1.681923076923077e-05,
      "loss": 1.2474,
      "step": 1655
    },
    {
      "epoch": 1.5923076923076924,
      "grad_norm": 8.802918434143066,
      "learning_rate": 1.6817307692307693e-05,
      "loss": 1.9476,
      "step": 1656
    },
    {
      "epoch": 1.5932692307692307,
      "grad_norm": 6.551437854766846,
      "learning_rate": 1.6815384615384616e-05,
      "loss": 2.1744,
      "step": 1657
    },
    {
      "epoch": 1.5942307692307693,
      "grad_norm": 7.699594497680664,
      "learning_rate": 1.681346153846154e-05,
      "loss": 0.3621,
      "step": 1658
    },
    {
      "epoch": 1.5951923076923076,
      "grad_norm": 5.5222487449646,
      "learning_rate": 1.6811538461538465e-05,
      "loss": 1.8249,
      "step": 1659
    },
    {
      "epoch": 1.5961538461538463,
      "grad_norm": 7.555170059204102,
      "learning_rate": 1.6809615384615385e-05,
      "loss": 0.952,
      "step": 1660
    },
    {
      "epoch": 1.5971153846153845,
      "grad_norm": 5.6029205322265625,
      "learning_rate": 1.680769230769231e-05,
      "loss": 2.1216,
      "step": 1661
    },
    {
      "epoch": 1.5980769230769232,
      "grad_norm": 5.456368446350098,
      "learning_rate": 1.680576923076923e-05,
      "loss": 2.3782,
      "step": 1662
    },
    {
      "epoch": 1.5990384615384614,
      "grad_norm": 6.563088893890381,
      "learning_rate": 1.6803846153846156e-05,
      "loss": 0.9216,
      "step": 1663
    },
    {
      "epoch": 1.6,
      "grad_norm": 6.11185359954834,
      "learning_rate": 1.680192307692308e-05,
      "loss": 0.9968,
      "step": 1664
    },
    {
      "epoch": 1.6009615384615383,
      "grad_norm": 6.330321311950684,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 1.5502,
      "step": 1665
    },
    {
      "epoch": 1.601923076923077,
      "grad_norm": 6.3852152824401855,
      "learning_rate": 1.6798076923076925e-05,
      "loss": 1.3818,
      "step": 1666
    },
    {
      "epoch": 1.6028846153846152,
      "grad_norm": 8.55157470703125,
      "learning_rate": 1.6796153846153847e-05,
      "loss": 1.2036,
      "step": 1667
    },
    {
      "epoch": 1.603846153846154,
      "grad_norm": 6.4780049324035645,
      "learning_rate": 1.679423076923077e-05,
      "loss": 1.4139,
      "step": 1668
    },
    {
      "epoch": 1.6048076923076922,
      "grad_norm": 8.098188400268555,
      "learning_rate": 1.6792307692307693e-05,
      "loss": 1.015,
      "step": 1669
    },
    {
      "epoch": 1.6057692307692308,
      "grad_norm": 7.5981950759887695,
      "learning_rate": 1.6790384615384616e-05,
      "loss": 1.5977,
      "step": 1670
    },
    {
      "epoch": 1.606730769230769,
      "grad_norm": 6.4801859855651855,
      "learning_rate": 1.6788461538461542e-05,
      "loss": 0.565,
      "step": 1671
    },
    {
      "epoch": 1.6076923076923078,
      "grad_norm": 8.119485855102539,
      "learning_rate": 1.678653846153846e-05,
      "loss": 0.9091,
      "step": 1672
    },
    {
      "epoch": 1.608653846153846,
      "grad_norm": 5.776118278503418,
      "learning_rate": 1.6784615384615387e-05,
      "loss": 1.2189,
      "step": 1673
    },
    {
      "epoch": 1.6096153846153847,
      "grad_norm": 11.419273376464844,
      "learning_rate": 1.678269230769231e-05,
      "loss": 0.4504,
      "step": 1674
    },
    {
      "epoch": 1.6105769230769231,
      "grad_norm": 13.830252647399902,
      "learning_rate": 1.6780769230769233e-05,
      "loss": 0.3456,
      "step": 1675
    },
    {
      "epoch": 1.6115384615384616,
      "grad_norm": 6.804150104522705,
      "learning_rate": 1.6778846153846156e-05,
      "loss": 0.9199,
      "step": 1676
    },
    {
      "epoch": 1.6125,
      "grad_norm": 5.3179779052734375,
      "learning_rate": 1.677692307692308e-05,
      "loss": 0.0342,
      "step": 1677
    },
    {
      "epoch": 1.6134615384615385,
      "grad_norm": 5.929739475250244,
      "learning_rate": 1.6775e-05,
      "loss": 1.1142,
      "step": 1678
    },
    {
      "epoch": 1.614423076923077,
      "grad_norm": 9.210824012756348,
      "learning_rate": 1.6773076923076924e-05,
      "loss": 1.1648,
      "step": 1679
    },
    {
      "epoch": 1.6153846153846154,
      "grad_norm": 5.042150974273682,
      "learning_rate": 1.6771153846153847e-05,
      "loss": 1.7675,
      "step": 1680
    },
    {
      "epoch": 1.6163461538461539,
      "grad_norm": 15.417679786682129,
      "learning_rate": 1.676923076923077e-05,
      "loss": 1.4855,
      "step": 1681
    },
    {
      "epoch": 1.6173076923076923,
      "grad_norm": 8.527936935424805,
      "learning_rate": 1.6767307692307692e-05,
      "loss": 0.8278,
      "step": 1682
    },
    {
      "epoch": 1.6182692307692308,
      "grad_norm": 51.83433151245117,
      "learning_rate": 1.6765384615384615e-05,
      "loss": 0.6746,
      "step": 1683
    },
    {
      "epoch": 1.6192307692307693,
      "grad_norm": 5.998493671417236,
      "learning_rate": 1.676346153846154e-05,
      "loss": 1.2633,
      "step": 1684
    },
    {
      "epoch": 1.6201923076923077,
      "grad_norm": 7.200119972229004,
      "learning_rate": 1.6761538461538464e-05,
      "loss": 1.5101,
      "step": 1685
    },
    {
      "epoch": 1.6211538461538462,
      "grad_norm": 5.503507137298584,
      "learning_rate": 1.6759615384615387e-05,
      "loss": 1.6139,
      "step": 1686
    },
    {
      "epoch": 1.6221153846153846,
      "grad_norm": 4.700117588043213,
      "learning_rate": 1.675769230769231e-05,
      "loss": 0.6778,
      "step": 1687
    },
    {
      "epoch": 1.623076923076923,
      "grad_norm": 6.375677108764648,
      "learning_rate": 1.6755769230769232e-05,
      "loss": 1.2812,
      "step": 1688
    },
    {
      "epoch": 1.6240384615384615,
      "grad_norm": 5.439341068267822,
      "learning_rate": 1.6753846153846155e-05,
      "loss": 1.5585,
      "step": 1689
    },
    {
      "epoch": 1.625,
      "grad_norm": 4.606553554534912,
      "learning_rate": 1.6751923076923078e-05,
      "loss": 0.9596,
      "step": 1690
    },
    {
      "epoch": 1.6259615384615385,
      "grad_norm": 6.831825256347656,
      "learning_rate": 1.675e-05,
      "loss": 1.3946,
      "step": 1691
    },
    {
      "epoch": 1.626923076923077,
      "grad_norm": 7.669737339019775,
      "learning_rate": 1.6748076923076923e-05,
      "loss": 0.7948,
      "step": 1692
    },
    {
      "epoch": 1.6278846153846154,
      "grad_norm": 7.255706787109375,
      "learning_rate": 1.6746153846153846e-05,
      "loss": 1.5801,
      "step": 1693
    },
    {
      "epoch": 1.6288461538461538,
      "grad_norm": 5.939610004425049,
      "learning_rate": 1.6744230769230772e-05,
      "loss": 1.766,
      "step": 1694
    },
    {
      "epoch": 1.6298076923076923,
      "grad_norm": 8.571528434753418,
      "learning_rate": 1.674230769230769e-05,
      "loss": 0.5757,
      "step": 1695
    },
    {
      "epoch": 1.6307692307692307,
      "grad_norm": 8.416539192199707,
      "learning_rate": 1.6740384615384618e-05,
      "loss": 1.4973,
      "step": 1696
    },
    {
      "epoch": 1.6317307692307692,
      "grad_norm": 9.551568031311035,
      "learning_rate": 1.673846153846154e-05,
      "loss": 0.674,
      "step": 1697
    },
    {
      "epoch": 1.6326923076923077,
      "grad_norm": 7.3377203941345215,
      "learning_rate": 1.6736538461538463e-05,
      "loss": 0.6503,
      "step": 1698
    },
    {
      "epoch": 1.6336538461538461,
      "grad_norm": 7.989750385284424,
      "learning_rate": 1.6734615384615386e-05,
      "loss": 0.1966,
      "step": 1699
    },
    {
      "epoch": 1.6346153846153846,
      "grad_norm": 5.395298957824707,
      "learning_rate": 1.673269230769231e-05,
      "loss": 0.0882,
      "step": 1700
    },
    {
      "epoch": 1.635576923076923,
      "grad_norm": 13.553498268127441,
      "learning_rate": 1.673076923076923e-05,
      "loss": 1.0711,
      "step": 1701
    },
    {
      "epoch": 1.6365384615384615,
      "grad_norm": 1.3514174222946167,
      "learning_rate": 1.6728846153846154e-05,
      "loss": 0.0098,
      "step": 1702
    },
    {
      "epoch": 1.6375,
      "grad_norm": 4.76146125793457,
      "learning_rate": 1.6726923076923077e-05,
      "loss": 0.0427,
      "step": 1703
    },
    {
      "epoch": 1.6384615384615384,
      "grad_norm": 6.881860733032227,
      "learning_rate": 1.6725000000000003e-05,
      "loss": 0.8284,
      "step": 1704
    },
    {
      "epoch": 1.6394230769230769,
      "grad_norm": 10.94601058959961,
      "learning_rate": 1.6723076923076923e-05,
      "loss": 1.3605,
      "step": 1705
    },
    {
      "epoch": 1.6403846153846153,
      "grad_norm": 11.432401657104492,
      "learning_rate": 1.672115384615385e-05,
      "loss": 0.5084,
      "step": 1706
    },
    {
      "epoch": 1.641346153846154,
      "grad_norm": 7.801647663116455,
      "learning_rate": 1.6719230769230768e-05,
      "loss": 0.6196,
      "step": 1707
    },
    {
      "epoch": 1.6423076923076922,
      "grad_norm": 7.216637134552002,
      "learning_rate": 1.6717307692307694e-05,
      "loss": 1.5474,
      "step": 1708
    },
    {
      "epoch": 1.643269230769231,
      "grad_norm": 10.154765129089355,
      "learning_rate": 1.6715384615384617e-05,
      "loss": 0.1402,
      "step": 1709
    },
    {
      "epoch": 1.6442307692307692,
      "grad_norm": 6.684713840484619,
      "learning_rate": 1.671346153846154e-05,
      "loss": 0.2955,
      "step": 1710
    },
    {
      "epoch": 1.6451923076923078,
      "grad_norm": 5.177251815795898,
      "learning_rate": 1.6711538461538463e-05,
      "loss": 1.7329,
      "step": 1711
    },
    {
      "epoch": 1.646153846153846,
      "grad_norm": 9.861273765563965,
      "learning_rate": 1.6709615384615386e-05,
      "loss": 1.308,
      "step": 1712
    },
    {
      "epoch": 1.6471153846153848,
      "grad_norm": 8.488238334655762,
      "learning_rate": 1.6707692307692308e-05,
      "loss": 2.0809,
      "step": 1713
    },
    {
      "epoch": 1.648076923076923,
      "grad_norm": 4.766358852386475,
      "learning_rate": 1.6705769230769234e-05,
      "loss": 1.9857,
      "step": 1714
    },
    {
      "epoch": 1.6490384615384617,
      "grad_norm": 7.287229537963867,
      "learning_rate": 1.6703846153846154e-05,
      "loss": 1.184,
      "step": 1715
    },
    {
      "epoch": 1.65,
      "grad_norm": 9.808134078979492,
      "learning_rate": 1.670192307692308e-05,
      "loss": 1.5131,
      "step": 1716
    },
    {
      "epoch": 1.6509615384615386,
      "grad_norm": 7.350147247314453,
      "learning_rate": 1.67e-05,
      "loss": 1.6079,
      "step": 1717
    },
    {
      "epoch": 1.6519230769230768,
      "grad_norm": 6.6577839851379395,
      "learning_rate": 1.6698076923076926e-05,
      "loss": 1.6359,
      "step": 1718
    },
    {
      "epoch": 1.6528846153846155,
      "grad_norm": 7.897132396697998,
      "learning_rate": 1.6696153846153848e-05,
      "loss": 1.3423,
      "step": 1719
    },
    {
      "epoch": 1.6538461538461537,
      "grad_norm": 7.339761734008789,
      "learning_rate": 1.669423076923077e-05,
      "loss": 1.4931,
      "step": 1720
    },
    {
      "epoch": 1.6548076923076924,
      "grad_norm": 6.211727142333984,
      "learning_rate": 1.6692307692307694e-05,
      "loss": 1.2563,
      "step": 1721
    },
    {
      "epoch": 1.6557692307692307,
      "grad_norm": 7.1840949058532715,
      "learning_rate": 1.6690384615384617e-05,
      "loss": 1.4253,
      "step": 1722
    },
    {
      "epoch": 1.6567307692307693,
      "grad_norm": 15.421660423278809,
      "learning_rate": 1.668846153846154e-05,
      "loss": 0.8552,
      "step": 1723
    },
    {
      "epoch": 1.6576923076923076,
      "grad_norm": 6.544206619262695,
      "learning_rate": 1.6686538461538462e-05,
      "loss": 0.9575,
      "step": 1724
    },
    {
      "epoch": 1.6586538461538463,
      "grad_norm": 10.33317756652832,
      "learning_rate": 1.6684615384615385e-05,
      "loss": 0.7332,
      "step": 1725
    },
    {
      "epoch": 1.6596153846153845,
      "grad_norm": 7.950588226318359,
      "learning_rate": 1.668269230769231e-05,
      "loss": 0.8683,
      "step": 1726
    },
    {
      "epoch": 1.6605769230769232,
      "grad_norm": 7.748778820037842,
      "learning_rate": 1.668076923076923e-05,
      "loss": 1.8193,
      "step": 1727
    },
    {
      "epoch": 1.6615384615384614,
      "grad_norm": 8.157297134399414,
      "learning_rate": 1.6678846153846157e-05,
      "loss": 0.8348,
      "step": 1728
    },
    {
      "epoch": 1.6625,
      "grad_norm": 5.604048252105713,
      "learning_rate": 1.6676923076923076e-05,
      "loss": 0.7651,
      "step": 1729
    },
    {
      "epoch": 1.6634615384615383,
      "grad_norm": 8.911012649536133,
      "learning_rate": 1.6675000000000002e-05,
      "loss": 1.8795,
      "step": 1730
    },
    {
      "epoch": 1.664423076923077,
      "grad_norm": 7.236196041107178,
      "learning_rate": 1.6673076923076925e-05,
      "loss": 0.1226,
      "step": 1731
    },
    {
      "epoch": 1.6653846153846152,
      "grad_norm": 8.002164840698242,
      "learning_rate": 1.6671153846153848e-05,
      "loss": 1.184,
      "step": 1732
    },
    {
      "epoch": 1.666346153846154,
      "grad_norm": 7.049431800842285,
      "learning_rate": 1.666923076923077e-05,
      "loss": 1.1012,
      "step": 1733
    },
    {
      "epoch": 1.6673076923076922,
      "grad_norm": 7.6034698486328125,
      "learning_rate": 1.6667307692307693e-05,
      "loss": 0.9095,
      "step": 1734
    },
    {
      "epoch": 1.6682692307692308,
      "grad_norm": 6.673296928405762,
      "learning_rate": 1.6665384615384616e-05,
      "loss": 0.1021,
      "step": 1735
    },
    {
      "epoch": 1.669230769230769,
      "grad_norm": 7.871885776519775,
      "learning_rate": 1.6663461538461542e-05,
      "loss": 1.6913,
      "step": 1736
    },
    {
      "epoch": 1.6701923076923078,
      "grad_norm": 7.035243511199951,
      "learning_rate": 1.666153846153846e-05,
      "loss": 1.2227,
      "step": 1737
    },
    {
      "epoch": 1.671153846153846,
      "grad_norm": 11.273073196411133,
      "learning_rate": 1.6659615384615388e-05,
      "loss": 1.4387,
      "step": 1738
    },
    {
      "epoch": 1.6721153846153847,
      "grad_norm": 7.908133029937744,
      "learning_rate": 1.6657692307692307e-05,
      "loss": 1.2089,
      "step": 1739
    },
    {
      "epoch": 1.6730769230769231,
      "grad_norm": 7.530860900878906,
      "learning_rate": 1.6655769230769233e-05,
      "loss": 1.1591,
      "step": 1740
    },
    {
      "epoch": 1.6740384615384616,
      "grad_norm": 6.5013556480407715,
      "learning_rate": 1.6653846153846156e-05,
      "loss": 1.34,
      "step": 1741
    },
    {
      "epoch": 1.675,
      "grad_norm": 12.191231727600098,
      "learning_rate": 1.665192307692308e-05,
      "loss": 2.1119,
      "step": 1742
    },
    {
      "epoch": 1.6759615384615385,
      "grad_norm": 7.5273871421813965,
      "learning_rate": 1.665e-05,
      "loss": 0.7449,
      "step": 1743
    },
    {
      "epoch": 1.676923076923077,
      "grad_norm": 6.729090690612793,
      "learning_rate": 1.6648076923076924e-05,
      "loss": 1.1465,
      "step": 1744
    },
    {
      "epoch": 1.6778846153846154,
      "grad_norm": 7.424289226531982,
      "learning_rate": 1.6646153846153847e-05,
      "loss": 1.4753,
      "step": 1745
    },
    {
      "epoch": 1.6788461538461539,
      "grad_norm": 8.36112117767334,
      "learning_rate": 1.664423076923077e-05,
      "loss": 1.9448,
      "step": 1746
    },
    {
      "epoch": 1.6798076923076923,
      "grad_norm": 8.447999000549316,
      "learning_rate": 1.6642307692307693e-05,
      "loss": 0.9042,
      "step": 1747
    },
    {
      "epoch": 1.6807692307692308,
      "grad_norm": 6.716073989868164,
      "learning_rate": 1.664038461538462e-05,
      "loss": 0.9053,
      "step": 1748
    },
    {
      "epoch": 1.6817307692307693,
      "grad_norm": 5.218757629394531,
      "learning_rate": 1.6638461538461538e-05,
      "loss": 0.3236,
      "step": 1749
    },
    {
      "epoch": 1.6826923076923077,
      "grad_norm": 7.1929755210876465,
      "learning_rate": 1.6636538461538464e-05,
      "loss": 1.4284,
      "step": 1750
    },
    {
      "epoch": 1.6836538461538462,
      "grad_norm": 6.153013229370117,
      "learning_rate": 1.6634615384615387e-05,
      "loss": 1.5413,
      "step": 1751
    },
    {
      "epoch": 1.6846153846153846,
      "grad_norm": 6.435032844543457,
      "learning_rate": 1.663269230769231e-05,
      "loss": 0.9029,
      "step": 1752
    },
    {
      "epoch": 1.685576923076923,
      "grad_norm": 6.231819152832031,
      "learning_rate": 1.6630769230769233e-05,
      "loss": 1.1831,
      "step": 1753
    },
    {
      "epoch": 1.6865384615384615,
      "grad_norm": 6.7026143074035645,
      "learning_rate": 1.6628846153846155e-05,
      "loss": 1.6741,
      "step": 1754
    },
    {
      "epoch": 1.6875,
      "grad_norm": 7.146178722381592,
      "learning_rate": 1.6626923076923078e-05,
      "loss": 1.2736,
      "step": 1755
    },
    {
      "epoch": 1.6884615384615385,
      "grad_norm": 4.173646450042725,
      "learning_rate": 1.6625e-05,
      "loss": 0.1371,
      "step": 1756
    },
    {
      "epoch": 1.689423076923077,
      "grad_norm": 6.6086225509643555,
      "learning_rate": 1.6623076923076924e-05,
      "loss": 1.5357,
      "step": 1757
    },
    {
      "epoch": 1.6903846153846154,
      "grad_norm": 7.268357276916504,
      "learning_rate": 1.6621153846153846e-05,
      "loss": 1.7128,
      "step": 1758
    },
    {
      "epoch": 1.6913461538461538,
      "grad_norm": 10.172816276550293,
      "learning_rate": 1.661923076923077e-05,
      "loss": 0.8236,
      "step": 1759
    },
    {
      "epoch": 1.6923076923076923,
      "grad_norm": 6.657017230987549,
      "learning_rate": 1.6617307692307695e-05,
      "loss": 1.129,
      "step": 1760
    },
    {
      "epoch": 1.6932692307692307,
      "grad_norm": 13.616683959960938,
      "learning_rate": 1.6615384615384618e-05,
      "loss": 0.2328,
      "step": 1761
    },
    {
      "epoch": 1.6942307692307692,
      "grad_norm": 8.458242416381836,
      "learning_rate": 1.661346153846154e-05,
      "loss": 0.3097,
      "step": 1762
    },
    {
      "epoch": 1.6951923076923077,
      "grad_norm": 6.562539100646973,
      "learning_rate": 1.6611538461538464e-05,
      "loss": 1.0895,
      "step": 1763
    },
    {
      "epoch": 1.6961538461538461,
      "grad_norm": 5.96745491027832,
      "learning_rate": 1.6609615384615386e-05,
      "loss": 0.5807,
      "step": 1764
    },
    {
      "epoch": 1.6971153846153846,
      "grad_norm": 1.5360734462738037,
      "learning_rate": 1.660769230769231e-05,
      "loss": 0.0086,
      "step": 1765
    },
    {
      "epoch": 1.698076923076923,
      "grad_norm": 6.74731969833374,
      "learning_rate": 1.6605769230769232e-05,
      "loss": 1.7015,
      "step": 1766
    },
    {
      "epoch": 1.6990384615384615,
      "grad_norm": 6.231553554534912,
      "learning_rate": 1.6603846153846155e-05,
      "loss": 1.866,
      "step": 1767
    },
    {
      "epoch": 1.7,
      "grad_norm": 5.294527053833008,
      "learning_rate": 1.6601923076923078e-05,
      "loss": 1.8769,
      "step": 1768
    },
    {
      "epoch": 1.7009615384615384,
      "grad_norm": 7.614539623260498,
      "learning_rate": 1.66e-05,
      "loss": 1.3016,
      "step": 1769
    },
    {
      "epoch": 1.7019230769230769,
      "grad_norm": 12.697010040283203,
      "learning_rate": 1.6598076923076923e-05,
      "loss": 0.5286,
      "step": 1770
    },
    {
      "epoch": 1.7028846153846153,
      "grad_norm": 6.569371223449707,
      "learning_rate": 1.659615384615385e-05,
      "loss": 1.4861,
      "step": 1771
    },
    {
      "epoch": 1.703846153846154,
      "grad_norm": 8.592257499694824,
      "learning_rate": 1.659423076923077e-05,
      "loss": 2.2471,
      "step": 1772
    },
    {
      "epoch": 1.7048076923076922,
      "grad_norm": 7.160495281219482,
      "learning_rate": 1.6592307692307695e-05,
      "loss": 0.9295,
      "step": 1773
    },
    {
      "epoch": 1.705769230769231,
      "grad_norm": 6.103002071380615,
      "learning_rate": 1.6590384615384618e-05,
      "loss": 1.0185,
      "step": 1774
    },
    {
      "epoch": 1.7067307692307692,
      "grad_norm": 6.753305912017822,
      "learning_rate": 1.658846153846154e-05,
      "loss": 1.2663,
      "step": 1775
    },
    {
      "epoch": 1.7076923076923078,
      "grad_norm": 6.546850681304932,
      "learning_rate": 1.6586538461538463e-05,
      "loss": 1.8409,
      "step": 1776
    },
    {
      "epoch": 1.708653846153846,
      "grad_norm": 10.775209426879883,
      "learning_rate": 1.6584615384615386e-05,
      "loss": 1.6533,
      "step": 1777
    },
    {
      "epoch": 1.7096153846153848,
      "grad_norm": 8.114875793457031,
      "learning_rate": 1.658269230769231e-05,
      "loss": 2.5826,
      "step": 1778
    },
    {
      "epoch": 1.710576923076923,
      "grad_norm": 7.611095905303955,
      "learning_rate": 1.658076923076923e-05,
      "loss": 0.7522,
      "step": 1779
    },
    {
      "epoch": 1.7115384615384617,
      "grad_norm": 10.31644344329834,
      "learning_rate": 1.6578846153846154e-05,
      "loss": 0.1072,
      "step": 1780
    },
    {
      "epoch": 1.7125,
      "grad_norm": 6.46040678024292,
      "learning_rate": 1.657692307692308e-05,
      "loss": 1.0795,
      "step": 1781
    },
    {
      "epoch": 1.7134615384615386,
      "grad_norm": 6.299928665161133,
      "learning_rate": 1.6575e-05,
      "loss": 0.379,
      "step": 1782
    },
    {
      "epoch": 1.7144230769230768,
      "grad_norm": 11.742854118347168,
      "learning_rate": 1.6573076923076926e-05,
      "loss": 0.8405,
      "step": 1783
    },
    {
      "epoch": 1.7153846153846155,
      "grad_norm": 8.313055992126465,
      "learning_rate": 1.6571153846153845e-05,
      "loss": 1.4655,
      "step": 1784
    },
    {
      "epoch": 1.7163461538461537,
      "grad_norm": 11.37623405456543,
      "learning_rate": 1.656923076923077e-05,
      "loss": 0.207,
      "step": 1785
    },
    {
      "epoch": 1.7173076923076924,
      "grad_norm": 13.06391716003418,
      "learning_rate": 1.6567307692307694e-05,
      "loss": 1.7831,
      "step": 1786
    },
    {
      "epoch": 1.7182692307692307,
      "grad_norm": 4.26381254196167,
      "learning_rate": 1.6565384615384617e-05,
      "loss": 0.2531,
      "step": 1787
    },
    {
      "epoch": 1.7192307692307693,
      "grad_norm": 7.934091091156006,
      "learning_rate": 1.656346153846154e-05,
      "loss": 1.3938,
      "step": 1788
    },
    {
      "epoch": 1.7201923076923076,
      "grad_norm": 7.739599704742432,
      "learning_rate": 1.6561538461538462e-05,
      "loss": 1.3735,
      "step": 1789
    },
    {
      "epoch": 1.7211538461538463,
      "grad_norm": 5.800947189331055,
      "learning_rate": 1.6559615384615385e-05,
      "loss": 1.2316,
      "step": 1790
    },
    {
      "epoch": 1.7221153846153845,
      "grad_norm": 9.574185371398926,
      "learning_rate": 1.6557692307692308e-05,
      "loss": 1.1118,
      "step": 1791
    },
    {
      "epoch": 1.7230769230769232,
      "grad_norm": 17.534231185913086,
      "learning_rate": 1.655576923076923e-05,
      "loss": 0.3096,
      "step": 1792
    },
    {
      "epoch": 1.7240384615384614,
      "grad_norm": 6.985840320587158,
      "learning_rate": 1.6553846153846157e-05,
      "loss": 1.7366,
      "step": 1793
    },
    {
      "epoch": 1.725,
      "grad_norm": 9.30622386932373,
      "learning_rate": 1.6551923076923076e-05,
      "loss": 1.2918,
      "step": 1794
    },
    {
      "epoch": 1.7259615384615383,
      "grad_norm": 5.6047821044921875,
      "learning_rate": 1.6550000000000002e-05,
      "loss": 1.2658,
      "step": 1795
    },
    {
      "epoch": 1.726923076923077,
      "grad_norm": 11.069454193115234,
      "learning_rate": 1.6548076923076922e-05,
      "loss": 1.4128,
      "step": 1796
    },
    {
      "epoch": 1.7278846153846152,
      "grad_norm": 8.556221961975098,
      "learning_rate": 1.6546153846153848e-05,
      "loss": 0.7857,
      "step": 1797
    },
    {
      "epoch": 1.728846153846154,
      "grad_norm": 15.297060012817383,
      "learning_rate": 1.654423076923077e-05,
      "loss": 0.8898,
      "step": 1798
    },
    {
      "epoch": 1.7298076923076922,
      "grad_norm": 10.932384490966797,
      "learning_rate": 1.6542307692307694e-05,
      "loss": 1.3342,
      "step": 1799
    },
    {
      "epoch": 1.7307692307692308,
      "grad_norm": 9.087265968322754,
      "learning_rate": 1.6540384615384616e-05,
      "loss": 1.3715,
      "step": 1800
    },
    {
      "epoch": 1.731730769230769,
      "grad_norm": 4.501835346221924,
      "learning_rate": 1.653846153846154e-05,
      "loss": 1.8001,
      "step": 1801
    },
    {
      "epoch": 1.7326923076923078,
      "grad_norm": 8.443159103393555,
      "learning_rate": 1.6536538461538462e-05,
      "loss": 1.734,
      "step": 1802
    },
    {
      "epoch": 1.733653846153846,
      "grad_norm": 6.710718154907227,
      "learning_rate": 1.6534615384615388e-05,
      "loss": 1.3436,
      "step": 1803
    },
    {
      "epoch": 1.7346153846153847,
      "grad_norm": 8.992195129394531,
      "learning_rate": 1.6532692307692307e-05,
      "loss": 0.5937,
      "step": 1804
    },
    {
      "epoch": 1.7355769230769231,
      "grad_norm": 4.977031707763672,
      "learning_rate": 1.6530769230769234e-05,
      "loss": 0.4956,
      "step": 1805
    },
    {
      "epoch": 1.7365384615384616,
      "grad_norm": 8.170890808105469,
      "learning_rate": 1.6528846153846153e-05,
      "loss": 0.1044,
      "step": 1806
    },
    {
      "epoch": 1.7375,
      "grad_norm": 9.011863708496094,
      "learning_rate": 1.652692307692308e-05,
      "loss": 1.4843,
      "step": 1807
    },
    {
      "epoch": 1.7384615384615385,
      "grad_norm": 10.600175857543945,
      "learning_rate": 1.6525000000000002e-05,
      "loss": 1.5241,
      "step": 1808
    },
    {
      "epoch": 1.739423076923077,
      "grad_norm": 5.803191661834717,
      "learning_rate": 1.6523076923076925e-05,
      "loss": 1.5841,
      "step": 1809
    },
    {
      "epoch": 1.7403846153846154,
      "grad_norm": 8.964375495910645,
      "learning_rate": 1.6521153846153847e-05,
      "loss": 1.7683,
      "step": 1810
    },
    {
      "epoch": 1.7413461538461539,
      "grad_norm": 13.737669944763184,
      "learning_rate": 1.651923076923077e-05,
      "loss": 1.5421,
      "step": 1811
    },
    {
      "epoch": 1.7423076923076923,
      "grad_norm": 5.567458152770996,
      "learning_rate": 1.6517307692307693e-05,
      "loss": 1.2908,
      "step": 1812
    },
    {
      "epoch": 1.7432692307692308,
      "grad_norm": 41.37973403930664,
      "learning_rate": 1.651538461538462e-05,
      "loss": 1.9484,
      "step": 1813
    },
    {
      "epoch": 1.7442307692307693,
      "grad_norm": 7.982646465301514,
      "learning_rate": 1.651346153846154e-05,
      "loss": 1.3357,
      "step": 1814
    },
    {
      "epoch": 1.7451923076923077,
      "grad_norm": 6.57487154006958,
      "learning_rate": 1.6511538461538465e-05,
      "loss": 1.2276,
      "step": 1815
    },
    {
      "epoch": 1.7461538461538462,
      "grad_norm": 12.03062629699707,
      "learning_rate": 1.6509615384615384e-05,
      "loss": 2.1941,
      "step": 1816
    },
    {
      "epoch": 1.7471153846153846,
      "grad_norm": 8.44942569732666,
      "learning_rate": 1.650769230769231e-05,
      "loss": 2.1039,
      "step": 1817
    },
    {
      "epoch": 1.748076923076923,
      "grad_norm": 9.246432304382324,
      "learning_rate": 1.6505769230769233e-05,
      "loss": 1.3954,
      "step": 1818
    },
    {
      "epoch": 1.7490384615384615,
      "grad_norm": 8.959112167358398,
      "learning_rate": 1.6503846153846156e-05,
      "loss": 1.4564,
      "step": 1819
    },
    {
      "epoch": 1.75,
      "grad_norm": 14.28302001953125,
      "learning_rate": 1.650192307692308e-05,
      "loss": 1.8177,
      "step": 1820
    },
    {
      "epoch": 1.7509615384615385,
      "grad_norm": 8.759315490722656,
      "learning_rate": 1.65e-05,
      "loss": 1.6088,
      "step": 1821
    },
    {
      "epoch": 1.751923076923077,
      "grad_norm": 16.461563110351562,
      "learning_rate": 1.6498076923076924e-05,
      "loss": 0.6476,
      "step": 1822
    },
    {
      "epoch": 1.7528846153846154,
      "grad_norm": 8.750345230102539,
      "learning_rate": 1.6496153846153847e-05,
      "loss": 0.2711,
      "step": 1823
    },
    {
      "epoch": 1.7538461538461538,
      "grad_norm": 6.609030246734619,
      "learning_rate": 1.649423076923077e-05,
      "loss": 1.2175,
      "step": 1824
    },
    {
      "epoch": 1.7548076923076923,
      "grad_norm": 6.873172283172607,
      "learning_rate": 1.6492307692307696e-05,
      "loss": 1.4161,
      "step": 1825
    },
    {
      "epoch": 1.7557692307692307,
      "grad_norm": 7.468815326690674,
      "learning_rate": 1.6490384615384615e-05,
      "loss": 1.2582,
      "step": 1826
    },
    {
      "epoch": 1.7567307692307692,
      "grad_norm": 7.4654765129089355,
      "learning_rate": 1.648846153846154e-05,
      "loss": 1.4182,
      "step": 1827
    },
    {
      "epoch": 1.7576923076923077,
      "grad_norm": 7.642784595489502,
      "learning_rate": 1.6486538461538464e-05,
      "loss": 1.4589,
      "step": 1828
    },
    {
      "epoch": 1.7586538461538461,
      "grad_norm": 6.1781792640686035,
      "learning_rate": 1.6484615384615387e-05,
      "loss": 1.7125,
      "step": 1829
    },
    {
      "epoch": 1.7596153846153846,
      "grad_norm": 8.969409942626953,
      "learning_rate": 1.648269230769231e-05,
      "loss": 1.1871,
      "step": 1830
    },
    {
      "epoch": 1.760576923076923,
      "grad_norm": 7.783875942230225,
      "learning_rate": 1.6480769230769232e-05,
      "loss": 1.232,
      "step": 1831
    },
    {
      "epoch": 1.7615384615384615,
      "grad_norm": 6.447662830352783,
      "learning_rate": 1.6478846153846155e-05,
      "loss": 0.9754,
      "step": 1832
    },
    {
      "epoch": 1.7625,
      "grad_norm": 5.859929084777832,
      "learning_rate": 1.6476923076923078e-05,
      "loss": 1.4156,
      "step": 1833
    },
    {
      "epoch": 1.7634615384615384,
      "grad_norm": 6.425597667694092,
      "learning_rate": 1.6475e-05,
      "loss": 1.5425,
      "step": 1834
    },
    {
      "epoch": 1.7644230769230769,
      "grad_norm": 7.553586483001709,
      "learning_rate": 1.6473076923076923e-05,
      "loss": 0.4029,
      "step": 1835
    },
    {
      "epoch": 1.7653846153846153,
      "grad_norm": 5.660864353179932,
      "learning_rate": 1.6471153846153846e-05,
      "loss": 1.4901,
      "step": 1836
    },
    {
      "epoch": 1.766346153846154,
      "grad_norm": 11.828795433044434,
      "learning_rate": 1.6469230769230772e-05,
      "loss": 2.0856,
      "step": 1837
    },
    {
      "epoch": 1.7673076923076922,
      "grad_norm": 4.5286054611206055,
      "learning_rate": 1.6467307692307695e-05,
      "loss": 0.053,
      "step": 1838
    },
    {
      "epoch": 1.768269230769231,
      "grad_norm": 7.664407253265381,
      "learning_rate": 1.6465384615384618e-05,
      "loss": 0.8027,
      "step": 1839
    },
    {
      "epoch": 1.7692307692307692,
      "grad_norm": 12.864773750305176,
      "learning_rate": 1.646346153846154e-05,
      "loss": 1.4874,
      "step": 1840
    },
    {
      "epoch": 1.7701923076923078,
      "grad_norm": 9.152652740478516,
      "learning_rate": 1.6461538461538463e-05,
      "loss": 1.7016,
      "step": 1841
    },
    {
      "epoch": 1.771153846153846,
      "grad_norm": 9.72258472442627,
      "learning_rate": 1.6459615384615386e-05,
      "loss": 1.7833,
      "step": 1842
    },
    {
      "epoch": 1.7721153846153848,
      "grad_norm": 8.346756935119629,
      "learning_rate": 1.645769230769231e-05,
      "loss": 1.054,
      "step": 1843
    },
    {
      "epoch": 1.773076923076923,
      "grad_norm": 11.921456336975098,
      "learning_rate": 1.6455769230769232e-05,
      "loss": 0.779,
      "step": 1844
    },
    {
      "epoch": 1.7740384615384617,
      "grad_norm": 7.633432865142822,
      "learning_rate": 1.6453846153846155e-05,
      "loss": 1.6164,
      "step": 1845
    },
    {
      "epoch": 1.775,
      "grad_norm": 8.991334915161133,
      "learning_rate": 1.6451923076923077e-05,
      "loss": 0.9781,
      "step": 1846
    },
    {
      "epoch": 1.7759615384615386,
      "grad_norm": 5.272639751434326,
      "learning_rate": 1.645e-05,
      "loss": 1.4373,
      "step": 1847
    },
    {
      "epoch": 1.7769230769230768,
      "grad_norm": 7.7145256996154785,
      "learning_rate": 1.6448076923076926e-05,
      "loss": 0.9513,
      "step": 1848
    },
    {
      "epoch": 1.7778846153846155,
      "grad_norm": 6.398937702178955,
      "learning_rate": 1.6446153846153846e-05,
      "loss": 1.2834,
      "step": 1849
    },
    {
      "epoch": 1.7788461538461537,
      "grad_norm": 6.780285835266113,
      "learning_rate": 1.6444230769230772e-05,
      "loss": 1.1183,
      "step": 1850
    },
    {
      "epoch": 1.7798076923076924,
      "grad_norm": 4.449232578277588,
      "learning_rate": 1.6442307692307695e-05,
      "loss": 1.2304,
      "step": 1851
    },
    {
      "epoch": 1.7807692307692307,
      "grad_norm": 8.247312545776367,
      "learning_rate": 1.6440384615384617e-05,
      "loss": 1.7967,
      "step": 1852
    },
    {
      "epoch": 1.7817307692307693,
      "grad_norm": 9.648176193237305,
      "learning_rate": 1.643846153846154e-05,
      "loss": 1.9738,
      "step": 1853
    },
    {
      "epoch": 1.7826923076923076,
      "grad_norm": 4.506691932678223,
      "learning_rate": 1.6436538461538463e-05,
      "loss": 0.7527,
      "step": 1854
    },
    {
      "epoch": 1.7836538461538463,
      "grad_norm": 7.558337211608887,
      "learning_rate": 1.6434615384615386e-05,
      "loss": 1.0665,
      "step": 1855
    },
    {
      "epoch": 1.7846153846153845,
      "grad_norm": 73.47591400146484,
      "learning_rate": 1.643269230769231e-05,
      "loss": 1.3745,
      "step": 1856
    },
    {
      "epoch": 1.7855769230769232,
      "grad_norm": 7.812288284301758,
      "learning_rate": 1.643076923076923e-05,
      "loss": 1.2558,
      "step": 1857
    },
    {
      "epoch": 1.7865384615384614,
      "grad_norm": 7.638749122619629,
      "learning_rate": 1.6428846153846154e-05,
      "loss": 1.1841,
      "step": 1858
    },
    {
      "epoch": 1.7875,
      "grad_norm": 5.4378838539123535,
      "learning_rate": 1.6426923076923077e-05,
      "loss": 0.4358,
      "step": 1859
    },
    {
      "epoch": 1.7884615384615383,
      "grad_norm": 5.648280143737793,
      "learning_rate": 1.6425000000000003e-05,
      "loss": 1.8023,
      "step": 1860
    },
    {
      "epoch": 1.789423076923077,
      "grad_norm": 8.095977783203125,
      "learning_rate": 1.6423076923076922e-05,
      "loss": 0.7645,
      "step": 1861
    },
    {
      "epoch": 1.7903846153846152,
      "grad_norm": 9.671142578125,
      "learning_rate": 1.642115384615385e-05,
      "loss": 1.4077,
      "step": 1862
    },
    {
      "epoch": 1.791346153846154,
      "grad_norm": 8.320858001708984,
      "learning_rate": 1.641923076923077e-05,
      "loss": 2.3265,
      "step": 1863
    },
    {
      "epoch": 1.7923076923076922,
      "grad_norm": 6.686349868774414,
      "learning_rate": 1.6417307692307694e-05,
      "loss": 1.8621,
      "step": 1864
    },
    {
      "epoch": 1.7932692307692308,
      "grad_norm": 6.234199523925781,
      "learning_rate": 1.6415384615384617e-05,
      "loss": 1.6428,
      "step": 1865
    },
    {
      "epoch": 1.794230769230769,
      "grad_norm": 8.900816917419434,
      "learning_rate": 1.641346153846154e-05,
      "loss": 0.8313,
      "step": 1866
    },
    {
      "epoch": 1.7951923076923078,
      "grad_norm": 7.105774879455566,
      "learning_rate": 1.6411538461538462e-05,
      "loss": 1.6996,
      "step": 1867
    },
    {
      "epoch": 1.796153846153846,
      "grad_norm": 6.774243354797363,
      "learning_rate": 1.6409615384615385e-05,
      "loss": 1.4217,
      "step": 1868
    },
    {
      "epoch": 1.7971153846153847,
      "grad_norm": 5.23370361328125,
      "learning_rate": 1.6407692307692308e-05,
      "loss": 1.355,
      "step": 1869
    },
    {
      "epoch": 1.7980769230769231,
      "grad_norm": 5.800739765167236,
      "learning_rate": 1.6405769230769234e-05,
      "loss": 1.3557,
      "step": 1870
    },
    {
      "epoch": 1.7990384615384616,
      "grad_norm": 5.413275241851807,
      "learning_rate": 1.6403846153846153e-05,
      "loss": 1.0967,
      "step": 1871
    },
    {
      "epoch": 1.8,
      "grad_norm": 9.850642204284668,
      "learning_rate": 1.640192307692308e-05,
      "loss": 1.1848,
      "step": 1872
    },
    {
      "epoch": 1.8009615384615385,
      "grad_norm": 11.163789749145508,
      "learning_rate": 1.64e-05,
      "loss": 1.426,
      "step": 1873
    },
    {
      "epoch": 1.801923076923077,
      "grad_norm": 6.042265892028809,
      "learning_rate": 1.6398076923076925e-05,
      "loss": 1.1266,
      "step": 1874
    },
    {
      "epoch": 1.8028846153846154,
      "grad_norm": 32.94291305541992,
      "learning_rate": 1.6396153846153848e-05,
      "loss": 0.9042,
      "step": 1875
    },
    {
      "epoch": 1.8038461538461539,
      "grad_norm": 3.3810794353485107,
      "learning_rate": 1.639423076923077e-05,
      "loss": 0.0272,
      "step": 1876
    },
    {
      "epoch": 1.8048076923076923,
      "grad_norm": 17.702770233154297,
      "learning_rate": 1.6392307692307693e-05,
      "loss": 1.4842,
      "step": 1877
    },
    {
      "epoch": 1.8057692307692308,
      "grad_norm": 6.21654748916626,
      "learning_rate": 1.6390384615384616e-05,
      "loss": 2.2061,
      "step": 1878
    },
    {
      "epoch": 1.8067307692307693,
      "grad_norm": 5.645794868469238,
      "learning_rate": 1.638846153846154e-05,
      "loss": 1.7163,
      "step": 1879
    },
    {
      "epoch": 1.8076923076923077,
      "grad_norm": 8.747044563293457,
      "learning_rate": 1.6386538461538465e-05,
      "loss": 1.4534,
      "step": 1880
    },
    {
      "epoch": 1.8086538461538462,
      "grad_norm": 25.706449508666992,
      "learning_rate": 1.6384615384615384e-05,
      "loss": 0.9756,
      "step": 1881
    },
    {
      "epoch": 1.8096153846153846,
      "grad_norm": 5.929856777191162,
      "learning_rate": 1.638269230769231e-05,
      "loss": 1.4596,
      "step": 1882
    },
    {
      "epoch": 1.810576923076923,
      "grad_norm": 7.312323570251465,
      "learning_rate": 1.638076923076923e-05,
      "loss": 1.9393,
      "step": 1883
    },
    {
      "epoch": 1.8115384615384615,
      "grad_norm": 10.690394401550293,
      "learning_rate": 1.6378846153846156e-05,
      "loss": 0.7513,
      "step": 1884
    },
    {
      "epoch": 1.8125,
      "grad_norm": 5.19911003112793,
      "learning_rate": 1.637692307692308e-05,
      "loss": 2.0277,
      "step": 1885
    },
    {
      "epoch": 1.8134615384615385,
      "grad_norm": 7.198575496673584,
      "learning_rate": 1.6375e-05,
      "loss": 1.6623,
      "step": 1886
    },
    {
      "epoch": 1.814423076923077,
      "grad_norm": 5.73149299621582,
      "learning_rate": 1.6373076923076924e-05,
      "loss": 1.3501,
      "step": 1887
    },
    {
      "epoch": 1.8153846153846154,
      "grad_norm": 6.695920467376709,
      "learning_rate": 1.6371153846153847e-05,
      "loss": 1.3255,
      "step": 1888
    },
    {
      "epoch": 1.8163461538461538,
      "grad_norm": 5.622996807098389,
      "learning_rate": 1.636923076923077e-05,
      "loss": 1.2424,
      "step": 1889
    },
    {
      "epoch": 1.8173076923076923,
      "grad_norm": 5.604401111602783,
      "learning_rate": 1.6367307692307696e-05,
      "loss": 1.7064,
      "step": 1890
    },
    {
      "epoch": 1.8182692307692307,
      "grad_norm": 5.438435077667236,
      "learning_rate": 1.6365384615384615e-05,
      "loss": 0.1345,
      "step": 1891
    },
    {
      "epoch": 1.8192307692307692,
      "grad_norm": 9.809414863586426,
      "learning_rate": 1.636346153846154e-05,
      "loss": 0.9308,
      "step": 1892
    },
    {
      "epoch": 1.8201923076923077,
      "grad_norm": 7.320613384246826,
      "learning_rate": 1.636153846153846e-05,
      "loss": 1.7263,
      "step": 1893
    },
    {
      "epoch": 1.8211538461538461,
      "grad_norm": 9.528648376464844,
      "learning_rate": 1.6359615384615387e-05,
      "loss": 1.6791,
      "step": 1894
    },
    {
      "epoch": 1.8221153846153846,
      "grad_norm": 9.322272300720215,
      "learning_rate": 1.635769230769231e-05,
      "loss": 1.7207,
      "step": 1895
    },
    {
      "epoch": 1.823076923076923,
      "grad_norm": 10.656936645507812,
      "learning_rate": 1.6355769230769233e-05,
      "loss": 0.695,
      "step": 1896
    },
    {
      "epoch": 1.8240384615384615,
      "grad_norm": 5.920652389526367,
      "learning_rate": 1.6353846153846155e-05,
      "loss": 0.6474,
      "step": 1897
    },
    {
      "epoch": 1.825,
      "grad_norm": 8.669979095458984,
      "learning_rate": 1.6351923076923078e-05,
      "loss": 0.9506,
      "step": 1898
    },
    {
      "epoch": 1.8259615384615384,
      "grad_norm": 7.458767414093018,
      "learning_rate": 1.635e-05,
      "loss": 1.7145,
      "step": 1899
    },
    {
      "epoch": 1.8269230769230769,
      "grad_norm": 5.3655781745910645,
      "learning_rate": 1.6348076923076924e-05,
      "loss": 1.9152,
      "step": 1900
    },
    {
      "epoch": 1.8278846153846153,
      "grad_norm": 5.63809061050415,
      "learning_rate": 1.6346153846153847e-05,
      "loss": 1.1239,
      "step": 1901
    },
    {
      "epoch": 1.828846153846154,
      "grad_norm": 6.934470176696777,
      "learning_rate": 1.6344230769230773e-05,
      "loss": 1.3386,
      "step": 1902
    },
    {
      "epoch": 1.8298076923076922,
      "grad_norm": 8.826090812683105,
      "learning_rate": 1.6342307692307692e-05,
      "loss": 1.1984,
      "step": 1903
    },
    {
      "epoch": 1.830769230769231,
      "grad_norm": 7.871182918548584,
      "learning_rate": 1.6340384615384618e-05,
      "loss": 0.1524,
      "step": 1904
    },
    {
      "epoch": 1.8317307692307692,
      "grad_norm": 6.45181941986084,
      "learning_rate": 1.633846153846154e-05,
      "loss": 1.1584,
      "step": 1905
    },
    {
      "epoch": 1.8326923076923078,
      "grad_norm": 9.184858322143555,
      "learning_rate": 1.6336538461538464e-05,
      "loss": 1.1433,
      "step": 1906
    },
    {
      "epoch": 1.833653846153846,
      "grad_norm": 5.753997802734375,
      "learning_rate": 1.6334615384615387e-05,
      "loss": 1.1273,
      "step": 1907
    },
    {
      "epoch": 1.8346153846153848,
      "grad_norm": 8.198064804077148,
      "learning_rate": 1.633269230769231e-05,
      "loss": 1.4606,
      "step": 1908
    },
    {
      "epoch": 1.835576923076923,
      "grad_norm": 8.012986183166504,
      "learning_rate": 1.6330769230769232e-05,
      "loss": 0.7035,
      "step": 1909
    },
    {
      "epoch": 1.8365384615384617,
      "grad_norm": 5.364163875579834,
      "learning_rate": 1.6328846153846155e-05,
      "loss": 2.0483,
      "step": 1910
    },
    {
      "epoch": 1.8375,
      "grad_norm": 7.348898410797119,
      "learning_rate": 1.6326923076923078e-05,
      "loss": 0.7869,
      "step": 1911
    },
    {
      "epoch": 1.8384615384615386,
      "grad_norm": 6.3715057373046875,
      "learning_rate": 1.6325e-05,
      "loss": 0.9666,
      "step": 1912
    },
    {
      "epoch": 1.8394230769230768,
      "grad_norm": 5.271612644195557,
      "learning_rate": 1.6323076923076923e-05,
      "loss": 1.3031,
      "step": 1913
    },
    {
      "epoch": 1.8403846153846155,
      "grad_norm": 5.339787483215332,
      "learning_rate": 1.632115384615385e-05,
      "loss": 1.3494,
      "step": 1914
    },
    {
      "epoch": 1.8413461538461537,
      "grad_norm": 6.248025894165039,
      "learning_rate": 1.6319230769230772e-05,
      "loss": 0.9705,
      "step": 1915
    },
    {
      "epoch": 1.8423076923076924,
      "grad_norm": 8.804878234863281,
      "learning_rate": 1.6317307692307695e-05,
      "loss": 1.0371,
      "step": 1916
    },
    {
      "epoch": 1.8432692307692307,
      "grad_norm": 14.17123031616211,
      "learning_rate": 1.6315384615384618e-05,
      "loss": 0.833,
      "step": 1917
    },
    {
      "epoch": 1.8442307692307693,
      "grad_norm": 7.055277347564697,
      "learning_rate": 1.631346153846154e-05,
      "loss": 1.5215,
      "step": 1918
    },
    {
      "epoch": 1.8451923076923076,
      "grad_norm": 6.034940242767334,
      "learning_rate": 1.6311538461538463e-05,
      "loss": 1.7919,
      "step": 1919
    },
    {
      "epoch": 1.8461538461538463,
      "grad_norm": 5.042874813079834,
      "learning_rate": 1.6309615384615386e-05,
      "loss": 1.3336,
      "step": 1920
    },
    {
      "epoch": 1.8471153846153845,
      "grad_norm": 7.413131237030029,
      "learning_rate": 1.630769230769231e-05,
      "loss": 1.2712,
      "step": 1921
    },
    {
      "epoch": 1.8480769230769232,
      "grad_norm": 6.9178853034973145,
      "learning_rate": 1.630576923076923e-05,
      "loss": 1.4686,
      "step": 1922
    },
    {
      "epoch": 1.8490384615384614,
      "grad_norm": 6.070854663848877,
      "learning_rate": 1.6303846153846154e-05,
      "loss": 1.3543,
      "step": 1923
    },
    {
      "epoch": 1.85,
      "grad_norm": 4.987901210784912,
      "learning_rate": 1.6301923076923077e-05,
      "loss": 1.3321,
      "step": 1924
    },
    {
      "epoch": 1.8509615384615383,
      "grad_norm": 6.720137596130371,
      "learning_rate": 1.63e-05,
      "loss": 1.8997,
      "step": 1925
    },
    {
      "epoch": 1.851923076923077,
      "grad_norm": 5.505876064300537,
      "learning_rate": 1.6298076923076923e-05,
      "loss": 0.9719,
      "step": 1926
    },
    {
      "epoch": 1.8528846153846152,
      "grad_norm": 8.419188499450684,
      "learning_rate": 1.629615384615385e-05,
      "loss": 0.9075,
      "step": 1927
    },
    {
      "epoch": 1.853846153846154,
      "grad_norm": 9.105232238769531,
      "learning_rate": 1.629423076923077e-05,
      "loss": 1.3262,
      "step": 1928
    },
    {
      "epoch": 1.8548076923076922,
      "grad_norm": 6.012508869171143,
      "learning_rate": 1.6292307692307694e-05,
      "loss": 1.2133,
      "step": 1929
    },
    {
      "epoch": 1.8557692307692308,
      "grad_norm": 5.487518310546875,
      "learning_rate": 1.6290384615384617e-05,
      "loss": 0.9512,
      "step": 1930
    },
    {
      "epoch": 1.856730769230769,
      "grad_norm": 15.389081954956055,
      "learning_rate": 1.628846153846154e-05,
      "loss": 0.9475,
      "step": 1931
    },
    {
      "epoch": 1.8576923076923078,
      "grad_norm": 4.447246551513672,
      "learning_rate": 1.6286538461538463e-05,
      "loss": 1.1954,
      "step": 1932
    },
    {
      "epoch": 1.858653846153846,
      "grad_norm": 10.433296203613281,
      "learning_rate": 1.6284615384615385e-05,
      "loss": 0.4303,
      "step": 1933
    },
    {
      "epoch": 1.8596153846153847,
      "grad_norm": 8.517550468444824,
      "learning_rate": 1.6282692307692308e-05,
      "loss": 1.368,
      "step": 1934
    },
    {
      "epoch": 1.8605769230769231,
      "grad_norm": 7.098341464996338,
      "learning_rate": 1.628076923076923e-05,
      "loss": 1.266,
      "step": 1935
    },
    {
      "epoch": 1.8615384615384616,
      "grad_norm": 11.23501968383789,
      "learning_rate": 1.6278846153846154e-05,
      "loss": 1.4793,
      "step": 1936
    },
    {
      "epoch": 1.8625,
      "grad_norm": 6.600275993347168,
      "learning_rate": 1.627692307692308e-05,
      "loss": 1.5623,
      "step": 1937
    },
    {
      "epoch": 1.8634615384615385,
      "grad_norm": 6.109394550323486,
      "learning_rate": 1.6275e-05,
      "loss": 0.2516,
      "step": 1938
    },
    {
      "epoch": 1.864423076923077,
      "grad_norm": 11.232381820678711,
      "learning_rate": 1.6273076923076925e-05,
      "loss": 1.6578,
      "step": 1939
    },
    {
      "epoch": 1.8653846153846154,
      "grad_norm": 7.930898666381836,
      "learning_rate": 1.6271153846153848e-05,
      "loss": 1.4113,
      "step": 1940
    },
    {
      "epoch": 1.8663461538461539,
      "grad_norm": 9.319832801818848,
      "learning_rate": 1.626923076923077e-05,
      "loss": 1.1479,
      "step": 1941
    },
    {
      "epoch": 1.8673076923076923,
      "grad_norm": 8.191349983215332,
      "learning_rate": 1.6267307692307694e-05,
      "loss": 0.3235,
      "step": 1942
    },
    {
      "epoch": 1.8682692307692308,
      "grad_norm": 6.495467662811279,
      "learning_rate": 1.6265384615384616e-05,
      "loss": 1.0669,
      "step": 1943
    },
    {
      "epoch": 1.8692307692307693,
      "grad_norm": 5.212742328643799,
      "learning_rate": 1.626346153846154e-05,
      "loss": 1.0801,
      "step": 1944
    },
    {
      "epoch": 1.8701923076923077,
      "grad_norm": 7.297472953796387,
      "learning_rate": 1.6261538461538462e-05,
      "loss": 1.9168,
      "step": 1945
    },
    {
      "epoch": 1.8711538461538462,
      "grad_norm": 6.516891956329346,
      "learning_rate": 1.6259615384615385e-05,
      "loss": 1.276,
      "step": 1946
    },
    {
      "epoch": 1.8721153846153846,
      "grad_norm": 6.645689487457275,
      "learning_rate": 1.625769230769231e-05,
      "loss": 1.2962,
      "step": 1947
    },
    {
      "epoch": 1.873076923076923,
      "grad_norm": 7.350721836090088,
      "learning_rate": 1.625576923076923e-05,
      "loss": 1.2204,
      "step": 1948
    },
    {
      "epoch": 1.8740384615384615,
      "grad_norm": 5.5837860107421875,
      "learning_rate": 1.6253846153846156e-05,
      "loss": 1.9533,
      "step": 1949
    },
    {
      "epoch": 1.875,
      "grad_norm": 8.1690034866333,
      "learning_rate": 1.6251923076923076e-05,
      "loss": 1.5664,
      "step": 1950
    },
    {
      "epoch": 1.8759615384615385,
      "grad_norm": 4.678302764892578,
      "learning_rate": 1.6250000000000002e-05,
      "loss": 0.5041,
      "step": 1951
    },
    {
      "epoch": 1.876923076923077,
      "grad_norm": 9.566603660583496,
      "learning_rate": 1.6248076923076925e-05,
      "loss": 0.9715,
      "step": 1952
    },
    {
      "epoch": 1.8778846153846154,
      "grad_norm": 7.112521171569824,
      "learning_rate": 1.6246153846153848e-05,
      "loss": 1.0707,
      "step": 1953
    },
    {
      "epoch": 1.8788461538461538,
      "grad_norm": 6.998937606811523,
      "learning_rate": 1.624423076923077e-05,
      "loss": 1.3624,
      "step": 1954
    },
    {
      "epoch": 1.8798076923076923,
      "grad_norm": 16.675132751464844,
      "learning_rate": 1.6242307692307693e-05,
      "loss": 1.5199,
      "step": 1955
    },
    {
      "epoch": 1.8807692307692307,
      "grad_norm": 5.799519062042236,
      "learning_rate": 1.6240384615384616e-05,
      "loss": 1.2273,
      "step": 1956
    },
    {
      "epoch": 1.8817307692307692,
      "grad_norm": 7.664446830749512,
      "learning_rate": 1.6238461538461542e-05,
      "loss": 1.3782,
      "step": 1957
    },
    {
      "epoch": 1.8826923076923077,
      "grad_norm": 7.218520164489746,
      "learning_rate": 1.623653846153846e-05,
      "loss": 1.3656,
      "step": 1958
    },
    {
      "epoch": 1.8836538461538461,
      "grad_norm": 6.210265159606934,
      "learning_rate": 1.6234615384615388e-05,
      "loss": 1.6192,
      "step": 1959
    },
    {
      "epoch": 1.8846153846153846,
      "grad_norm": 7.82808256149292,
      "learning_rate": 1.6232692307692307e-05,
      "loss": 0.7445,
      "step": 1960
    },
    {
      "epoch": 1.885576923076923,
      "grad_norm": 4.453023433685303,
      "learning_rate": 1.6230769230769233e-05,
      "loss": 0.0493,
      "step": 1961
    },
    {
      "epoch": 1.8865384615384615,
      "grad_norm": 10.85432243347168,
      "learning_rate": 1.6228846153846156e-05,
      "loss": 1.1301,
      "step": 1962
    },
    {
      "epoch": 1.8875,
      "grad_norm": 9.631217956542969,
      "learning_rate": 1.622692307692308e-05,
      "loss": 1.1806,
      "step": 1963
    },
    {
      "epoch": 1.8884615384615384,
      "grad_norm": 5.711562156677246,
      "learning_rate": 1.6225e-05,
      "loss": 0.502,
      "step": 1964
    },
    {
      "epoch": 1.8894230769230769,
      "grad_norm": 6.0320210456848145,
      "learning_rate": 1.6223076923076924e-05,
      "loss": 1.6925,
      "step": 1965
    },
    {
      "epoch": 1.8903846153846153,
      "grad_norm": 7.410854339599609,
      "learning_rate": 1.6221153846153847e-05,
      "loss": 1.4082,
      "step": 1966
    },
    {
      "epoch": 1.891346153846154,
      "grad_norm": 7.073095321655273,
      "learning_rate": 1.6219230769230773e-05,
      "loss": 1.2396,
      "step": 1967
    },
    {
      "epoch": 1.8923076923076922,
      "grad_norm": 8.618432998657227,
      "learning_rate": 1.6217307692307692e-05,
      "loss": 0.2857,
      "step": 1968
    },
    {
      "epoch": 1.893269230769231,
      "grad_norm": 8.459792137145996,
      "learning_rate": 1.621538461538462e-05,
      "loss": 0.2723,
      "step": 1969
    },
    {
      "epoch": 1.8942307692307692,
      "grad_norm": 6.813920974731445,
      "learning_rate": 1.6213461538461538e-05,
      "loss": 1.4235,
      "step": 1970
    },
    {
      "epoch": 1.8951923076923078,
      "grad_norm": 8.73069953918457,
      "learning_rate": 1.6211538461538464e-05,
      "loss": 1.5247,
      "step": 1971
    },
    {
      "epoch": 1.896153846153846,
      "grad_norm": 7.732305526733398,
      "learning_rate": 1.6209615384615387e-05,
      "loss": 0.9241,
      "step": 1972
    },
    {
      "epoch": 1.8971153846153848,
      "grad_norm": 4.966732501983643,
      "learning_rate": 1.620769230769231e-05,
      "loss": 1.2437,
      "step": 1973
    },
    {
      "epoch": 1.898076923076923,
      "grad_norm": 6.35805082321167,
      "learning_rate": 1.6205769230769232e-05,
      "loss": 0.3431,
      "step": 1974
    },
    {
      "epoch": 1.8990384615384617,
      "grad_norm": 6.1884918212890625,
      "learning_rate": 1.6203846153846155e-05,
      "loss": 1.215,
      "step": 1975
    },
    {
      "epoch": 1.9,
      "grad_norm": 7.2697296142578125,
      "learning_rate": 1.6201923076923078e-05,
      "loss": 1.27,
      "step": 1976
    },
    {
      "epoch": 1.9009615384615386,
      "grad_norm": 10.398523330688477,
      "learning_rate": 1.62e-05,
      "loss": 1.5357,
      "step": 1977
    },
    {
      "epoch": 1.9019230769230768,
      "grad_norm": 15.343512535095215,
      "learning_rate": 1.6198076923076924e-05,
      "loss": 1.9778,
      "step": 1978
    },
    {
      "epoch": 1.9028846153846155,
      "grad_norm": 6.486144065856934,
      "learning_rate": 1.619615384615385e-05,
      "loss": 1.8163,
      "step": 1979
    },
    {
      "epoch": 1.9038461538461537,
      "grad_norm": 6.959950923919678,
      "learning_rate": 1.619423076923077e-05,
      "loss": 1.2894,
      "step": 1980
    },
    {
      "epoch": 1.9048076923076924,
      "grad_norm": 5.688124179840088,
      "learning_rate": 1.6192307692307695e-05,
      "loss": 1.304,
      "step": 1981
    },
    {
      "epoch": 1.9057692307692307,
      "grad_norm": 6.795542240142822,
      "learning_rate": 1.6190384615384618e-05,
      "loss": 0.8778,
      "step": 1982
    },
    {
      "epoch": 1.9067307692307693,
      "grad_norm": 6.523853778839111,
      "learning_rate": 1.618846153846154e-05,
      "loss": 1.4689,
      "step": 1983
    },
    {
      "epoch": 1.9076923076923076,
      "grad_norm": 5.028562068939209,
      "learning_rate": 1.6186538461538464e-05,
      "loss": 1.4637,
      "step": 1984
    },
    {
      "epoch": 1.9086538461538463,
      "grad_norm": 5.415205955505371,
      "learning_rate": 1.6184615384615386e-05,
      "loss": 1.3456,
      "step": 1985
    },
    {
      "epoch": 1.9096153846153845,
      "grad_norm": 7.3938703536987305,
      "learning_rate": 1.618269230769231e-05,
      "loss": 0.9708,
      "step": 1986
    },
    {
      "epoch": 1.9105769230769232,
      "grad_norm": 6.028899192810059,
      "learning_rate": 1.6180769230769232e-05,
      "loss": 0.777,
      "step": 1987
    },
    {
      "epoch": 1.9115384615384614,
      "grad_norm": 5.68849515914917,
      "learning_rate": 1.6178846153846155e-05,
      "loss": 0.2197,
      "step": 1988
    },
    {
      "epoch": 1.9125,
      "grad_norm": 13.803363800048828,
      "learning_rate": 1.6176923076923077e-05,
      "loss": 0.9715,
      "step": 1989
    },
    {
      "epoch": 1.9134615384615383,
      "grad_norm": 5.844914436340332,
      "learning_rate": 1.6175e-05,
      "loss": 1.775,
      "step": 1990
    },
    {
      "epoch": 1.914423076923077,
      "grad_norm": 5.856529235839844,
      "learning_rate": 1.6173076923076926e-05,
      "loss": 0.6969,
      "step": 1991
    },
    {
      "epoch": 1.9153846153846152,
      "grad_norm": 9.29832935333252,
      "learning_rate": 1.6171153846153846e-05,
      "loss": 1.6559,
      "step": 1992
    },
    {
      "epoch": 1.916346153846154,
      "grad_norm": 5.849427700042725,
      "learning_rate": 1.6169230769230772e-05,
      "loss": 1.3081,
      "step": 1993
    },
    {
      "epoch": 1.9173076923076922,
      "grad_norm": 8.22684097290039,
      "learning_rate": 1.6167307692307695e-05,
      "loss": 0.2885,
      "step": 1994
    },
    {
      "epoch": 1.9182692307692308,
      "grad_norm": 9.552298545837402,
      "learning_rate": 1.6165384615384617e-05,
      "loss": 1.273,
      "step": 1995
    },
    {
      "epoch": 1.919230769230769,
      "grad_norm": 10.091959953308105,
      "learning_rate": 1.616346153846154e-05,
      "loss": 1.38,
      "step": 1996
    },
    {
      "epoch": 1.9201923076923078,
      "grad_norm": 10.212574005126953,
      "learning_rate": 1.6161538461538463e-05,
      "loss": 0.4509,
      "step": 1997
    },
    {
      "epoch": 1.921153846153846,
      "grad_norm": 8.368180274963379,
      "learning_rate": 1.6159615384615386e-05,
      "loss": 0.7761,
      "step": 1998
    },
    {
      "epoch": 1.9221153846153847,
      "grad_norm": 6.361927032470703,
      "learning_rate": 1.615769230769231e-05,
      "loss": 1.1795,
      "step": 1999
    },
    {
      "epoch": 1.9230769230769231,
      "grad_norm": 8.163265228271484,
      "learning_rate": 1.615576923076923e-05,
      "loss": 1.3745,
      "step": 2000
    },
    {
      "epoch": 1.9240384615384616,
      "grad_norm": 5.925070285797119,
      "learning_rate": 1.6153846153846154e-05,
      "loss": 1.8511,
      "step": 2001
    },
    {
      "epoch": 1.925,
      "grad_norm": 5.351576805114746,
      "learning_rate": 1.6151923076923077e-05,
      "loss": 0.7625,
      "step": 2002
    },
    {
      "epoch": 1.9259615384615385,
      "grad_norm": 4.9983439445495605,
      "learning_rate": 1.6150000000000003e-05,
      "loss": 0.7699,
      "step": 2003
    },
    {
      "epoch": 1.926923076923077,
      "grad_norm": 5.776206016540527,
      "learning_rate": 1.6148076923076926e-05,
      "loss": 1.3128,
      "step": 2004
    },
    {
      "epoch": 1.9278846153846154,
      "grad_norm": 8.04969310760498,
      "learning_rate": 1.614615384615385e-05,
      "loss": 1.3596,
      "step": 2005
    },
    {
      "epoch": 1.9288461538461539,
      "grad_norm": 5.592718601226807,
      "learning_rate": 1.614423076923077e-05,
      "loss": 1.4223,
      "step": 2006
    },
    {
      "epoch": 1.9298076923076923,
      "grad_norm": 6.680544853210449,
      "learning_rate": 1.6142307692307694e-05,
      "loss": 0.9876,
      "step": 2007
    },
    {
      "epoch": 1.9307692307692308,
      "grad_norm": 7.399256229400635,
      "learning_rate": 1.6140384615384617e-05,
      "loss": 1.3078,
      "step": 2008
    },
    {
      "epoch": 1.9317307692307693,
      "grad_norm": 5.398623943328857,
      "learning_rate": 1.613846153846154e-05,
      "loss": 1.1665,
      "step": 2009
    },
    {
      "epoch": 1.9326923076923077,
      "grad_norm": 8.126639366149902,
      "learning_rate": 1.6136538461538462e-05,
      "loss": 0.9115,
      "step": 2010
    },
    {
      "epoch": 1.9336538461538462,
      "grad_norm": 7.332918643951416,
      "learning_rate": 1.6134615384615385e-05,
      "loss": 1.496,
      "step": 2011
    },
    {
      "epoch": 1.9346153846153846,
      "grad_norm": 9.304422378540039,
      "learning_rate": 1.6132692307692308e-05,
      "loss": 0.3258,
      "step": 2012
    },
    {
      "epoch": 1.935576923076923,
      "grad_norm": 4.047938823699951,
      "learning_rate": 1.613076923076923e-05,
      "loss": 0.0992,
      "step": 2013
    },
    {
      "epoch": 1.9365384615384615,
      "grad_norm": 6.259859561920166,
      "learning_rate": 1.6128846153846157e-05,
      "loss": 1.8447,
      "step": 2014
    },
    {
      "epoch": 1.9375,
      "grad_norm": 6.285309791564941,
      "learning_rate": 1.6126923076923076e-05,
      "loss": 1.3815,
      "step": 2015
    },
    {
      "epoch": 1.9384615384615385,
      "grad_norm": 6.610029697418213,
      "learning_rate": 1.6125000000000002e-05,
      "loss": 1.9481,
      "step": 2016
    },
    {
      "epoch": 1.939423076923077,
      "grad_norm": 13.475692749023438,
      "learning_rate": 1.6123076923076925e-05,
      "loss": 1.6008,
      "step": 2017
    },
    {
      "epoch": 1.9403846153846154,
      "grad_norm": 12.423246383666992,
      "learning_rate": 1.6121153846153848e-05,
      "loss": 1.6713,
      "step": 2018
    },
    {
      "epoch": 1.9413461538461538,
      "grad_norm": 5.505310535430908,
      "learning_rate": 1.611923076923077e-05,
      "loss": 1.5661,
      "step": 2019
    },
    {
      "epoch": 1.9423076923076923,
      "grad_norm": 8.041099548339844,
      "learning_rate": 1.6117307692307693e-05,
      "loss": 2.1389,
      "step": 2020
    },
    {
      "epoch": 1.9432692307692307,
      "grad_norm": 5.194242477416992,
      "learning_rate": 1.6115384615384616e-05,
      "loss": 0.3474,
      "step": 2021
    },
    {
      "epoch": 1.9442307692307692,
      "grad_norm": 9.585309982299805,
      "learning_rate": 1.611346153846154e-05,
      "loss": 1.2907,
      "step": 2022
    },
    {
      "epoch": 1.9451923076923077,
      "grad_norm": 7.361310958862305,
      "learning_rate": 1.6111538461538462e-05,
      "loss": 1.1435,
      "step": 2023
    },
    {
      "epoch": 1.9461538461538461,
      "grad_norm": 4.966867446899414,
      "learning_rate": 1.6109615384615388e-05,
      "loss": 1.8576,
      "step": 2024
    },
    {
      "epoch": 1.9471153846153846,
      "grad_norm": 2.272719144821167,
      "learning_rate": 1.6107692307692307e-05,
      "loss": 0.0206,
      "step": 2025
    },
    {
      "epoch": 1.948076923076923,
      "grad_norm": 6.761167526245117,
      "learning_rate": 1.6105769230769233e-05,
      "loss": 1.5308,
      "step": 2026
    },
    {
      "epoch": 1.9490384615384615,
      "grad_norm": 5.900012493133545,
      "learning_rate": 1.6103846153846153e-05,
      "loss": 1.3364,
      "step": 2027
    },
    {
      "epoch": 1.95,
      "grad_norm": 5.3731689453125,
      "learning_rate": 1.610192307692308e-05,
      "loss": 1.4735,
      "step": 2028
    },
    {
      "epoch": 1.9509615384615384,
      "grad_norm": 13.924066543579102,
      "learning_rate": 1.6100000000000002e-05,
      "loss": 0.6889,
      "step": 2029
    },
    {
      "epoch": 1.9519230769230769,
      "grad_norm": 7.42461633682251,
      "learning_rate": 1.6098076923076924e-05,
      "loss": 1.9721,
      "step": 2030
    },
    {
      "epoch": 1.9528846153846153,
      "grad_norm": 7.370818614959717,
      "learning_rate": 1.6096153846153847e-05,
      "loss": 1.5295,
      "step": 2031
    },
    {
      "epoch": 1.953846153846154,
      "grad_norm": 9.731813430786133,
      "learning_rate": 1.609423076923077e-05,
      "loss": 1.6137,
      "step": 2032
    },
    {
      "epoch": 1.9548076923076922,
      "grad_norm": 5.9595842361450195,
      "learning_rate": 1.6092307692307693e-05,
      "loss": 1.1103,
      "step": 2033
    },
    {
      "epoch": 1.955769230769231,
      "grad_norm": 6.103504180908203,
      "learning_rate": 1.609038461538462e-05,
      "loss": 1.587,
      "step": 2034
    },
    {
      "epoch": 1.9567307692307692,
      "grad_norm": 8.145483016967773,
      "learning_rate": 1.608846153846154e-05,
      "loss": 1.3039,
      "step": 2035
    },
    {
      "epoch": 1.9576923076923078,
      "grad_norm": 7.919471263885498,
      "learning_rate": 1.6086538461538464e-05,
      "loss": 1.5769,
      "step": 2036
    },
    {
      "epoch": 1.958653846153846,
      "grad_norm": 6.391797065734863,
      "learning_rate": 1.6084615384615384e-05,
      "loss": 1.5454,
      "step": 2037
    },
    {
      "epoch": 1.9596153846153848,
      "grad_norm": 29.18770980834961,
      "learning_rate": 1.608269230769231e-05,
      "loss": 1.3074,
      "step": 2038
    },
    {
      "epoch": 1.960576923076923,
      "grad_norm": 7.8837738037109375,
      "learning_rate": 1.6080769230769233e-05,
      "loss": 1.5544,
      "step": 2039
    },
    {
      "epoch": 1.9615384615384617,
      "grad_norm": 7.800563335418701,
      "learning_rate": 1.6078846153846156e-05,
      "loss": 1.1908,
      "step": 2040
    },
    {
      "epoch": 1.9625,
      "grad_norm": 9.173737525939941,
      "learning_rate": 1.607692307692308e-05,
      "loss": 2.0063,
      "step": 2041
    },
    {
      "epoch": 1.9634615384615386,
      "grad_norm": 13.697127342224121,
      "learning_rate": 1.6075e-05,
      "loss": 0.8968,
      "step": 2042
    },
    {
      "epoch": 1.9644230769230768,
      "grad_norm": 7.60862398147583,
      "learning_rate": 1.6073076923076924e-05,
      "loss": 1.5426,
      "step": 2043
    },
    {
      "epoch": 1.9653846153846155,
      "grad_norm": 8.93067455291748,
      "learning_rate": 1.607115384615385e-05,
      "loss": 0.7338,
      "step": 2044
    },
    {
      "epoch": 1.9663461538461537,
      "grad_norm": 3.6739327907562256,
      "learning_rate": 1.606923076923077e-05,
      "loss": 0.0764,
      "step": 2045
    },
    {
      "epoch": 1.9673076923076924,
      "grad_norm": 6.488947868347168,
      "learning_rate": 1.6067307692307696e-05,
      "loss": 1.4711,
      "step": 2046
    },
    {
      "epoch": 1.9682692307692307,
      "grad_norm": 5.1684041023254395,
      "learning_rate": 1.6065384615384615e-05,
      "loss": 1.1143,
      "step": 2047
    },
    {
      "epoch": 1.9692307692307693,
      "grad_norm": 5.772698402404785,
      "learning_rate": 1.606346153846154e-05,
      "loss": 1.6738,
      "step": 2048
    },
    {
      "epoch": 1.9701923076923076,
      "grad_norm": 5.581559181213379,
      "learning_rate": 1.606153846153846e-05,
      "loss": 0.0998,
      "step": 2049
    },
    {
      "epoch": 1.9711538461538463,
      "grad_norm": 7.049792766571045,
      "learning_rate": 1.6059615384615387e-05,
      "loss": 1.8663,
      "step": 2050
    },
    {
      "epoch": 1.9721153846153845,
      "grad_norm": 6.317220211029053,
      "learning_rate": 1.605769230769231e-05,
      "loss": 1.3989,
      "step": 2051
    },
    {
      "epoch": 1.9730769230769232,
      "grad_norm": 6.7607598304748535,
      "learning_rate": 1.6055769230769232e-05,
      "loss": 1.0699,
      "step": 2052
    },
    {
      "epoch": 1.9740384615384614,
      "grad_norm": 6.926473617553711,
      "learning_rate": 1.6053846153846155e-05,
      "loss": 1.6473,
      "step": 2053
    },
    {
      "epoch": 1.975,
      "grad_norm": 6.738247871398926,
      "learning_rate": 1.6051923076923078e-05,
      "loss": 1.3797,
      "step": 2054
    },
    {
      "epoch": 1.9759615384615383,
      "grad_norm": 8.299127578735352,
      "learning_rate": 1.605e-05,
      "loss": 1.0629,
      "step": 2055
    },
    {
      "epoch": 1.976923076923077,
      "grad_norm": 6.513525009155273,
      "learning_rate": 1.6048076923076927e-05,
      "loss": 1.9325,
      "step": 2056
    },
    {
      "epoch": 1.9778846153846152,
      "grad_norm": 6.009270668029785,
      "learning_rate": 1.6046153846153846e-05,
      "loss": 1.8428,
      "step": 2057
    },
    {
      "epoch": 1.978846153846154,
      "grad_norm": 6.408902645111084,
      "learning_rate": 1.6044230769230772e-05,
      "loss": 2.0241,
      "step": 2058
    },
    {
      "epoch": 1.9798076923076922,
      "grad_norm": 10.687145233154297,
      "learning_rate": 1.604230769230769e-05,
      "loss": 0.9983,
      "step": 2059
    },
    {
      "epoch": 1.9807692307692308,
      "grad_norm": 8.821942329406738,
      "learning_rate": 1.6040384615384618e-05,
      "loss": 1.5443,
      "step": 2060
    },
    {
      "epoch": 1.981730769230769,
      "grad_norm": 11.427081108093262,
      "learning_rate": 1.603846153846154e-05,
      "loss": 1.4723,
      "step": 2061
    },
    {
      "epoch": 1.9826923076923078,
      "grad_norm": 7.903630256652832,
      "learning_rate": 1.6036538461538463e-05,
      "loss": 1.3411,
      "step": 2062
    },
    {
      "epoch": 1.983653846153846,
      "grad_norm": 5.176398754119873,
      "learning_rate": 1.6034615384615386e-05,
      "loss": 1.3112,
      "step": 2063
    },
    {
      "epoch": 1.9846153846153847,
      "grad_norm": 14.780170440673828,
      "learning_rate": 1.603269230769231e-05,
      "loss": 2.5576,
      "step": 2064
    },
    {
      "epoch": 1.9855769230769231,
      "grad_norm": 7.531895637512207,
      "learning_rate": 1.603076923076923e-05,
      "loss": 1.4777,
      "step": 2065
    },
    {
      "epoch": 1.9865384615384616,
      "grad_norm": 6.518707752227783,
      "learning_rate": 1.6028846153846154e-05,
      "loss": 2.0152,
      "step": 2066
    },
    {
      "epoch": 1.9875,
      "grad_norm": 7.48134183883667,
      "learning_rate": 1.6026923076923077e-05,
      "loss": 1.003,
      "step": 2067
    },
    {
      "epoch": 1.9884615384615385,
      "grad_norm": 10.329699516296387,
      "learning_rate": 1.6025000000000003e-05,
      "loss": 1.5706,
      "step": 2068
    },
    {
      "epoch": 1.989423076923077,
      "grad_norm": 8.779524803161621,
      "learning_rate": 1.6023076923076923e-05,
      "loss": 1.2742,
      "step": 2069
    },
    {
      "epoch": 1.9903846153846154,
      "grad_norm": 4.91042947769165,
      "learning_rate": 1.602115384615385e-05,
      "loss": 0.246,
      "step": 2070
    },
    {
      "epoch": 1.9913461538461539,
      "grad_norm": 9.118616104125977,
      "learning_rate": 1.601923076923077e-05,
      "loss": 0.6405,
      "step": 2071
    },
    {
      "epoch": 1.9923076923076923,
      "grad_norm": 6.581051826477051,
      "learning_rate": 1.6017307692307694e-05,
      "loss": 1.9852,
      "step": 2072
    },
    {
      "epoch": 1.9932692307692308,
      "grad_norm": 6.835845470428467,
      "learning_rate": 1.6015384615384617e-05,
      "loss": 2.1737,
      "step": 2073
    },
    {
      "epoch": 1.9942307692307693,
      "grad_norm": 12.234651565551758,
      "learning_rate": 1.601346153846154e-05,
      "loss": 0.8336,
      "step": 2074
    },
    {
      "epoch": 1.9951923076923077,
      "grad_norm": 9.268725395202637,
      "learning_rate": 1.6011538461538463e-05,
      "loss": 0.9117,
      "step": 2075
    },
    {
      "epoch": 1.9961538461538462,
      "grad_norm": 5.290547847747803,
      "learning_rate": 1.6009615384615385e-05,
      "loss": 1.6575,
      "step": 2076
    },
    {
      "epoch": 1.9971153846153846,
      "grad_norm": 15.975418090820312,
      "learning_rate": 1.6007692307692308e-05,
      "loss": 2.0313,
      "step": 2077
    },
    {
      "epoch": 1.998076923076923,
      "grad_norm": 5.241466522216797,
      "learning_rate": 1.600576923076923e-05,
      "loss": 1.2407,
      "step": 2078
    },
    {
      "epoch": 1.9990384615384615,
      "grad_norm": 5.6728434562683105,
      "learning_rate": 1.6003846153846154e-05,
      "loss": 1.5143,
      "step": 2079
    },
    {
      "epoch": 2.0,
      "grad_norm": 8.87636947631836,
      "learning_rate": 1.600192307692308e-05,
      "loss": 1.7727,
      "step": 2080
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.0771836042404175,
      "eval_runtime": 22.7343,
      "eval_samples_per_second": 10.161,
      "eval_steps_per_second": 10.161,
      "step": 2080
    },
    {
      "epoch": 2.0009615384615387,
      "grad_norm": 6.553740978240967,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.3919,
      "step": 2081
    },
    {
      "epoch": 2.001923076923077,
      "grad_norm": 5.286375045776367,
      "learning_rate": 1.5998076923076925e-05,
      "loss": 1.2524,
      "step": 2082
    },
    {
      "epoch": 2.0028846153846156,
      "grad_norm": 6.074121475219727,
      "learning_rate": 1.5996153846153848e-05,
      "loss": 1.6778,
      "step": 2083
    },
    {
      "epoch": 2.003846153846154,
      "grad_norm": 5.495248317718506,
      "learning_rate": 1.599423076923077e-05,
      "loss": 1.8103,
      "step": 2084
    },
    {
      "epoch": 2.0048076923076925,
      "grad_norm": 5.604207992553711,
      "learning_rate": 1.5992307692307694e-05,
      "loss": 0.8094,
      "step": 2085
    },
    {
      "epoch": 2.0057692307692307,
      "grad_norm": 4.782142639160156,
      "learning_rate": 1.5990384615384617e-05,
      "loss": 1.2325,
      "step": 2086
    },
    {
      "epoch": 2.0067307692307694,
      "grad_norm": 4.434542655944824,
      "learning_rate": 1.598846153846154e-05,
      "loss": 1.2753,
      "step": 2087
    },
    {
      "epoch": 2.0076923076923077,
      "grad_norm": 7.068711280822754,
      "learning_rate": 1.5986538461538462e-05,
      "loss": 1.3071,
      "step": 2088
    },
    {
      "epoch": 2.0086538461538463,
      "grad_norm": 5.069392681121826,
      "learning_rate": 1.5984615384615385e-05,
      "loss": 1.5326,
      "step": 2089
    },
    {
      "epoch": 2.0096153846153846,
      "grad_norm": 8.0171480178833,
      "learning_rate": 1.5982692307692308e-05,
      "loss": 0.2794,
      "step": 2090
    },
    {
      "epoch": 2.0105769230769233,
      "grad_norm": 5.456066131591797,
      "learning_rate": 1.5980769230769234e-05,
      "loss": 0.8205,
      "step": 2091
    },
    {
      "epoch": 2.0115384615384615,
      "grad_norm": 11.621848106384277,
      "learning_rate": 1.5978846153846153e-05,
      "loss": 1.2345,
      "step": 2092
    },
    {
      "epoch": 2.0125,
      "grad_norm": 6.341873645782471,
      "learning_rate": 1.597692307692308e-05,
      "loss": 1.4734,
      "step": 2093
    },
    {
      "epoch": 2.0134615384615384,
      "grad_norm": 10.222736358642578,
      "learning_rate": 1.5975000000000002e-05,
      "loss": 1.0387,
      "step": 2094
    },
    {
      "epoch": 2.014423076923077,
      "grad_norm": 7.481212139129639,
      "learning_rate": 1.5973076923076925e-05,
      "loss": 0.5021,
      "step": 2095
    },
    {
      "epoch": 2.0153846153846153,
      "grad_norm": 11.374774932861328,
      "learning_rate": 1.5971153846153848e-05,
      "loss": 2.5115,
      "step": 2096
    },
    {
      "epoch": 2.016346153846154,
      "grad_norm": 10.707120895385742,
      "learning_rate": 1.596923076923077e-05,
      "loss": 0.3385,
      "step": 2097
    },
    {
      "epoch": 2.0173076923076922,
      "grad_norm": 6.562229156494141,
      "learning_rate": 1.5967307692307693e-05,
      "loss": 1.3269,
      "step": 2098
    },
    {
      "epoch": 2.018269230769231,
      "grad_norm": 8.861615180969238,
      "learning_rate": 1.5965384615384616e-05,
      "loss": 1.4513,
      "step": 2099
    },
    {
      "epoch": 2.019230769230769,
      "grad_norm": 5.960991382598877,
      "learning_rate": 1.596346153846154e-05,
      "loss": 0.9261,
      "step": 2100
    },
    {
      "epoch": 2.020192307692308,
      "grad_norm": 5.719711780548096,
      "learning_rate": 1.5961538461538465e-05,
      "loss": 1.2954,
      "step": 2101
    },
    {
      "epoch": 2.021153846153846,
      "grad_norm": 5.550304889678955,
      "learning_rate": 1.5959615384615384e-05,
      "loss": 2.1078,
      "step": 2102
    },
    {
      "epoch": 2.0221153846153848,
      "grad_norm": 7.082235813140869,
      "learning_rate": 1.595769230769231e-05,
      "loss": 0.2328,
      "step": 2103
    },
    {
      "epoch": 2.023076923076923,
      "grad_norm": 4.994142532348633,
      "learning_rate": 1.595576923076923e-05,
      "loss": 1.4496,
      "step": 2104
    },
    {
      "epoch": 2.0240384615384617,
      "grad_norm": 6.6631269454956055,
      "learning_rate": 1.5953846153846156e-05,
      "loss": 1.0068,
      "step": 2105
    },
    {
      "epoch": 2.025,
      "grad_norm": 7.0849080085754395,
      "learning_rate": 1.595192307692308e-05,
      "loss": 1.7715,
      "step": 2106
    },
    {
      "epoch": 2.0259615384615386,
      "grad_norm": 2.391117811203003,
      "learning_rate": 1.595e-05,
      "loss": 0.0253,
      "step": 2107
    },
    {
      "epoch": 2.026923076923077,
      "grad_norm": 8.130337715148926,
      "learning_rate": 1.5948076923076924e-05,
      "loss": 1.492,
      "step": 2108
    },
    {
      "epoch": 2.0278846153846155,
      "grad_norm": 5.5826897621154785,
      "learning_rate": 1.5946153846153847e-05,
      "loss": 0.2065,
      "step": 2109
    },
    {
      "epoch": 2.0288461538461537,
      "grad_norm": 5.630127906799316,
      "learning_rate": 1.594423076923077e-05,
      "loss": 1.3422,
      "step": 2110
    },
    {
      "epoch": 2.0298076923076924,
      "grad_norm": 8.584209442138672,
      "learning_rate": 1.5942307692307696e-05,
      "loss": 1.7626,
      "step": 2111
    },
    {
      "epoch": 2.0307692307692307,
      "grad_norm": 6.111644744873047,
      "learning_rate": 1.5940384615384615e-05,
      "loss": 1.3902,
      "step": 2112
    },
    {
      "epoch": 2.0317307692307693,
      "grad_norm": 8.001999855041504,
      "learning_rate": 1.593846153846154e-05,
      "loss": 1.6547,
      "step": 2113
    },
    {
      "epoch": 2.0326923076923076,
      "grad_norm": 11.888970375061035,
      "learning_rate": 1.593653846153846e-05,
      "loss": 1.1172,
      "step": 2114
    },
    {
      "epoch": 2.0336538461538463,
      "grad_norm": 8.24275016784668,
      "learning_rate": 1.5934615384615387e-05,
      "loss": 1.4422,
      "step": 2115
    },
    {
      "epoch": 2.0346153846153845,
      "grad_norm": 5.274475574493408,
      "learning_rate": 1.5932692307692306e-05,
      "loss": 1.9604,
      "step": 2116
    },
    {
      "epoch": 2.035576923076923,
      "grad_norm": 9.685075759887695,
      "learning_rate": 1.5930769230769233e-05,
      "loss": 1.166,
      "step": 2117
    },
    {
      "epoch": 2.0365384615384614,
      "grad_norm": 9.941707611083984,
      "learning_rate": 1.5928846153846155e-05,
      "loss": 0.7366,
      "step": 2118
    },
    {
      "epoch": 2.0375,
      "grad_norm": 5.930477619171143,
      "learning_rate": 1.5926923076923078e-05,
      "loss": 0.8534,
      "step": 2119
    },
    {
      "epoch": 2.0384615384615383,
      "grad_norm": 62.35731506347656,
      "learning_rate": 1.5925e-05,
      "loss": 0.5208,
      "step": 2120
    },
    {
      "epoch": 2.039423076923077,
      "grad_norm": 13.75981616973877,
      "learning_rate": 1.5923076923076924e-05,
      "loss": 0.4587,
      "step": 2121
    },
    {
      "epoch": 2.0403846153846152,
      "grad_norm": 5.981024742126465,
      "learning_rate": 1.5921153846153846e-05,
      "loss": 1.331,
      "step": 2122
    },
    {
      "epoch": 2.041346153846154,
      "grad_norm": 6.367982864379883,
      "learning_rate": 1.5919230769230773e-05,
      "loss": 1.0288,
      "step": 2123
    },
    {
      "epoch": 2.042307692307692,
      "grad_norm": 5.99264669418335,
      "learning_rate": 1.5917307692307692e-05,
      "loss": 1.5572,
      "step": 2124
    },
    {
      "epoch": 2.043269230769231,
      "grad_norm": 10.625438690185547,
      "learning_rate": 1.5915384615384618e-05,
      "loss": 0.8372,
      "step": 2125
    },
    {
      "epoch": 2.044230769230769,
      "grad_norm": 6.183873176574707,
      "learning_rate": 1.5913461538461537e-05,
      "loss": 0.2038,
      "step": 2126
    },
    {
      "epoch": 2.0451923076923078,
      "grad_norm": 5.363213539123535,
      "learning_rate": 1.5911538461538464e-05,
      "loss": 0.8471,
      "step": 2127
    },
    {
      "epoch": 2.046153846153846,
      "grad_norm": 5.114327907562256,
      "learning_rate": 1.5909615384615386e-05,
      "loss": 0.9273,
      "step": 2128
    },
    {
      "epoch": 2.0471153846153847,
      "grad_norm": 6.0357561111450195,
      "learning_rate": 1.590769230769231e-05,
      "loss": 1.3084,
      "step": 2129
    },
    {
      "epoch": 2.048076923076923,
      "grad_norm": 6.560948848724365,
      "learning_rate": 1.5905769230769232e-05,
      "loss": 1.4924,
      "step": 2130
    },
    {
      "epoch": 2.0490384615384616,
      "grad_norm": 17.287887573242188,
      "learning_rate": 1.5903846153846155e-05,
      "loss": 1.3099,
      "step": 2131
    },
    {
      "epoch": 2.05,
      "grad_norm": 8.00766658782959,
      "learning_rate": 1.5901923076923077e-05,
      "loss": 1.3204,
      "step": 2132
    },
    {
      "epoch": 2.0509615384615385,
      "grad_norm": 7.433656692504883,
      "learning_rate": 1.5900000000000004e-05,
      "loss": 1.9323,
      "step": 2133
    },
    {
      "epoch": 2.0519230769230767,
      "grad_norm": 6.615509033203125,
      "learning_rate": 1.5898076923076923e-05,
      "loss": 1.5785,
      "step": 2134
    },
    {
      "epoch": 2.0528846153846154,
      "grad_norm": 6.014193058013916,
      "learning_rate": 1.589615384615385e-05,
      "loss": 0.1501,
      "step": 2135
    },
    {
      "epoch": 2.0538461538461537,
      "grad_norm": 6.661532878875732,
      "learning_rate": 1.589423076923077e-05,
      "loss": 0.5328,
      "step": 2136
    },
    {
      "epoch": 2.0548076923076923,
      "grad_norm": 8.452876091003418,
      "learning_rate": 1.5892307692307695e-05,
      "loss": 1.0176,
      "step": 2137
    },
    {
      "epoch": 2.0557692307692306,
      "grad_norm": 5.249610424041748,
      "learning_rate": 1.5890384615384617e-05,
      "loss": 1.4087,
      "step": 2138
    },
    {
      "epoch": 2.0567307692307693,
      "grad_norm": 6.149997711181641,
      "learning_rate": 1.588846153846154e-05,
      "loss": 0.6227,
      "step": 2139
    },
    {
      "epoch": 2.0576923076923075,
      "grad_norm": 3.7699484825134277,
      "learning_rate": 1.5886538461538463e-05,
      "loss": 0.0804,
      "step": 2140
    },
    {
      "epoch": 2.058653846153846,
      "grad_norm": 5.806366920471191,
      "learning_rate": 1.5884615384615386e-05,
      "loss": 0.7478,
      "step": 2141
    },
    {
      "epoch": 2.0596153846153844,
      "grad_norm": 6.644627094268799,
      "learning_rate": 1.588269230769231e-05,
      "loss": 1.8797,
      "step": 2142
    },
    {
      "epoch": 2.060576923076923,
      "grad_norm": 7.5386834144592285,
      "learning_rate": 1.588076923076923e-05,
      "loss": 1.2577,
      "step": 2143
    },
    {
      "epoch": 2.0615384615384613,
      "grad_norm": 13.978562355041504,
      "learning_rate": 1.5878846153846154e-05,
      "loss": 0.9061,
      "step": 2144
    },
    {
      "epoch": 2.0625,
      "grad_norm": 9.612263679504395,
      "learning_rate": 1.587692307692308e-05,
      "loss": 0.8588,
      "step": 2145
    },
    {
      "epoch": 2.0634615384615387,
      "grad_norm": 6.565391540527344,
      "learning_rate": 1.5875e-05,
      "loss": 1.1179,
      "step": 2146
    },
    {
      "epoch": 2.064423076923077,
      "grad_norm": 8.311747550964355,
      "learning_rate": 1.5873076923076926e-05,
      "loss": 1.0444,
      "step": 2147
    },
    {
      "epoch": 2.0653846153846156,
      "grad_norm": 7.266361713409424,
      "learning_rate": 1.587115384615385e-05,
      "loss": 1.7559,
      "step": 2148
    },
    {
      "epoch": 2.066346153846154,
      "grad_norm": 6.351730823516846,
      "learning_rate": 1.586923076923077e-05,
      "loss": 0.8648,
      "step": 2149
    },
    {
      "epoch": 2.0673076923076925,
      "grad_norm": 8.071480751037598,
      "learning_rate": 1.5867307692307694e-05,
      "loss": 1.0182,
      "step": 2150
    },
    {
      "epoch": 2.0682692307692307,
      "grad_norm": 6.84756326675415,
      "learning_rate": 1.5865384615384617e-05,
      "loss": 1.3313,
      "step": 2151
    },
    {
      "epoch": 2.0692307692307694,
      "grad_norm": 2.473414421081543,
      "learning_rate": 1.586346153846154e-05,
      "loss": 0.0252,
      "step": 2152
    },
    {
      "epoch": 2.0701923076923077,
      "grad_norm": 6.25766658782959,
      "learning_rate": 1.5861538461538462e-05,
      "loss": 1.1635,
      "step": 2153
    },
    {
      "epoch": 2.0711538461538463,
      "grad_norm": 11.568316459655762,
      "learning_rate": 1.5859615384615385e-05,
      "loss": 0.7672,
      "step": 2154
    },
    {
      "epoch": 2.0721153846153846,
      "grad_norm": 6.276719093322754,
      "learning_rate": 1.5857692307692308e-05,
      "loss": 1.0548,
      "step": 2155
    },
    {
      "epoch": 2.0730769230769233,
      "grad_norm": 5.442720413208008,
      "learning_rate": 1.585576923076923e-05,
      "loss": 1.2318,
      "step": 2156
    },
    {
      "epoch": 2.0740384615384615,
      "grad_norm": 6.330758094787598,
      "learning_rate": 1.5853846153846157e-05,
      "loss": 0.5884,
      "step": 2157
    },
    {
      "epoch": 2.075,
      "grad_norm": 9.693588256835938,
      "learning_rate": 1.585192307692308e-05,
      "loss": 0.5768,
      "step": 2158
    },
    {
      "epoch": 2.0759615384615384,
      "grad_norm": 6.840682506561279,
      "learning_rate": 1.5850000000000002e-05,
      "loss": 0.6489,
      "step": 2159
    },
    {
      "epoch": 2.076923076923077,
      "grad_norm": 8.934017181396484,
      "learning_rate": 1.5848076923076925e-05,
      "loss": 1.195,
      "step": 2160
    },
    {
      "epoch": 2.0778846153846153,
      "grad_norm": 8.76657485961914,
      "learning_rate": 1.5846153846153848e-05,
      "loss": 0.3427,
      "step": 2161
    },
    {
      "epoch": 2.078846153846154,
      "grad_norm": 8.337162017822266,
      "learning_rate": 1.584423076923077e-05,
      "loss": 1.0306,
      "step": 2162
    },
    {
      "epoch": 2.0798076923076922,
      "grad_norm": 5.3006367683410645,
      "learning_rate": 1.5842307692307693e-05,
      "loss": 1.136,
      "step": 2163
    },
    {
      "epoch": 2.080769230769231,
      "grad_norm": 5.947980880737305,
      "learning_rate": 1.5840384615384616e-05,
      "loss": 1.4126,
      "step": 2164
    },
    {
      "epoch": 2.081730769230769,
      "grad_norm": 8.671198844909668,
      "learning_rate": 1.583846153846154e-05,
      "loss": 1.0896,
      "step": 2165
    },
    {
      "epoch": 2.082692307692308,
      "grad_norm": 6.873534679412842,
      "learning_rate": 1.5836538461538462e-05,
      "loss": 0.6234,
      "step": 2166
    },
    {
      "epoch": 2.083653846153846,
      "grad_norm": 12.047667503356934,
      "learning_rate": 1.5834615384615385e-05,
      "loss": 0.1963,
      "step": 2167
    },
    {
      "epoch": 2.0846153846153848,
      "grad_norm": 7.369927883148193,
      "learning_rate": 1.583269230769231e-05,
      "loss": 1.3024,
      "step": 2168
    },
    {
      "epoch": 2.085576923076923,
      "grad_norm": 5.978841304779053,
      "learning_rate": 1.583076923076923e-05,
      "loss": 1.3231,
      "step": 2169
    },
    {
      "epoch": 2.0865384615384617,
      "grad_norm": 10.023139953613281,
      "learning_rate": 1.5828846153846156e-05,
      "loss": 1.5131,
      "step": 2170
    },
    {
      "epoch": 2.0875,
      "grad_norm": 7.535898208618164,
      "learning_rate": 1.582692307692308e-05,
      "loss": 0.8104,
      "step": 2171
    },
    {
      "epoch": 2.0884615384615386,
      "grad_norm": 5.466134071350098,
      "learning_rate": 1.5825000000000002e-05,
      "loss": 0.6182,
      "step": 2172
    },
    {
      "epoch": 2.089423076923077,
      "grad_norm": 7.328108787536621,
      "learning_rate": 1.5823076923076925e-05,
      "loss": 0.5293,
      "step": 2173
    },
    {
      "epoch": 2.0903846153846155,
      "grad_norm": 5.4853973388671875,
      "learning_rate": 1.5821153846153847e-05,
      "loss": 1.6391,
      "step": 2174
    },
    {
      "epoch": 2.0913461538461537,
      "grad_norm": 6.32663106918335,
      "learning_rate": 1.581923076923077e-05,
      "loss": 1.0577,
      "step": 2175
    },
    {
      "epoch": 2.0923076923076924,
      "grad_norm": 6.853585243225098,
      "learning_rate": 1.5817307692307693e-05,
      "loss": 0.4676,
      "step": 2176
    },
    {
      "epoch": 2.0932692307692307,
      "grad_norm": 9.58525562286377,
      "learning_rate": 1.5815384615384616e-05,
      "loss": 1.7538,
      "step": 2177
    },
    {
      "epoch": 2.0942307692307693,
      "grad_norm": 9.938372611999512,
      "learning_rate": 1.5813461538461542e-05,
      "loss": 1.1726,
      "step": 2178
    },
    {
      "epoch": 2.0951923076923076,
      "grad_norm": 9.890833854675293,
      "learning_rate": 1.581153846153846e-05,
      "loss": 1.1079,
      "step": 2179
    },
    {
      "epoch": 2.0961538461538463,
      "grad_norm": 11.360554695129395,
      "learning_rate": 1.5809615384615387e-05,
      "loss": 0.4255,
      "step": 2180
    },
    {
      "epoch": 2.0971153846153845,
      "grad_norm": 7.527711868286133,
      "learning_rate": 1.5807692307692307e-05,
      "loss": 1.3402,
      "step": 2181
    },
    {
      "epoch": 2.098076923076923,
      "grad_norm": 7.933333396911621,
      "learning_rate": 1.5805769230769233e-05,
      "loss": 0.7962,
      "step": 2182
    },
    {
      "epoch": 2.0990384615384614,
      "grad_norm": 7.434067726135254,
      "learning_rate": 1.5803846153846156e-05,
      "loss": 0.9743,
      "step": 2183
    },
    {
      "epoch": 2.1,
      "grad_norm": 7.183732986450195,
      "learning_rate": 1.580192307692308e-05,
      "loss": 1.1652,
      "step": 2184
    },
    {
      "epoch": 2.1009615384615383,
      "grad_norm": 5.784126281738281,
      "learning_rate": 1.58e-05,
      "loss": 1.1458,
      "step": 2185
    },
    {
      "epoch": 2.101923076923077,
      "grad_norm": 5.740279674530029,
      "learning_rate": 1.5798076923076924e-05,
      "loss": 1.377,
      "step": 2186
    },
    {
      "epoch": 2.1028846153846152,
      "grad_norm": 7.73600435256958,
      "learning_rate": 1.5796153846153847e-05,
      "loss": 1.8336,
      "step": 2187
    },
    {
      "epoch": 2.103846153846154,
      "grad_norm": 1.4411906003952026,
      "learning_rate": 1.579423076923077e-05,
      "loss": 0.017,
      "step": 2188
    },
    {
      "epoch": 2.104807692307692,
      "grad_norm": 10.146678924560547,
      "learning_rate": 1.5792307692307692e-05,
      "loss": 1.604,
      "step": 2189
    },
    {
      "epoch": 2.105769230769231,
      "grad_norm": 5.467104434967041,
      "learning_rate": 1.579038461538462e-05,
      "loss": 1.0696,
      "step": 2190
    },
    {
      "epoch": 2.106730769230769,
      "grad_norm": 7.855000019073486,
      "learning_rate": 1.5788461538461538e-05,
      "loss": 1.0001,
      "step": 2191
    },
    {
      "epoch": 2.1076923076923078,
      "grad_norm": 7.750090599060059,
      "learning_rate": 1.5786538461538464e-05,
      "loss": 1.5427,
      "step": 2192
    },
    {
      "epoch": 2.108653846153846,
      "grad_norm": 6.5304951667785645,
      "learning_rate": 1.5784615384615383e-05,
      "loss": 0.796,
      "step": 2193
    },
    {
      "epoch": 2.1096153846153847,
      "grad_norm": 9.660745620727539,
      "learning_rate": 1.578269230769231e-05,
      "loss": 1.4312,
      "step": 2194
    },
    {
      "epoch": 2.110576923076923,
      "grad_norm": 8.935149192810059,
      "learning_rate": 1.5780769230769232e-05,
      "loss": 1.2563,
      "step": 2195
    },
    {
      "epoch": 2.1115384615384616,
      "grad_norm": 7.0415568351745605,
      "learning_rate": 1.5778846153846155e-05,
      "loss": 0.0224,
      "step": 2196
    },
    {
      "epoch": 2.1125,
      "grad_norm": 6.964410781860352,
      "learning_rate": 1.5776923076923078e-05,
      "loss": 1.0713,
      "step": 2197
    },
    {
      "epoch": 2.1134615384615385,
      "grad_norm": 9.035327911376953,
      "learning_rate": 1.5775e-05,
      "loss": 0.9433,
      "step": 2198
    },
    {
      "epoch": 2.1144230769230767,
      "grad_norm": 19.216093063354492,
      "learning_rate": 1.5773076923076923e-05,
      "loss": 1.3907,
      "step": 2199
    },
    {
      "epoch": 2.1153846153846154,
      "grad_norm": 9.918272018432617,
      "learning_rate": 1.577115384615385e-05,
      "loss": 0.6492,
      "step": 2200
    },
    {
      "epoch": 2.1163461538461537,
      "grad_norm": 9.16617488861084,
      "learning_rate": 1.576923076923077e-05,
      "loss": 0.9222,
      "step": 2201
    },
    {
      "epoch": 2.1173076923076923,
      "grad_norm": 5.677835941314697,
      "learning_rate": 1.5767307692307695e-05,
      "loss": 1.1588,
      "step": 2202
    },
    {
      "epoch": 2.1182692307692306,
      "grad_norm": 5.820681571960449,
      "learning_rate": 1.5765384615384614e-05,
      "loss": 0.0353,
      "step": 2203
    },
    {
      "epoch": 2.1192307692307693,
      "grad_norm": 11.580347061157227,
      "learning_rate": 1.576346153846154e-05,
      "loss": 1.619,
      "step": 2204
    },
    {
      "epoch": 2.1201923076923075,
      "grad_norm": 5.941991806030273,
      "learning_rate": 1.5761538461538463e-05,
      "loss": 0.6138,
      "step": 2205
    },
    {
      "epoch": 2.121153846153846,
      "grad_norm": 5.58657169342041,
      "learning_rate": 1.5759615384615386e-05,
      "loss": 0.0754,
      "step": 2206
    },
    {
      "epoch": 2.1221153846153844,
      "grad_norm": 7.638330459594727,
      "learning_rate": 1.575769230769231e-05,
      "loss": 0.9057,
      "step": 2207
    },
    {
      "epoch": 2.123076923076923,
      "grad_norm": 10.633749008178711,
      "learning_rate": 1.575576923076923e-05,
      "loss": 1.0009,
      "step": 2208
    },
    {
      "epoch": 2.1240384615384613,
      "grad_norm": 7.945683479309082,
      "learning_rate": 1.5753846153846154e-05,
      "loss": 0.819,
      "step": 2209
    },
    {
      "epoch": 2.125,
      "grad_norm": 6.667728424072266,
      "learning_rate": 1.575192307692308e-05,
      "loss": 0.3965,
      "step": 2210
    },
    {
      "epoch": 2.1259615384615387,
      "grad_norm": 6.41085147857666,
      "learning_rate": 1.575e-05,
      "loss": 1.5401,
      "step": 2211
    },
    {
      "epoch": 2.126923076923077,
      "grad_norm": 8.253244400024414,
      "learning_rate": 1.5748076923076926e-05,
      "loss": 1.3316,
      "step": 2212
    },
    {
      "epoch": 2.127884615384615,
      "grad_norm": 5.873397350311279,
      "learning_rate": 1.5746153846153846e-05,
      "loss": 0.5831,
      "step": 2213
    },
    {
      "epoch": 2.128846153846154,
      "grad_norm": 7.413168907165527,
      "learning_rate": 1.574423076923077e-05,
      "loss": 1.1371,
      "step": 2214
    },
    {
      "epoch": 2.1298076923076925,
      "grad_norm": 9.874505043029785,
      "learning_rate": 1.5742307692307694e-05,
      "loss": 1.249,
      "step": 2215
    },
    {
      "epoch": 2.1307692307692307,
      "grad_norm": 8.640460968017578,
      "learning_rate": 1.5740384615384617e-05,
      "loss": 0.6812,
      "step": 2216
    },
    {
      "epoch": 2.1317307692307694,
      "grad_norm": 6.859170436859131,
      "learning_rate": 1.573846153846154e-05,
      "loss": 1.2808,
      "step": 2217
    },
    {
      "epoch": 2.1326923076923077,
      "grad_norm": 6.856330394744873,
      "learning_rate": 1.5736538461538463e-05,
      "loss": 0.8244,
      "step": 2218
    },
    {
      "epoch": 2.1336538461538463,
      "grad_norm": 7.582540988922119,
      "learning_rate": 1.5734615384615386e-05,
      "loss": 1.281,
      "step": 2219
    },
    {
      "epoch": 2.1346153846153846,
      "grad_norm": 5.720359802246094,
      "learning_rate": 1.5732692307692308e-05,
      "loss": 1.0864,
      "step": 2220
    },
    {
      "epoch": 2.1355769230769233,
      "grad_norm": 7.155442714691162,
      "learning_rate": 1.573076923076923e-05,
      "loss": 1.3732,
      "step": 2221
    },
    {
      "epoch": 2.1365384615384615,
      "grad_norm": 6.729788780212402,
      "learning_rate": 1.5728846153846157e-05,
      "loss": 1.7694,
      "step": 2222
    },
    {
      "epoch": 2.1375,
      "grad_norm": 11.504335403442383,
      "learning_rate": 1.5726923076923077e-05,
      "loss": 0.6558,
      "step": 2223
    },
    {
      "epoch": 2.1384615384615384,
      "grad_norm": 13.691414833068848,
      "learning_rate": 1.5725000000000003e-05,
      "loss": 1.2212,
      "step": 2224
    },
    {
      "epoch": 2.139423076923077,
      "grad_norm": 7.152390480041504,
      "learning_rate": 1.5723076923076926e-05,
      "loss": 1.3653,
      "step": 2225
    },
    {
      "epoch": 2.1403846153846153,
      "grad_norm": 7.4188385009765625,
      "learning_rate": 1.5721153846153848e-05,
      "loss": 1.2042,
      "step": 2226
    },
    {
      "epoch": 2.141346153846154,
      "grad_norm": 3.731968879699707,
      "learning_rate": 1.571923076923077e-05,
      "loss": 0.0191,
      "step": 2227
    },
    {
      "epoch": 2.1423076923076922,
      "grad_norm": 4.894285202026367,
      "learning_rate": 1.5717307692307694e-05,
      "loss": 0.859,
      "step": 2228
    },
    {
      "epoch": 2.143269230769231,
      "grad_norm": 6.1403374671936035,
      "learning_rate": 1.5715384615384617e-05,
      "loss": 1.6982,
      "step": 2229
    },
    {
      "epoch": 2.144230769230769,
      "grad_norm": 6.130209922790527,
      "learning_rate": 1.571346153846154e-05,
      "loss": 1.4746,
      "step": 2230
    },
    {
      "epoch": 2.145192307692308,
      "grad_norm": 7.864573955535889,
      "learning_rate": 1.5711538461538462e-05,
      "loss": 1.187,
      "step": 2231
    },
    {
      "epoch": 2.146153846153846,
      "grad_norm": 6.1424031257629395,
      "learning_rate": 1.5709615384615385e-05,
      "loss": 1.2518,
      "step": 2232
    },
    {
      "epoch": 2.1471153846153848,
      "grad_norm": 9.785414695739746,
      "learning_rate": 1.5707692307692308e-05,
      "loss": 1.1935,
      "step": 2233
    },
    {
      "epoch": 2.148076923076923,
      "grad_norm": 6.2355875968933105,
      "learning_rate": 1.5705769230769234e-05,
      "loss": 1.1219,
      "step": 2234
    },
    {
      "epoch": 2.1490384615384617,
      "grad_norm": 6.930116653442383,
      "learning_rate": 1.5703846153846157e-05,
      "loss": 1.4726,
      "step": 2235
    },
    {
      "epoch": 2.15,
      "grad_norm": 8.45541763305664,
      "learning_rate": 1.570192307692308e-05,
      "loss": 0.8037,
      "step": 2236
    },
    {
      "epoch": 2.1509615384615386,
      "grad_norm": 11.779139518737793,
      "learning_rate": 1.5700000000000002e-05,
      "loss": 0.1521,
      "step": 2237
    },
    {
      "epoch": 2.151923076923077,
      "grad_norm": 11.5225830078125,
      "learning_rate": 1.5698076923076925e-05,
      "loss": 1.1101,
      "step": 2238
    },
    {
      "epoch": 2.1528846153846155,
      "grad_norm": 7.419250965118408,
      "learning_rate": 1.5696153846153848e-05,
      "loss": 0.9616,
      "step": 2239
    },
    {
      "epoch": 2.1538461538461537,
      "grad_norm": 5.421406269073486,
      "learning_rate": 1.569423076923077e-05,
      "loss": 0.8907,
      "step": 2240
    },
    {
      "epoch": 2.1548076923076924,
      "grad_norm": 7.08132791519165,
      "learning_rate": 1.5692307692307693e-05,
      "loss": 0.9746,
      "step": 2241
    },
    {
      "epoch": 2.1557692307692307,
      "grad_norm": 7.919125080108643,
      "learning_rate": 1.5690384615384616e-05,
      "loss": 0.9166,
      "step": 2242
    },
    {
      "epoch": 2.1567307692307693,
      "grad_norm": 7.7336907386779785,
      "learning_rate": 1.568846153846154e-05,
      "loss": 1.6235,
      "step": 2243
    },
    {
      "epoch": 2.1576923076923076,
      "grad_norm": 10.22049331665039,
      "learning_rate": 1.568653846153846e-05,
      "loss": 0.142,
      "step": 2244
    },
    {
      "epoch": 2.1586538461538463,
      "grad_norm": 6.091569900512695,
      "learning_rate": 1.5684615384615384e-05,
      "loss": 0.6089,
      "step": 2245
    },
    {
      "epoch": 2.1596153846153845,
      "grad_norm": 9.226280212402344,
      "learning_rate": 1.568269230769231e-05,
      "loss": 1.0272,
      "step": 2246
    },
    {
      "epoch": 2.160576923076923,
      "grad_norm": 7.457513809204102,
      "learning_rate": 1.5680769230769233e-05,
      "loss": 1.6272,
      "step": 2247
    },
    {
      "epoch": 2.1615384615384614,
      "grad_norm": 7.313079357147217,
      "learning_rate": 1.5678846153846156e-05,
      "loss": 0.9886,
      "step": 2248
    },
    {
      "epoch": 2.1625,
      "grad_norm": 5.846545696258545,
      "learning_rate": 1.567692307692308e-05,
      "loss": 1.5627,
      "step": 2249
    },
    {
      "epoch": 2.1634615384615383,
      "grad_norm": 13.212281227111816,
      "learning_rate": 1.5675e-05,
      "loss": 1.0244,
      "step": 2250
    },
    {
      "epoch": 2.164423076923077,
      "grad_norm": 6.928263187408447,
      "learning_rate": 1.5673076923076924e-05,
      "loss": 1.1013,
      "step": 2251
    },
    {
      "epoch": 2.1653846153846152,
      "grad_norm": 6.87102746963501,
      "learning_rate": 1.5671153846153847e-05,
      "loss": 1.0628,
      "step": 2252
    },
    {
      "epoch": 2.166346153846154,
      "grad_norm": 7.386466026306152,
      "learning_rate": 1.566923076923077e-05,
      "loss": 0.1111,
      "step": 2253
    },
    {
      "epoch": 2.167307692307692,
      "grad_norm": 0.5142215490341187,
      "learning_rate": 1.5667307692307693e-05,
      "loss": 0.0038,
      "step": 2254
    },
    {
      "epoch": 2.168269230769231,
      "grad_norm": 7.401956081390381,
      "learning_rate": 1.5665384615384615e-05,
      "loss": 0.8254,
      "step": 2255
    },
    {
      "epoch": 2.169230769230769,
      "grad_norm": 7.371987342834473,
      "learning_rate": 1.5663461538461538e-05,
      "loss": 1.0502,
      "step": 2256
    },
    {
      "epoch": 2.1701923076923078,
      "grad_norm": 11.882657051086426,
      "learning_rate": 1.5661538461538464e-05,
      "loss": 1.1218,
      "step": 2257
    },
    {
      "epoch": 2.171153846153846,
      "grad_norm": 7.029792785644531,
      "learning_rate": 1.5659615384615384e-05,
      "loss": 1.4244,
      "step": 2258
    },
    {
      "epoch": 2.1721153846153847,
      "grad_norm": 4.485072135925293,
      "learning_rate": 1.565769230769231e-05,
      "loss": 0.0148,
      "step": 2259
    },
    {
      "epoch": 2.173076923076923,
      "grad_norm": 6.626597881317139,
      "learning_rate": 1.5655769230769233e-05,
      "loss": 0.5528,
      "step": 2260
    },
    {
      "epoch": 2.1740384615384616,
      "grad_norm": 7.721027851104736,
      "learning_rate": 1.5653846153846155e-05,
      "loss": 1.4024,
      "step": 2261
    },
    {
      "epoch": 2.175,
      "grad_norm": 8.344649314880371,
      "learning_rate": 1.5651923076923078e-05,
      "loss": 0.878,
      "step": 2262
    },
    {
      "epoch": 2.1759615384615385,
      "grad_norm": 12.092805862426758,
      "learning_rate": 1.565e-05,
      "loss": 1.6944,
      "step": 2263
    },
    {
      "epoch": 2.1769230769230767,
      "grad_norm": 9.590635299682617,
      "learning_rate": 1.5648076923076924e-05,
      "loss": 0.9761,
      "step": 2264
    },
    {
      "epoch": 2.1778846153846154,
      "grad_norm": 6.412044048309326,
      "learning_rate": 1.5646153846153846e-05,
      "loss": 0.6,
      "step": 2265
    },
    {
      "epoch": 2.1788461538461537,
      "grad_norm": 8.560193061828613,
      "learning_rate": 1.564423076923077e-05,
      "loss": 1.2975,
      "step": 2266
    },
    {
      "epoch": 2.1798076923076923,
      "grad_norm": 11.249995231628418,
      "learning_rate": 1.5642307692307695e-05,
      "loss": 2.1499,
      "step": 2267
    },
    {
      "epoch": 2.1807692307692306,
      "grad_norm": 7.945410251617432,
      "learning_rate": 1.5640384615384615e-05,
      "loss": 1.4185,
      "step": 2268
    },
    {
      "epoch": 2.1817307692307693,
      "grad_norm": 10.81639575958252,
      "learning_rate": 1.563846153846154e-05,
      "loss": 1.0725,
      "step": 2269
    },
    {
      "epoch": 2.1826923076923075,
      "grad_norm": 2.525151014328003,
      "learning_rate": 1.563653846153846e-05,
      "loss": 0.0193,
      "step": 2270
    },
    {
      "epoch": 2.183653846153846,
      "grad_norm": 6.06218957901001,
      "learning_rate": 1.5634615384615386e-05,
      "loss": 0.3044,
      "step": 2271
    },
    {
      "epoch": 2.184615384615385,
      "grad_norm": 6.943436145782471,
      "learning_rate": 1.563269230769231e-05,
      "loss": 1.353,
      "step": 2272
    },
    {
      "epoch": 2.185576923076923,
      "grad_norm": 9.865239143371582,
      "learning_rate": 1.5630769230769232e-05,
      "loss": 1.2044,
      "step": 2273
    },
    {
      "epoch": 2.1865384615384613,
      "grad_norm": 9.398489952087402,
      "learning_rate": 1.5628846153846155e-05,
      "loss": 1.0974,
      "step": 2274
    },
    {
      "epoch": 2.1875,
      "grad_norm": 9.505410194396973,
      "learning_rate": 1.5626923076923078e-05,
      "loss": 1.5048,
      "step": 2275
    },
    {
      "epoch": 2.1884615384615387,
      "grad_norm": 7.291731834411621,
      "learning_rate": 1.5625e-05,
      "loss": 1.0716,
      "step": 2276
    },
    {
      "epoch": 2.189423076923077,
      "grad_norm": 6.451929092407227,
      "learning_rate": 1.5623076923076926e-05,
      "loss": 1.1296,
      "step": 2277
    },
    {
      "epoch": 2.190384615384615,
      "grad_norm": 9.376962661743164,
      "learning_rate": 1.5621153846153846e-05,
      "loss": 1.1045,
      "step": 2278
    },
    {
      "epoch": 2.191346153846154,
      "grad_norm": 12.387228965759277,
      "learning_rate": 1.5619230769230772e-05,
      "loss": 1.981,
      "step": 2279
    },
    {
      "epoch": 2.1923076923076925,
      "grad_norm": 6.993914604187012,
      "learning_rate": 1.561730769230769e-05,
      "loss": 1.4949,
      "step": 2280
    },
    {
      "epoch": 2.1932692307692307,
      "grad_norm": 6.749290943145752,
      "learning_rate": 1.5615384615384618e-05,
      "loss": 0.8045,
      "step": 2281
    },
    {
      "epoch": 2.1942307692307694,
      "grad_norm": 9.317429542541504,
      "learning_rate": 1.561346153846154e-05,
      "loss": 0.8487,
      "step": 2282
    },
    {
      "epoch": 2.1951923076923077,
      "grad_norm": 8.194247245788574,
      "learning_rate": 1.5611538461538463e-05,
      "loss": 1.8228,
      "step": 2283
    },
    {
      "epoch": 2.1961538461538463,
      "grad_norm": 10.121991157531738,
      "learning_rate": 1.5609615384615386e-05,
      "loss": 1.1145,
      "step": 2284
    },
    {
      "epoch": 2.1971153846153846,
      "grad_norm": 5.64957857131958,
      "learning_rate": 1.560769230769231e-05,
      "loss": 0.4056,
      "step": 2285
    },
    {
      "epoch": 2.1980769230769233,
      "grad_norm": 9.47693920135498,
      "learning_rate": 1.560576923076923e-05,
      "loss": 0.4754,
      "step": 2286
    },
    {
      "epoch": 2.1990384615384615,
      "grad_norm": 8.381746292114258,
      "learning_rate": 1.5603846153846158e-05,
      "loss": 1.7106,
      "step": 2287
    },
    {
      "epoch": 2.2,
      "grad_norm": 5.518743515014648,
      "learning_rate": 1.5601923076923077e-05,
      "loss": 0.6643,
      "step": 2288
    },
    {
      "epoch": 2.2009615384615384,
      "grad_norm": 6.422544956207275,
      "learning_rate": 1.5600000000000003e-05,
      "loss": 1.6287,
      "step": 2289
    },
    {
      "epoch": 2.201923076923077,
      "grad_norm": 1.2693874835968018,
      "learning_rate": 1.5598076923076922e-05,
      "loss": 0.0117,
      "step": 2290
    },
    {
      "epoch": 2.2028846153846153,
      "grad_norm": 6.101798057556152,
      "learning_rate": 1.559615384615385e-05,
      "loss": 0.6055,
      "step": 2291
    },
    {
      "epoch": 2.203846153846154,
      "grad_norm": 8.77856731414795,
      "learning_rate": 1.559423076923077e-05,
      "loss": 1.4031,
      "step": 2292
    },
    {
      "epoch": 2.2048076923076922,
      "grad_norm": 6.292959213256836,
      "learning_rate": 1.5592307692307694e-05,
      "loss": 1.0888,
      "step": 2293
    },
    {
      "epoch": 2.205769230769231,
      "grad_norm": 27.475975036621094,
      "learning_rate": 1.5590384615384617e-05,
      "loss": 1.2493,
      "step": 2294
    },
    {
      "epoch": 2.206730769230769,
      "grad_norm": 8.806879043579102,
      "learning_rate": 1.558846153846154e-05,
      "loss": 1.1594,
      "step": 2295
    },
    {
      "epoch": 2.207692307692308,
      "grad_norm": 6.241188049316406,
      "learning_rate": 1.5586538461538462e-05,
      "loss": 1.5884,
      "step": 2296
    },
    {
      "epoch": 2.208653846153846,
      "grad_norm": 6.2615790367126465,
      "learning_rate": 1.5584615384615385e-05,
      "loss": 1.088,
      "step": 2297
    },
    {
      "epoch": 2.2096153846153848,
      "grad_norm": 7.24094295501709,
      "learning_rate": 1.5582692307692308e-05,
      "loss": 0.9433,
      "step": 2298
    },
    {
      "epoch": 2.210576923076923,
      "grad_norm": 7.626286029815674,
      "learning_rate": 1.5580769230769234e-05,
      "loss": 1.3177,
      "step": 2299
    },
    {
      "epoch": 2.2115384615384617,
      "grad_norm": 8.335433006286621,
      "learning_rate": 1.5578846153846154e-05,
      "loss": 1.5145,
      "step": 2300
    },
    {
      "epoch": 2.2125,
      "grad_norm": 3.009903907775879,
      "learning_rate": 1.557692307692308e-05,
      "loss": 0.036,
      "step": 2301
    },
    {
      "epoch": 2.2134615384615386,
      "grad_norm": 6.378203392028809,
      "learning_rate": 1.5575000000000002e-05,
      "loss": 1.2332,
      "step": 2302
    },
    {
      "epoch": 2.214423076923077,
      "grad_norm": 6.693501949310303,
      "learning_rate": 1.5573076923076925e-05,
      "loss": 1.1662,
      "step": 2303
    },
    {
      "epoch": 2.2153846153846155,
      "grad_norm": 5.530068397521973,
      "learning_rate": 1.5571153846153848e-05,
      "loss": 2.0218,
      "step": 2304
    },
    {
      "epoch": 2.2163461538461537,
      "grad_norm": 7.045986175537109,
      "learning_rate": 1.556923076923077e-05,
      "loss": 0.7971,
      "step": 2305
    },
    {
      "epoch": 2.2173076923076924,
      "grad_norm": 7.2190141677856445,
      "learning_rate": 1.5567307692307694e-05,
      "loss": 1.1919,
      "step": 2306
    },
    {
      "epoch": 2.2182692307692307,
      "grad_norm": 4.702575206756592,
      "learning_rate": 1.5565384615384616e-05,
      "loss": 0.7355,
      "step": 2307
    },
    {
      "epoch": 2.2192307692307693,
      "grad_norm": 8.614277839660645,
      "learning_rate": 1.556346153846154e-05,
      "loss": 1.1302,
      "step": 2308
    },
    {
      "epoch": 2.2201923076923076,
      "grad_norm": 7.096172332763672,
      "learning_rate": 1.5561538461538462e-05,
      "loss": 0.5986,
      "step": 2309
    },
    {
      "epoch": 2.2211538461538463,
      "grad_norm": 4.966085433959961,
      "learning_rate": 1.5559615384615385e-05,
      "loss": 1.3697,
      "step": 2310
    },
    {
      "epoch": 2.2221153846153845,
      "grad_norm": 5.5272321701049805,
      "learning_rate": 1.555769230769231e-05,
      "loss": 1.6792,
      "step": 2311
    },
    {
      "epoch": 2.223076923076923,
      "grad_norm": 6.225198745727539,
      "learning_rate": 1.555576923076923e-05,
      "loss": 0.88,
      "step": 2312
    },
    {
      "epoch": 2.2240384615384614,
      "grad_norm": 5.196440696716309,
      "learning_rate": 1.5553846153846156e-05,
      "loss": 0.2084,
      "step": 2313
    },
    {
      "epoch": 2.225,
      "grad_norm": 8.43975830078125,
      "learning_rate": 1.555192307692308e-05,
      "loss": 1.3415,
      "step": 2314
    },
    {
      "epoch": 2.2259615384615383,
      "grad_norm": 7.60177755355835,
      "learning_rate": 1.5550000000000002e-05,
      "loss": 1.7955,
      "step": 2315
    },
    {
      "epoch": 2.226923076923077,
      "grad_norm": 8.162069320678711,
      "learning_rate": 1.5548076923076925e-05,
      "loss": 0.9899,
      "step": 2316
    },
    {
      "epoch": 2.2278846153846152,
      "grad_norm": 8.552821159362793,
      "learning_rate": 1.5546153846153847e-05,
      "loss": 0.8561,
      "step": 2317
    },
    {
      "epoch": 2.228846153846154,
      "grad_norm": 5.8412184715271,
      "learning_rate": 1.554423076923077e-05,
      "loss": 1.0893,
      "step": 2318
    },
    {
      "epoch": 2.229807692307692,
      "grad_norm": 7.9684882164001465,
      "learning_rate": 1.5542307692307693e-05,
      "loss": 0.3135,
      "step": 2319
    },
    {
      "epoch": 2.230769230769231,
      "grad_norm": 7.969353199005127,
      "learning_rate": 1.5540384615384616e-05,
      "loss": 0.9594,
      "step": 2320
    },
    {
      "epoch": 2.231730769230769,
      "grad_norm": 17.325986862182617,
      "learning_rate": 1.553846153846154e-05,
      "loss": 0.9813,
      "step": 2321
    },
    {
      "epoch": 2.2326923076923078,
      "grad_norm": 9.548873901367188,
      "learning_rate": 1.553653846153846e-05,
      "loss": 0.8611,
      "step": 2322
    },
    {
      "epoch": 2.233653846153846,
      "grad_norm": 6.030880451202393,
      "learning_rate": 1.5534615384615387e-05,
      "loss": 0.5256,
      "step": 2323
    },
    {
      "epoch": 2.2346153846153847,
      "grad_norm": 6.390537738800049,
      "learning_rate": 1.553269230769231e-05,
      "loss": 0.9452,
      "step": 2324
    },
    {
      "epoch": 2.235576923076923,
      "grad_norm": 7.906251907348633,
      "learning_rate": 1.5530769230769233e-05,
      "loss": 1.385,
      "step": 2325
    },
    {
      "epoch": 2.2365384615384616,
      "grad_norm": 7.065876007080078,
      "learning_rate": 1.5528846153846156e-05,
      "loss": 1.717,
      "step": 2326
    },
    {
      "epoch": 2.2375,
      "grad_norm": 6.153007984161377,
      "learning_rate": 1.552692307692308e-05,
      "loss": 1.1465,
      "step": 2327
    },
    {
      "epoch": 2.2384615384615385,
      "grad_norm": 8.582223892211914,
      "learning_rate": 1.5525e-05,
      "loss": 0.8646,
      "step": 2328
    },
    {
      "epoch": 2.2394230769230767,
      "grad_norm": 6.32701301574707,
      "learning_rate": 1.5523076923076924e-05,
      "loss": 1.7243,
      "step": 2329
    },
    {
      "epoch": 2.2403846153846154,
      "grad_norm": 6.101017951965332,
      "learning_rate": 1.5521153846153847e-05,
      "loss": 1.507,
      "step": 2330
    },
    {
      "epoch": 2.2413461538461537,
      "grad_norm": 8.031761169433594,
      "learning_rate": 1.551923076923077e-05,
      "loss": 1.2345,
      "step": 2331
    },
    {
      "epoch": 2.2423076923076923,
      "grad_norm": 13.194418907165527,
      "learning_rate": 1.5517307692307692e-05,
      "loss": 1.2906,
      "step": 2332
    },
    {
      "epoch": 2.2432692307692306,
      "grad_norm": 7.0096940994262695,
      "learning_rate": 1.5515384615384615e-05,
      "loss": 0.5903,
      "step": 2333
    },
    {
      "epoch": 2.2442307692307693,
      "grad_norm": 7.109586715698242,
      "learning_rate": 1.551346153846154e-05,
      "loss": 1.4863,
      "step": 2334
    },
    {
      "epoch": 2.2451923076923075,
      "grad_norm": 5.910264492034912,
      "learning_rate": 1.551153846153846e-05,
      "loss": 1.1478,
      "step": 2335
    },
    {
      "epoch": 2.246153846153846,
      "grad_norm": 6.166622161865234,
      "learning_rate": 1.5509615384615387e-05,
      "loss": 1.3883,
      "step": 2336
    },
    {
      "epoch": 2.247115384615385,
      "grad_norm": 11.203251838684082,
      "learning_rate": 1.550769230769231e-05,
      "loss": 0.7784,
      "step": 2337
    },
    {
      "epoch": 2.248076923076923,
      "grad_norm": 6.0568156242370605,
      "learning_rate": 1.5505769230769232e-05,
      "loss": 1.2177,
      "step": 2338
    },
    {
      "epoch": 2.2490384615384613,
      "grad_norm": 6.781397342681885,
      "learning_rate": 1.5503846153846155e-05,
      "loss": 1.2365,
      "step": 2339
    },
    {
      "epoch": 2.25,
      "grad_norm": 5.974975109100342,
      "learning_rate": 1.5501923076923078e-05,
      "loss": 1.061,
      "step": 2340
    },
    {
      "epoch": 2.2509615384615387,
      "grad_norm": 6.865684509277344,
      "learning_rate": 1.55e-05,
      "loss": 1.426,
      "step": 2341
    },
    {
      "epoch": 2.251923076923077,
      "grad_norm": 7.677322864532471,
      "learning_rate": 1.5498076923076923e-05,
      "loss": 2.0118,
      "step": 2342
    },
    {
      "epoch": 2.252884615384615,
      "grad_norm": 8.962800025939941,
      "learning_rate": 1.5496153846153846e-05,
      "loss": 1.4923,
      "step": 2343
    },
    {
      "epoch": 2.253846153846154,
      "grad_norm": 6.253341197967529,
      "learning_rate": 1.5494230769230772e-05,
      "loss": 0.9775,
      "step": 2344
    },
    {
      "epoch": 2.2548076923076925,
      "grad_norm": 7.553477764129639,
      "learning_rate": 1.5492307692307692e-05,
      "loss": 1.7868,
      "step": 2345
    },
    {
      "epoch": 2.2557692307692307,
      "grad_norm": 6.740208148956299,
      "learning_rate": 1.5490384615384618e-05,
      "loss": 1.4394,
      "step": 2346
    },
    {
      "epoch": 2.256730769230769,
      "grad_norm": 4.3894147872924805,
      "learning_rate": 1.5488461538461537e-05,
      "loss": 0.0903,
      "step": 2347
    },
    {
      "epoch": 2.2576923076923077,
      "grad_norm": 8.278862953186035,
      "learning_rate": 1.5486538461538463e-05,
      "loss": 1.0614,
      "step": 2348
    },
    {
      "epoch": 2.2586538461538463,
      "grad_norm": 7.319096088409424,
      "learning_rate": 1.5484615384615386e-05,
      "loss": 1.7004,
      "step": 2349
    },
    {
      "epoch": 2.2596153846153846,
      "grad_norm": 5.792403221130371,
      "learning_rate": 1.548269230769231e-05,
      "loss": 1.43,
      "step": 2350
    },
    {
      "epoch": 2.2605769230769233,
      "grad_norm": 9.516488075256348,
      "learning_rate": 1.5480769230769232e-05,
      "loss": 0.5584,
      "step": 2351
    },
    {
      "epoch": 2.2615384615384615,
      "grad_norm": 7.098084926605225,
      "learning_rate": 1.5478846153846155e-05,
      "loss": 1.6425,
      "step": 2352
    },
    {
      "epoch": 2.2625,
      "grad_norm": 5.840274810791016,
      "learning_rate": 1.5476923076923077e-05,
      "loss": 1.0697,
      "step": 2353
    },
    {
      "epoch": 2.2634615384615384,
      "grad_norm": 8.768056869506836,
      "learning_rate": 1.5475000000000003e-05,
      "loss": 1.5503,
      "step": 2354
    },
    {
      "epoch": 2.264423076923077,
      "grad_norm": 8.034292221069336,
      "learning_rate": 1.5473076923076923e-05,
      "loss": 0.746,
      "step": 2355
    },
    {
      "epoch": 2.2653846153846153,
      "grad_norm": 6.1055097579956055,
      "learning_rate": 1.547115384615385e-05,
      "loss": 1.4994,
      "step": 2356
    },
    {
      "epoch": 2.266346153846154,
      "grad_norm": 8.365769386291504,
      "learning_rate": 1.546923076923077e-05,
      "loss": 1.4209,
      "step": 2357
    },
    {
      "epoch": 2.2673076923076922,
      "grad_norm": 7.893104553222656,
      "learning_rate": 1.5467307692307695e-05,
      "loss": 1.1053,
      "step": 2358
    },
    {
      "epoch": 2.268269230769231,
      "grad_norm": 8.285309791564941,
      "learning_rate": 1.5465384615384617e-05,
      "loss": 1.6779,
      "step": 2359
    },
    {
      "epoch": 2.269230769230769,
      "grad_norm": 6.363640785217285,
      "learning_rate": 1.546346153846154e-05,
      "loss": 1.2083,
      "step": 2360
    },
    {
      "epoch": 2.270192307692308,
      "grad_norm": 8.85801887512207,
      "learning_rate": 1.5461538461538463e-05,
      "loss": 1.2733,
      "step": 2361
    },
    {
      "epoch": 2.271153846153846,
      "grad_norm": 6.015564918518066,
      "learning_rate": 1.5459615384615386e-05,
      "loss": 1.2211,
      "step": 2362
    },
    {
      "epoch": 2.2721153846153848,
      "grad_norm": 8.433279037475586,
      "learning_rate": 1.545769230769231e-05,
      "loss": 1.1979,
      "step": 2363
    },
    {
      "epoch": 2.273076923076923,
      "grad_norm": 8.021124839782715,
      "learning_rate": 1.5455769230769235e-05,
      "loss": 1.7656,
      "step": 2364
    },
    {
      "epoch": 2.2740384615384617,
      "grad_norm": 7.058034896850586,
      "learning_rate": 1.5453846153846154e-05,
      "loss": 0.0484,
      "step": 2365
    },
    {
      "epoch": 2.275,
      "grad_norm": 3.6090118885040283,
      "learning_rate": 1.545192307692308e-05,
      "loss": 0.0359,
      "step": 2366
    },
    {
      "epoch": 2.2759615384615386,
      "grad_norm": 9.941160202026367,
      "learning_rate": 1.545e-05,
      "loss": 1.1577,
      "step": 2367
    },
    {
      "epoch": 2.276923076923077,
      "grad_norm": 12.38436222076416,
      "learning_rate": 1.5448076923076926e-05,
      "loss": 1.706,
      "step": 2368
    },
    {
      "epoch": 2.2778846153846155,
      "grad_norm": 8.80104923248291,
      "learning_rate": 1.544615384615385e-05,
      "loss": 1.76,
      "step": 2369
    },
    {
      "epoch": 2.2788461538461537,
      "grad_norm": 6.5144429206848145,
      "learning_rate": 1.544423076923077e-05,
      "loss": 1.0033,
      "step": 2370
    },
    {
      "epoch": 2.2798076923076924,
      "grad_norm": 5.4242143630981445,
      "learning_rate": 1.5442307692307694e-05,
      "loss": 0.1083,
      "step": 2371
    },
    {
      "epoch": 2.2807692307692307,
      "grad_norm": 6.578988075256348,
      "learning_rate": 1.5440384615384617e-05,
      "loss": 0.1524,
      "step": 2372
    },
    {
      "epoch": 2.2817307692307693,
      "grad_norm": 6.935756206512451,
      "learning_rate": 1.543846153846154e-05,
      "loss": 1.2841,
      "step": 2373
    },
    {
      "epoch": 2.2826923076923076,
      "grad_norm": 9.180405616760254,
      "learning_rate": 1.5436538461538462e-05,
      "loss": 1.2359,
      "step": 2374
    },
    {
      "epoch": 2.2836538461538463,
      "grad_norm": 23.29261016845703,
      "learning_rate": 1.5434615384615385e-05,
      "loss": 0.9396,
      "step": 2375
    },
    {
      "epoch": 2.2846153846153845,
      "grad_norm": 4.408445835113525,
      "learning_rate": 1.543269230769231e-05,
      "loss": 1.479,
      "step": 2376
    },
    {
      "epoch": 2.285576923076923,
      "grad_norm": 8.826326370239258,
      "learning_rate": 1.543076923076923e-05,
      "loss": 0.5766,
      "step": 2377
    },
    {
      "epoch": 2.2865384615384614,
      "grad_norm": 7.629615783691406,
      "learning_rate": 1.5428846153846157e-05,
      "loss": 1.1555,
      "step": 2378
    },
    {
      "epoch": 2.2875,
      "grad_norm": 5.169440746307373,
      "learning_rate": 1.5426923076923076e-05,
      "loss": 0.255,
      "step": 2379
    },
    {
      "epoch": 2.2884615384615383,
      "grad_norm": 6.86454963684082,
      "learning_rate": 1.5425000000000002e-05,
      "loss": 1.5294,
      "step": 2380
    },
    {
      "epoch": 2.289423076923077,
      "grad_norm": 11.737427711486816,
      "learning_rate": 1.5423076923076925e-05,
      "loss": 1.2641,
      "step": 2381
    },
    {
      "epoch": 2.2903846153846152,
      "grad_norm": 13.07105827331543,
      "learning_rate": 1.5421153846153848e-05,
      "loss": 0.5662,
      "step": 2382
    },
    {
      "epoch": 2.291346153846154,
      "grad_norm": 6.8997416496276855,
      "learning_rate": 1.541923076923077e-05,
      "loss": 1.4217,
      "step": 2383
    },
    {
      "epoch": 2.292307692307692,
      "grad_norm": 6.580315589904785,
      "learning_rate": 1.5417307692307693e-05,
      "loss": 1.6018,
      "step": 2384
    },
    {
      "epoch": 2.293269230769231,
      "grad_norm": 8.998891830444336,
      "learning_rate": 1.5415384615384616e-05,
      "loss": 1.0072,
      "step": 2385
    },
    {
      "epoch": 2.294230769230769,
      "grad_norm": 9.328619956970215,
      "learning_rate": 1.541346153846154e-05,
      "loss": 1.4468,
      "step": 2386
    },
    {
      "epoch": 2.2951923076923078,
      "grad_norm": 7.607208728790283,
      "learning_rate": 1.541153846153846e-05,
      "loss": 0.9169,
      "step": 2387
    },
    {
      "epoch": 2.296153846153846,
      "grad_norm": 5.488137245178223,
      "learning_rate": 1.5409615384615388e-05,
      "loss": 1.4483,
      "step": 2388
    },
    {
      "epoch": 2.2971153846153847,
      "grad_norm": 6.040188789367676,
      "learning_rate": 1.5407692307692307e-05,
      "loss": 0.5254,
      "step": 2389
    },
    {
      "epoch": 2.298076923076923,
      "grad_norm": 5.4406938552856445,
      "learning_rate": 1.5405769230769233e-05,
      "loss": 0.9648,
      "step": 2390
    },
    {
      "epoch": 2.2990384615384616,
      "grad_norm": 6.082499027252197,
      "learning_rate": 1.5403846153846156e-05,
      "loss": 0.2669,
      "step": 2391
    },
    {
      "epoch": 2.3,
      "grad_norm": 9.002013206481934,
      "learning_rate": 1.540192307692308e-05,
      "loss": 0.2666,
      "step": 2392
    },
    {
      "epoch": 2.3009615384615385,
      "grad_norm": 9.046045303344727,
      "learning_rate": 1.54e-05,
      "loss": 1.2198,
      "step": 2393
    },
    {
      "epoch": 2.3019230769230767,
      "grad_norm": 6.643927574157715,
      "learning_rate": 1.5398076923076924e-05,
      "loss": 1.075,
      "step": 2394
    },
    {
      "epoch": 2.3028846153846154,
      "grad_norm": 10.979751586914062,
      "learning_rate": 1.5396153846153847e-05,
      "loss": 1.8292,
      "step": 2395
    },
    {
      "epoch": 2.3038461538461537,
      "grad_norm": 7.695205211639404,
      "learning_rate": 1.539423076923077e-05,
      "loss": 1.5422,
      "step": 2396
    },
    {
      "epoch": 2.3048076923076923,
      "grad_norm": 5.802228927612305,
      "learning_rate": 1.5392307692307693e-05,
      "loss": 1.5424,
      "step": 2397
    },
    {
      "epoch": 2.305769230769231,
      "grad_norm": 11.183008193969727,
      "learning_rate": 1.5390384615384615e-05,
      "loss": 1.4079,
      "step": 2398
    },
    {
      "epoch": 2.3067307692307693,
      "grad_norm": 5.531593322753906,
      "learning_rate": 1.5388461538461538e-05,
      "loss": 1.4714,
      "step": 2399
    },
    {
      "epoch": 2.3076923076923075,
      "grad_norm": 5.784765243530273,
      "learning_rate": 1.5386538461538464e-05,
      "loss": 1.208,
      "step": 2400
    },
    {
      "epoch": 2.308653846153846,
      "grad_norm": 6.13215446472168,
      "learning_rate": 1.5384615384615387e-05,
      "loss": 1.0051,
      "step": 2401
    },
    {
      "epoch": 2.309615384615385,
      "grad_norm": 12.040976524353027,
      "learning_rate": 1.538269230769231e-05,
      "loss": 0.8114,
      "step": 2402
    },
    {
      "epoch": 2.310576923076923,
      "grad_norm": 5.793372631072998,
      "learning_rate": 1.5380769230769233e-05,
      "loss": 0.7988,
      "step": 2403
    },
    {
      "epoch": 2.3115384615384613,
      "grad_norm": 7.2815022468566895,
      "learning_rate": 1.5378846153846155e-05,
      "loss": 1.1766,
      "step": 2404
    },
    {
      "epoch": 2.3125,
      "grad_norm": 7.73958683013916,
      "learning_rate": 1.5376923076923078e-05,
      "loss": 0.8413,
      "step": 2405
    },
    {
      "epoch": 2.3134615384615387,
      "grad_norm": 9.565838813781738,
      "learning_rate": 1.5375e-05,
      "loss": 0.7651,
      "step": 2406
    },
    {
      "epoch": 2.314423076923077,
      "grad_norm": 10.854613304138184,
      "learning_rate": 1.5373076923076924e-05,
      "loss": 0.6247,
      "step": 2407
    },
    {
      "epoch": 2.315384615384615,
      "grad_norm": 5.173485279083252,
      "learning_rate": 1.5371153846153847e-05,
      "loss": 0.6212,
      "step": 2408
    },
    {
      "epoch": 2.316346153846154,
      "grad_norm": 17.84440040588379,
      "learning_rate": 1.536923076923077e-05,
      "loss": 1.5636,
      "step": 2409
    },
    {
      "epoch": 2.3173076923076925,
      "grad_norm": 7.5886664390563965,
      "learning_rate": 1.5367307692307692e-05,
      "loss": 1.2621,
      "step": 2410
    },
    {
      "epoch": 2.3182692307692307,
      "grad_norm": 7.563826560974121,
      "learning_rate": 1.5365384615384618e-05,
      "loss": 1.4534,
      "step": 2411
    },
    {
      "epoch": 2.319230769230769,
      "grad_norm": 4.886754989624023,
      "learning_rate": 1.5363461538461538e-05,
      "loss": 0.4981,
      "step": 2412
    },
    {
      "epoch": 2.3201923076923077,
      "grad_norm": 7.134048938751221,
      "learning_rate": 1.5361538461538464e-05,
      "loss": 1.216,
      "step": 2413
    },
    {
      "epoch": 2.3211538461538463,
      "grad_norm": 5.624200820922852,
      "learning_rate": 1.5359615384615387e-05,
      "loss": 0.7844,
      "step": 2414
    },
    {
      "epoch": 2.3221153846153846,
      "grad_norm": 5.199100971221924,
      "learning_rate": 1.535769230769231e-05,
      "loss": 1.0588,
      "step": 2415
    },
    {
      "epoch": 2.3230769230769233,
      "grad_norm": 7.222662448883057,
      "learning_rate": 1.5355769230769232e-05,
      "loss": 1.2382,
      "step": 2416
    },
    {
      "epoch": 2.3240384615384615,
      "grad_norm": 5.624639511108398,
      "learning_rate": 1.5353846153846155e-05,
      "loss": 1.1482,
      "step": 2417
    },
    {
      "epoch": 2.325,
      "grad_norm": 7.089653968811035,
      "learning_rate": 1.5351923076923078e-05,
      "loss": 0.9473,
      "step": 2418
    },
    {
      "epoch": 2.3259615384615384,
      "grad_norm": 5.750602722167969,
      "learning_rate": 1.535e-05,
      "loss": 1.0413,
      "step": 2419
    },
    {
      "epoch": 2.326923076923077,
      "grad_norm": 6.298043251037598,
      "learning_rate": 1.5348076923076923e-05,
      "loss": 1.5589,
      "step": 2420
    },
    {
      "epoch": 2.3278846153846153,
      "grad_norm": 6.717876434326172,
      "learning_rate": 1.534615384615385e-05,
      "loss": 1.1147,
      "step": 2421
    },
    {
      "epoch": 2.328846153846154,
      "grad_norm": 7.521420001983643,
      "learning_rate": 1.534423076923077e-05,
      "loss": 1.0496,
      "step": 2422
    },
    {
      "epoch": 2.3298076923076922,
      "grad_norm": 6.687273979187012,
      "learning_rate": 1.5342307692307695e-05,
      "loss": 1.5364,
      "step": 2423
    },
    {
      "epoch": 2.330769230769231,
      "grad_norm": 6.979995250701904,
      "learning_rate": 1.5340384615384614e-05,
      "loss": 0.5323,
      "step": 2424
    },
    {
      "epoch": 2.331730769230769,
      "grad_norm": 13.814722061157227,
      "learning_rate": 1.533846153846154e-05,
      "loss": 1.4947,
      "step": 2425
    },
    {
      "epoch": 2.332692307692308,
      "grad_norm": 7.357981204986572,
      "learning_rate": 1.5336538461538463e-05,
      "loss": 0.9544,
      "step": 2426
    },
    {
      "epoch": 2.333653846153846,
      "grad_norm": 6.238796234130859,
      "learning_rate": 1.5334615384615386e-05,
      "loss": 1.2082,
      "step": 2427
    },
    {
      "epoch": 2.3346153846153848,
      "grad_norm": 6.855585098266602,
      "learning_rate": 1.533269230769231e-05,
      "loss": 1.7644,
      "step": 2428
    },
    {
      "epoch": 2.335576923076923,
      "grad_norm": 6.9722185134887695,
      "learning_rate": 1.533076923076923e-05,
      "loss": 1.3675,
      "step": 2429
    },
    {
      "epoch": 2.3365384615384617,
      "grad_norm": 5.298525333404541,
      "learning_rate": 1.5328846153846154e-05,
      "loss": 1.1971,
      "step": 2430
    },
    {
      "epoch": 2.3375,
      "grad_norm": 8.816765785217285,
      "learning_rate": 1.532692307692308e-05,
      "loss": 1.1892,
      "step": 2431
    },
    {
      "epoch": 2.3384615384615386,
      "grad_norm": 8.01426887512207,
      "learning_rate": 1.5325e-05,
      "loss": 1.3682,
      "step": 2432
    },
    {
      "epoch": 2.339423076923077,
      "grad_norm": 7.065020561218262,
      "learning_rate": 1.5323076923076926e-05,
      "loss": 1.1133,
      "step": 2433
    },
    {
      "epoch": 2.3403846153846155,
      "grad_norm": 6.770949363708496,
      "learning_rate": 1.5321153846153845e-05,
      "loss": 1.0841,
      "step": 2434
    },
    {
      "epoch": 2.3413461538461537,
      "grad_norm": 8.274012565612793,
      "learning_rate": 1.531923076923077e-05,
      "loss": 2.071,
      "step": 2435
    },
    {
      "epoch": 2.3423076923076924,
      "grad_norm": 5.906100273132324,
      "learning_rate": 1.5317307692307694e-05,
      "loss": 1.3315,
      "step": 2436
    },
    {
      "epoch": 2.3432692307692307,
      "grad_norm": 7.346860408782959,
      "learning_rate": 1.5315384615384617e-05,
      "loss": 1.0936,
      "step": 2437
    },
    {
      "epoch": 2.3442307692307693,
      "grad_norm": 15.099250793457031,
      "learning_rate": 1.531346153846154e-05,
      "loss": 1.0595,
      "step": 2438
    },
    {
      "epoch": 2.3451923076923076,
      "grad_norm": 5.503138542175293,
      "learning_rate": 1.5311538461538463e-05,
      "loss": 1.5812,
      "step": 2439
    },
    {
      "epoch": 2.3461538461538463,
      "grad_norm": 6.8581414222717285,
      "learning_rate": 1.5309615384615385e-05,
      "loss": 0.6839,
      "step": 2440
    },
    {
      "epoch": 2.3471153846153845,
      "grad_norm": 7.313173770904541,
      "learning_rate": 1.5307692307692308e-05,
      "loss": 0.5713,
      "step": 2441
    },
    {
      "epoch": 2.348076923076923,
      "grad_norm": 5.953409671783447,
      "learning_rate": 1.530576923076923e-05,
      "loss": 1.422,
      "step": 2442
    },
    {
      "epoch": 2.3490384615384614,
      "grad_norm": 11.162751197814941,
      "learning_rate": 1.5303846153846157e-05,
      "loss": 1.015,
      "step": 2443
    },
    {
      "epoch": 2.35,
      "grad_norm": 7.370990753173828,
      "learning_rate": 1.5301923076923076e-05,
      "loss": 1.4752,
      "step": 2444
    },
    {
      "epoch": 2.3509615384615383,
      "grad_norm": 3.2970166206359863,
      "learning_rate": 1.5300000000000003e-05,
      "loss": 0.0519,
      "step": 2445
    },
    {
      "epoch": 2.351923076923077,
      "grad_norm": 9.095805168151855,
      "learning_rate": 1.5298076923076922e-05,
      "loss": 0.9917,
      "step": 2446
    },
    {
      "epoch": 2.3528846153846152,
      "grad_norm": 9.955023765563965,
      "learning_rate": 1.5296153846153848e-05,
      "loss": 1.0327,
      "step": 2447
    },
    {
      "epoch": 2.353846153846154,
      "grad_norm": 7.974667072296143,
      "learning_rate": 1.529423076923077e-05,
      "loss": 1.0922,
      "step": 2448
    },
    {
      "epoch": 2.354807692307692,
      "grad_norm": 6.740225791931152,
      "learning_rate": 1.5292307692307694e-05,
      "loss": 0.0439,
      "step": 2449
    },
    {
      "epoch": 2.355769230769231,
      "grad_norm": 4.970646381378174,
      "learning_rate": 1.5290384615384616e-05,
      "loss": 1.6613,
      "step": 2450
    },
    {
      "epoch": 2.356730769230769,
      "grad_norm": 8.110488891601562,
      "learning_rate": 1.528846153846154e-05,
      "loss": 1.1483,
      "step": 2451
    },
    {
      "epoch": 2.3576923076923078,
      "grad_norm": 7.430108547210693,
      "learning_rate": 1.5286538461538462e-05,
      "loss": 1.2023,
      "step": 2452
    },
    {
      "epoch": 2.358653846153846,
      "grad_norm": 5.921379566192627,
      "learning_rate": 1.5284615384615388e-05,
      "loss": 1.4833,
      "step": 2453
    },
    {
      "epoch": 2.3596153846153847,
      "grad_norm": 7.270022392272949,
      "learning_rate": 1.5282692307692308e-05,
      "loss": 1.1561,
      "step": 2454
    },
    {
      "epoch": 2.360576923076923,
      "grad_norm": 11.682878494262695,
      "learning_rate": 1.5280769230769234e-05,
      "loss": 0.6163,
      "step": 2455
    },
    {
      "epoch": 2.3615384615384616,
      "grad_norm": 7.238492965698242,
      "learning_rate": 1.5278846153846153e-05,
      "loss": 0.7358,
      "step": 2456
    },
    {
      "epoch": 2.3625,
      "grad_norm": 5.8044023513793945,
      "learning_rate": 1.527692307692308e-05,
      "loss": 1.6445,
      "step": 2457
    },
    {
      "epoch": 2.3634615384615385,
      "grad_norm": 5.814533710479736,
      "learning_rate": 1.5275000000000002e-05,
      "loss": 1.1725,
      "step": 2458
    },
    {
      "epoch": 2.3644230769230767,
      "grad_norm": 8.353511810302734,
      "learning_rate": 1.5273076923076925e-05,
      "loss": 1.0681,
      "step": 2459
    },
    {
      "epoch": 2.3653846153846154,
      "grad_norm": 6.833092212677002,
      "learning_rate": 1.5271153846153848e-05,
      "loss": 1.2469,
      "step": 2460
    },
    {
      "epoch": 2.3663461538461537,
      "grad_norm": 12.48971939086914,
      "learning_rate": 1.526923076923077e-05,
      "loss": 0.169,
      "step": 2461
    },
    {
      "epoch": 2.3673076923076923,
      "grad_norm": 7.862042427062988,
      "learning_rate": 1.5267307692307693e-05,
      "loss": 1.455,
      "step": 2462
    },
    {
      "epoch": 2.368269230769231,
      "grad_norm": 6.605043411254883,
      "learning_rate": 1.5265384615384616e-05,
      "loss": 0.9425,
      "step": 2463
    },
    {
      "epoch": 2.3692307692307693,
      "grad_norm": 14.236662864685059,
      "learning_rate": 1.526346153846154e-05,
      "loss": 0.5909,
      "step": 2464
    },
    {
      "epoch": 2.3701923076923075,
      "grad_norm": 4.898730278015137,
      "learning_rate": 1.5261538461538465e-05,
      "loss": 1.0914,
      "step": 2465
    },
    {
      "epoch": 2.371153846153846,
      "grad_norm": 7.427409648895264,
      "learning_rate": 1.5259615384615384e-05,
      "loss": 1.4078,
      "step": 2466
    },
    {
      "epoch": 2.372115384615385,
      "grad_norm": 11.660849571228027,
      "learning_rate": 1.5257692307692309e-05,
      "loss": 0.6773,
      "step": 2467
    },
    {
      "epoch": 2.373076923076923,
      "grad_norm": 6.4248504638671875,
      "learning_rate": 1.5255769230769233e-05,
      "loss": 1.0473,
      "step": 2468
    },
    {
      "epoch": 2.3740384615384613,
      "grad_norm": 7.202408313751221,
      "learning_rate": 1.5253846153846154e-05,
      "loss": 1.5555,
      "step": 2469
    },
    {
      "epoch": 2.375,
      "grad_norm": 8.785852432250977,
      "learning_rate": 1.5251923076923079e-05,
      "loss": 0.8713,
      "step": 2470
    },
    {
      "epoch": 2.3759615384615387,
      "grad_norm": 5.079291343688965,
      "learning_rate": 1.525e-05,
      "loss": 1.478,
      "step": 2471
    },
    {
      "epoch": 2.376923076923077,
      "grad_norm": 8.62426471710205,
      "learning_rate": 1.5248076923076924e-05,
      "loss": 1.0552,
      "step": 2472
    },
    {
      "epoch": 2.377884615384615,
      "grad_norm": 8.924383163452148,
      "learning_rate": 1.5246153846153849e-05,
      "loss": 1.4358,
      "step": 2473
    },
    {
      "epoch": 2.378846153846154,
      "grad_norm": 6.970930099487305,
      "learning_rate": 1.524423076923077e-05,
      "loss": 0.8496,
      "step": 2474
    },
    {
      "epoch": 2.3798076923076925,
      "grad_norm": 8.44778060913086,
      "learning_rate": 1.5242307692307694e-05,
      "loss": 1.3558,
      "step": 2475
    },
    {
      "epoch": 2.3807692307692307,
      "grad_norm": 10.822294235229492,
      "learning_rate": 1.5240384615384615e-05,
      "loss": 0.5953,
      "step": 2476
    },
    {
      "epoch": 2.381730769230769,
      "grad_norm": 17.281314849853516,
      "learning_rate": 1.523846153846154e-05,
      "loss": 1.2049,
      "step": 2477
    },
    {
      "epoch": 2.3826923076923077,
      "grad_norm": 7.498358249664307,
      "learning_rate": 1.5236538461538464e-05,
      "loss": 1.4182,
      "step": 2478
    },
    {
      "epoch": 2.3836538461538463,
      "grad_norm": 7.732288360595703,
      "learning_rate": 1.5234615384615385e-05,
      "loss": 1.0666,
      "step": 2479
    },
    {
      "epoch": 2.3846153846153846,
      "grad_norm": 10.415899276733398,
      "learning_rate": 1.523269230769231e-05,
      "loss": 1.186,
      "step": 2480
    },
    {
      "epoch": 2.3855769230769233,
      "grad_norm": 9.594757080078125,
      "learning_rate": 1.523076923076923e-05,
      "loss": 1.2795,
      "step": 2481
    },
    {
      "epoch": 2.3865384615384615,
      "grad_norm": 4.512599945068359,
      "learning_rate": 1.5228846153846155e-05,
      "loss": 0.0498,
      "step": 2482
    },
    {
      "epoch": 2.3875,
      "grad_norm": 8.411462783813477,
      "learning_rate": 1.522692307692308e-05,
      "loss": 1.2781,
      "step": 2483
    },
    {
      "epoch": 2.3884615384615384,
      "grad_norm": 7.905545234680176,
      "learning_rate": 1.5225e-05,
      "loss": 1.0896,
      "step": 2484
    },
    {
      "epoch": 2.389423076923077,
      "grad_norm": 9.831338882446289,
      "learning_rate": 1.5223076923076925e-05,
      "loss": 0.2105,
      "step": 2485
    },
    {
      "epoch": 2.3903846153846153,
      "grad_norm": 6.416050434112549,
      "learning_rate": 1.5221153846153846e-05,
      "loss": 1.1031,
      "step": 2486
    },
    {
      "epoch": 2.391346153846154,
      "grad_norm": 6.676692008972168,
      "learning_rate": 1.521923076923077e-05,
      "loss": 1.8617,
      "step": 2487
    },
    {
      "epoch": 2.3923076923076922,
      "grad_norm": 6.9305100440979,
      "learning_rate": 1.5217307692307695e-05,
      "loss": 1.2176,
      "step": 2488
    },
    {
      "epoch": 2.393269230769231,
      "grad_norm": 11.135340690612793,
      "learning_rate": 1.5215384615384616e-05,
      "loss": 1.2099,
      "step": 2489
    },
    {
      "epoch": 2.394230769230769,
      "grad_norm": 7.556827068328857,
      "learning_rate": 1.521346153846154e-05,
      "loss": 0.7512,
      "step": 2490
    },
    {
      "epoch": 2.395192307692308,
      "grad_norm": 10.407425880432129,
      "learning_rate": 1.5211538461538462e-05,
      "loss": 1.8858,
      "step": 2491
    },
    {
      "epoch": 2.396153846153846,
      "grad_norm": 6.084803104400635,
      "learning_rate": 1.5209615384615386e-05,
      "loss": 0.6587,
      "step": 2492
    },
    {
      "epoch": 2.3971153846153848,
      "grad_norm": 6.38009786605835,
      "learning_rate": 1.520769230769231e-05,
      "loss": 0.6695,
      "step": 2493
    },
    {
      "epoch": 2.398076923076923,
      "grad_norm": 9.356372833251953,
      "learning_rate": 1.5205769230769232e-05,
      "loss": 1.2497,
      "step": 2494
    },
    {
      "epoch": 2.3990384615384617,
      "grad_norm": 9.20823860168457,
      "learning_rate": 1.5203846153846156e-05,
      "loss": 0.8992,
      "step": 2495
    },
    {
      "epoch": 2.4,
      "grad_norm": 23.66482162475586,
      "learning_rate": 1.5201923076923077e-05,
      "loss": 0.907,
      "step": 2496
    },
    {
      "epoch": 2.4009615384615386,
      "grad_norm": 7.424279689788818,
      "learning_rate": 1.5200000000000002e-05,
      "loss": 0.9395,
      "step": 2497
    },
    {
      "epoch": 2.401923076923077,
      "grad_norm": 6.377523422241211,
      "learning_rate": 1.5198076923076925e-05,
      "loss": 1.3601,
      "step": 2498
    },
    {
      "epoch": 2.4028846153846155,
      "grad_norm": 10.129199981689453,
      "learning_rate": 1.5196153846153847e-05,
      "loss": 1.5922,
      "step": 2499
    },
    {
      "epoch": 2.4038461538461537,
      "grad_norm": 5.283593654632568,
      "learning_rate": 1.5194230769230772e-05,
      "loss": 1.3979,
      "step": 2500
    },
    {
      "epoch": 2.4048076923076924,
      "grad_norm": 8.096043586730957,
      "learning_rate": 1.5192307692307693e-05,
      "loss": 1.4925,
      "step": 2501
    },
    {
      "epoch": 2.4057692307692307,
      "grad_norm": 10.438643455505371,
      "learning_rate": 1.5190384615384617e-05,
      "loss": 0.4066,
      "step": 2502
    },
    {
      "epoch": 2.4067307692307693,
      "grad_norm": 6.650524139404297,
      "learning_rate": 1.518846153846154e-05,
      "loss": 1.0601,
      "step": 2503
    },
    {
      "epoch": 2.4076923076923076,
      "grad_norm": 5.168906211853027,
      "learning_rate": 1.5186538461538463e-05,
      "loss": 1.529,
      "step": 2504
    },
    {
      "epoch": 2.4086538461538463,
      "grad_norm": 7.80670690536499,
      "learning_rate": 1.5184615384615386e-05,
      "loss": 0.9964,
      "step": 2505
    },
    {
      "epoch": 2.4096153846153845,
      "grad_norm": 10.607888221740723,
      "learning_rate": 1.5182692307692308e-05,
      "loss": 1.625,
      "step": 2506
    },
    {
      "epoch": 2.410576923076923,
      "grad_norm": 5.5447564125061035,
      "learning_rate": 1.5180769230769233e-05,
      "loss": 1.2859,
      "step": 2507
    },
    {
      "epoch": 2.4115384615384614,
      "grad_norm": 11.315715789794922,
      "learning_rate": 1.5178846153846154e-05,
      "loss": 1.4431,
      "step": 2508
    },
    {
      "epoch": 2.4125,
      "grad_norm": 6.744110584259033,
      "learning_rate": 1.5176923076923078e-05,
      "loss": 1.077,
      "step": 2509
    },
    {
      "epoch": 2.4134615384615383,
      "grad_norm": 11.044194221496582,
      "learning_rate": 1.5175000000000001e-05,
      "loss": 1.1821,
      "step": 2510
    },
    {
      "epoch": 2.414423076923077,
      "grad_norm": 6.332521438598633,
      "learning_rate": 1.5173076923076924e-05,
      "loss": 1.1226,
      "step": 2511
    },
    {
      "epoch": 2.4153846153846152,
      "grad_norm": 11.540447235107422,
      "learning_rate": 1.5171153846153848e-05,
      "loss": 1.3405,
      "step": 2512
    },
    {
      "epoch": 2.416346153846154,
      "grad_norm": 5.377595901489258,
      "learning_rate": 1.516923076923077e-05,
      "loss": 1.6567,
      "step": 2513
    },
    {
      "epoch": 2.417307692307692,
      "grad_norm": 16.18889808654785,
      "learning_rate": 1.5167307692307694e-05,
      "loss": 1.5195,
      "step": 2514
    },
    {
      "epoch": 2.418269230769231,
      "grad_norm": 7.673929691314697,
      "learning_rate": 1.5165384615384617e-05,
      "loss": 1.2927,
      "step": 2515
    },
    {
      "epoch": 2.419230769230769,
      "grad_norm": 9.220540046691895,
      "learning_rate": 1.516346153846154e-05,
      "loss": 0.8576,
      "step": 2516
    },
    {
      "epoch": 2.4201923076923078,
      "grad_norm": 8.920741081237793,
      "learning_rate": 1.5161538461538462e-05,
      "loss": 1.0338,
      "step": 2517
    },
    {
      "epoch": 2.421153846153846,
      "grad_norm": 14.719722747802734,
      "learning_rate": 1.5159615384615385e-05,
      "loss": 1.4269,
      "step": 2518
    },
    {
      "epoch": 2.4221153846153847,
      "grad_norm": 5.781734466552734,
      "learning_rate": 1.515769230769231e-05,
      "loss": 1.2446,
      "step": 2519
    },
    {
      "epoch": 2.423076923076923,
      "grad_norm": 7.7571024894714355,
      "learning_rate": 1.5155769230769232e-05,
      "loss": 1.4604,
      "step": 2520
    },
    {
      "epoch": 2.4240384615384616,
      "grad_norm": 6.01127815246582,
      "learning_rate": 1.5153846153846155e-05,
      "loss": 0.9548,
      "step": 2521
    },
    {
      "epoch": 2.425,
      "grad_norm": 7.875495910644531,
      "learning_rate": 1.5151923076923078e-05,
      "loss": 1.0982,
      "step": 2522
    },
    {
      "epoch": 2.4259615384615385,
      "grad_norm": 7.332261562347412,
      "learning_rate": 1.515e-05,
      "loss": 1.5418,
      "step": 2523
    },
    {
      "epoch": 2.4269230769230767,
      "grad_norm": 6.598318099975586,
      "learning_rate": 1.5148076923076923e-05,
      "loss": 1.2028,
      "step": 2524
    },
    {
      "epoch": 2.4278846153846154,
      "grad_norm": 7.340402126312256,
      "learning_rate": 1.5146153846153848e-05,
      "loss": 0.5346,
      "step": 2525
    },
    {
      "epoch": 2.4288461538461537,
      "grad_norm": 4.818821907043457,
      "learning_rate": 1.514423076923077e-05,
      "loss": 1.1883,
      "step": 2526
    },
    {
      "epoch": 2.4298076923076923,
      "grad_norm": 7.801273345947266,
      "learning_rate": 1.5142307692307693e-05,
      "loss": 1.4342,
      "step": 2527
    },
    {
      "epoch": 2.430769230769231,
      "grad_norm": 12.436971664428711,
      "learning_rate": 1.5140384615384616e-05,
      "loss": 1.5946,
      "step": 2528
    },
    {
      "epoch": 2.4317307692307693,
      "grad_norm": 6.712306499481201,
      "learning_rate": 1.5138461538461539e-05,
      "loss": 1.2967,
      "step": 2529
    },
    {
      "epoch": 2.4326923076923075,
      "grad_norm": 9.933676719665527,
      "learning_rate": 1.5136538461538463e-05,
      "loss": 1.1023,
      "step": 2530
    },
    {
      "epoch": 2.433653846153846,
      "grad_norm": 3.831475257873535,
      "learning_rate": 1.5134615384615386e-05,
      "loss": 0.2599,
      "step": 2531
    },
    {
      "epoch": 2.434615384615385,
      "grad_norm": 9.56947135925293,
      "learning_rate": 1.5132692307692309e-05,
      "loss": 1.6831,
      "step": 2532
    },
    {
      "epoch": 2.435576923076923,
      "grad_norm": 7.708418846130371,
      "learning_rate": 1.5130769230769232e-05,
      "loss": 1.1662,
      "step": 2533
    },
    {
      "epoch": 2.4365384615384613,
      "grad_norm": 7.244637966156006,
      "learning_rate": 1.5128846153846154e-05,
      "loss": 1.476,
      "step": 2534
    },
    {
      "epoch": 2.4375,
      "grad_norm": 6.886122703552246,
      "learning_rate": 1.5126923076923079e-05,
      "loss": 0.158,
      "step": 2535
    },
    {
      "epoch": 2.4384615384615387,
      "grad_norm": 32.92527389526367,
      "learning_rate": 1.5125e-05,
      "loss": 0.3239,
      "step": 2536
    },
    {
      "epoch": 2.439423076923077,
      "grad_norm": 6.073482513427734,
      "learning_rate": 1.5123076923076924e-05,
      "loss": 0.5518,
      "step": 2537
    },
    {
      "epoch": 2.440384615384615,
      "grad_norm": 7.146710395812988,
      "learning_rate": 1.5121153846153847e-05,
      "loss": 0.3033,
      "step": 2538
    },
    {
      "epoch": 2.441346153846154,
      "grad_norm": 13.075833320617676,
      "learning_rate": 1.511923076923077e-05,
      "loss": 1.9007,
      "step": 2539
    },
    {
      "epoch": 2.4423076923076925,
      "grad_norm": 10.080572128295898,
      "learning_rate": 1.5117307692307694e-05,
      "loss": 0.3915,
      "step": 2540
    },
    {
      "epoch": 2.4432692307692307,
      "grad_norm": 9.45992660522461,
      "learning_rate": 1.5115384615384616e-05,
      "loss": 1.1515,
      "step": 2541
    },
    {
      "epoch": 2.444230769230769,
      "grad_norm": 9.444137573242188,
      "learning_rate": 1.511346153846154e-05,
      "loss": 1.1787,
      "step": 2542
    },
    {
      "epoch": 2.4451923076923077,
      "grad_norm": 8.250717163085938,
      "learning_rate": 1.5111538461538461e-05,
      "loss": 1.6097,
      "step": 2543
    },
    {
      "epoch": 2.4461538461538463,
      "grad_norm": 14.08137321472168,
      "learning_rate": 1.5109615384615386e-05,
      "loss": 1.7567,
      "step": 2544
    },
    {
      "epoch": 2.4471153846153846,
      "grad_norm": 7.164212703704834,
      "learning_rate": 1.510769230769231e-05,
      "loss": 1.1669,
      "step": 2545
    },
    {
      "epoch": 2.4480769230769233,
      "grad_norm": 7.645793437957764,
      "learning_rate": 1.5105769230769231e-05,
      "loss": 0.6664,
      "step": 2546
    },
    {
      "epoch": 2.4490384615384615,
      "grad_norm": 7.024567127227783,
      "learning_rate": 1.5103846153846156e-05,
      "loss": 0.9228,
      "step": 2547
    },
    {
      "epoch": 2.45,
      "grad_norm": 4.266313076019287,
      "learning_rate": 1.5101923076923077e-05,
      "loss": 0.1721,
      "step": 2548
    },
    {
      "epoch": 2.4509615384615384,
      "grad_norm": 11.066993713378906,
      "learning_rate": 1.5100000000000001e-05,
      "loss": 0.5538,
      "step": 2549
    },
    {
      "epoch": 2.451923076923077,
      "grad_norm": 7.919879913330078,
      "learning_rate": 1.5098076923076926e-05,
      "loss": 1.8211,
      "step": 2550
    },
    {
      "epoch": 2.4528846153846153,
      "grad_norm": 7.250777721405029,
      "learning_rate": 1.5096153846153847e-05,
      "loss": 1.2407,
      "step": 2551
    },
    {
      "epoch": 2.453846153846154,
      "grad_norm": 7.330770015716553,
      "learning_rate": 1.5094230769230771e-05,
      "loss": 0.6321,
      "step": 2552
    },
    {
      "epoch": 2.4548076923076922,
      "grad_norm": 8.303705215454102,
      "learning_rate": 1.5092307692307692e-05,
      "loss": 1.094,
      "step": 2553
    },
    {
      "epoch": 2.455769230769231,
      "grad_norm": 6.128197193145752,
      "learning_rate": 1.5090384615384617e-05,
      "loss": 0.9547,
      "step": 2554
    },
    {
      "epoch": 2.456730769230769,
      "grad_norm": 7.045146465301514,
      "learning_rate": 1.5088461538461541e-05,
      "loss": 1.6208,
      "step": 2555
    },
    {
      "epoch": 2.457692307692308,
      "grad_norm": 11.164501190185547,
      "learning_rate": 1.5086538461538462e-05,
      "loss": 1.4266,
      "step": 2556
    },
    {
      "epoch": 2.458653846153846,
      "grad_norm": 9.231949806213379,
      "learning_rate": 1.5084615384615387e-05,
      "loss": 1.3024,
      "step": 2557
    },
    {
      "epoch": 2.4596153846153848,
      "grad_norm": 9.177565574645996,
      "learning_rate": 1.5082692307692308e-05,
      "loss": 0.8844,
      "step": 2558
    },
    {
      "epoch": 2.460576923076923,
      "grad_norm": 7.945921421051025,
      "learning_rate": 1.5080769230769232e-05,
      "loss": 0.8294,
      "step": 2559
    },
    {
      "epoch": 2.4615384615384617,
      "grad_norm": 5.761846542358398,
      "learning_rate": 1.5078846153846157e-05,
      "loss": 1.4011,
      "step": 2560
    },
    {
      "epoch": 2.4625,
      "grad_norm": 12.557519912719727,
      "learning_rate": 1.5076923076923078e-05,
      "loss": 0.8825,
      "step": 2561
    },
    {
      "epoch": 2.4634615384615386,
      "grad_norm": 10.779595375061035,
      "learning_rate": 1.5075000000000002e-05,
      "loss": 1.3578,
      "step": 2562
    },
    {
      "epoch": 2.464423076923077,
      "grad_norm": 6.10067892074585,
      "learning_rate": 1.5073076923076923e-05,
      "loss": 0.9594,
      "step": 2563
    },
    {
      "epoch": 2.4653846153846155,
      "grad_norm": 6.733580112457275,
      "learning_rate": 1.5071153846153848e-05,
      "loss": 0.7332,
      "step": 2564
    },
    {
      "epoch": 2.4663461538461537,
      "grad_norm": 17.38373374938965,
      "learning_rate": 1.5069230769230772e-05,
      "loss": 0.7225,
      "step": 2565
    },
    {
      "epoch": 2.4673076923076924,
      "grad_norm": 5.450506687164307,
      "learning_rate": 1.5067307692307693e-05,
      "loss": 1.1748,
      "step": 2566
    },
    {
      "epoch": 2.4682692307692307,
      "grad_norm": 10.418675422668457,
      "learning_rate": 1.5065384615384618e-05,
      "loss": 0.405,
      "step": 2567
    },
    {
      "epoch": 2.4692307692307693,
      "grad_norm": 7.369482517242432,
      "learning_rate": 1.5063461538461539e-05,
      "loss": 1.0967,
      "step": 2568
    },
    {
      "epoch": 2.4701923076923076,
      "grad_norm": 7.476597309112549,
      "learning_rate": 1.5061538461538463e-05,
      "loss": 1.2722,
      "step": 2569
    },
    {
      "epoch": 2.4711538461538463,
      "grad_norm": 6.096284866333008,
      "learning_rate": 1.5059615384615384e-05,
      "loss": 1.0769,
      "step": 2570
    },
    {
      "epoch": 2.4721153846153845,
      "grad_norm": 8.532492637634277,
      "learning_rate": 1.5057692307692309e-05,
      "loss": 0.1926,
      "step": 2571
    },
    {
      "epoch": 2.473076923076923,
      "grad_norm": 6.74486780166626,
      "learning_rate": 1.5055769230769233e-05,
      "loss": 0.8272,
      "step": 2572
    },
    {
      "epoch": 2.4740384615384614,
      "grad_norm": 5.2559051513671875,
      "learning_rate": 1.5053846153846154e-05,
      "loss": 1.4484,
      "step": 2573
    },
    {
      "epoch": 2.475,
      "grad_norm": 4.951578617095947,
      "learning_rate": 1.5051923076923079e-05,
      "loss": 0.1892,
      "step": 2574
    },
    {
      "epoch": 2.4759615384615383,
      "grad_norm": 9.398468971252441,
      "learning_rate": 1.505e-05,
      "loss": 1.0801,
      "step": 2575
    },
    {
      "epoch": 2.476923076923077,
      "grad_norm": 8.603503227233887,
      "learning_rate": 1.5048076923076924e-05,
      "loss": 0.9812,
      "step": 2576
    },
    {
      "epoch": 2.4778846153846152,
      "grad_norm": 5.922712802886963,
      "learning_rate": 1.5046153846153849e-05,
      "loss": 1.4879,
      "step": 2577
    },
    {
      "epoch": 2.478846153846154,
      "grad_norm": 6.061567306518555,
      "learning_rate": 1.504423076923077e-05,
      "loss": 1.2635,
      "step": 2578
    },
    {
      "epoch": 2.479807692307692,
      "grad_norm": 9.112146377563477,
      "learning_rate": 1.5042307692307694e-05,
      "loss": 1.2824,
      "step": 2579
    },
    {
      "epoch": 2.480769230769231,
      "grad_norm": 4.405335426330566,
      "learning_rate": 1.5040384615384615e-05,
      "loss": 0.0538,
      "step": 2580
    },
    {
      "epoch": 2.481730769230769,
      "grad_norm": 6.792515754699707,
      "learning_rate": 1.503846153846154e-05,
      "loss": 1.1973,
      "step": 2581
    },
    {
      "epoch": 2.4826923076923078,
      "grad_norm": 5.407083988189697,
      "learning_rate": 1.5036538461538464e-05,
      "loss": 1.4138,
      "step": 2582
    },
    {
      "epoch": 2.483653846153846,
      "grad_norm": 8.627145767211914,
      "learning_rate": 1.5034615384615385e-05,
      "loss": 1.483,
      "step": 2583
    },
    {
      "epoch": 2.4846153846153847,
      "grad_norm": 7.338160514831543,
      "learning_rate": 1.503269230769231e-05,
      "loss": 1.3803,
      "step": 2584
    },
    {
      "epoch": 2.485576923076923,
      "grad_norm": 3.5663201808929443,
      "learning_rate": 1.5030769230769231e-05,
      "loss": 0.0272,
      "step": 2585
    },
    {
      "epoch": 2.4865384615384616,
      "grad_norm": 6.701585292816162,
      "learning_rate": 1.5028846153846155e-05,
      "loss": 1.1712,
      "step": 2586
    },
    {
      "epoch": 2.4875,
      "grad_norm": 9.680261611938477,
      "learning_rate": 1.5026923076923078e-05,
      "loss": 1.2979,
      "step": 2587
    },
    {
      "epoch": 2.4884615384615385,
      "grad_norm": 9.201251029968262,
      "learning_rate": 1.5025000000000001e-05,
      "loss": 1.3203,
      "step": 2588
    },
    {
      "epoch": 2.4894230769230767,
      "grad_norm": 7.369772911071777,
      "learning_rate": 1.5023076923076925e-05,
      "loss": 0.7491,
      "step": 2589
    },
    {
      "epoch": 2.4903846153846154,
      "grad_norm": 8.463709831237793,
      "learning_rate": 1.5021153846153847e-05,
      "loss": 0.7899,
      "step": 2590
    },
    {
      "epoch": 2.4913461538461537,
      "grad_norm": 8.528531074523926,
      "learning_rate": 1.5019230769230771e-05,
      "loss": 1.0755,
      "step": 2591
    },
    {
      "epoch": 2.4923076923076923,
      "grad_norm": 8.75124740600586,
      "learning_rate": 1.5017307692307694e-05,
      "loss": 1.3925,
      "step": 2592
    },
    {
      "epoch": 2.493269230769231,
      "grad_norm": 6.167721271514893,
      "learning_rate": 1.5015384615384617e-05,
      "loss": 1.5389,
      "step": 2593
    },
    {
      "epoch": 2.4942307692307693,
      "grad_norm": 8.978302955627441,
      "learning_rate": 1.501346153846154e-05,
      "loss": 0.9105,
      "step": 2594
    },
    {
      "epoch": 2.4951923076923075,
      "grad_norm": 7.543013572692871,
      "learning_rate": 1.5011538461538462e-05,
      "loss": 0.581,
      "step": 2595
    },
    {
      "epoch": 2.496153846153846,
      "grad_norm": 6.593694686889648,
      "learning_rate": 1.5009615384615387e-05,
      "loss": 1.2358,
      "step": 2596
    },
    {
      "epoch": 2.497115384615385,
      "grad_norm": 5.957122802734375,
      "learning_rate": 1.500769230769231e-05,
      "loss": 1.6989,
      "step": 2597
    },
    {
      "epoch": 2.498076923076923,
      "grad_norm": 6.5193352699279785,
      "learning_rate": 1.5005769230769232e-05,
      "loss": 1.364,
      "step": 2598
    },
    {
      "epoch": 2.4990384615384613,
      "grad_norm": 2.23085618019104,
      "learning_rate": 1.5003846153846155e-05,
      "loss": 0.0156,
      "step": 2599
    },
    {
      "epoch": 2.5,
      "grad_norm": 15.397344589233398,
      "learning_rate": 1.5001923076923078e-05,
      "loss": 1.2618,
      "step": 2600
    },
    {
      "epoch": 2.5009615384615387,
      "grad_norm": 7.859701156616211,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 1.4186,
      "step": 2601
    },
    {
      "epoch": 2.501923076923077,
      "grad_norm": 5.645761013031006,
      "learning_rate": 1.4998076923076925e-05,
      "loss": 1.4469,
      "step": 2602
    },
    {
      "epoch": 2.502884615384615,
      "grad_norm": 8.768837928771973,
      "learning_rate": 1.4996153846153848e-05,
      "loss": 0.8363,
      "step": 2603
    },
    {
      "epoch": 2.503846153846154,
      "grad_norm": 4.424022197723389,
      "learning_rate": 1.499423076923077e-05,
      "loss": 1.584,
      "step": 2604
    },
    {
      "epoch": 2.5048076923076925,
      "grad_norm": 7.1015706062316895,
      "learning_rate": 1.4992307692307693e-05,
      "loss": 1.2049,
      "step": 2605
    },
    {
      "epoch": 2.5057692307692307,
      "grad_norm": 6.032211780548096,
      "learning_rate": 1.4990384615384616e-05,
      "loss": 0.4397,
      "step": 2606
    },
    {
      "epoch": 2.506730769230769,
      "grad_norm": 6.607552528381348,
      "learning_rate": 1.498846153846154e-05,
      "loss": 0.7879,
      "step": 2607
    },
    {
      "epoch": 2.5076923076923077,
      "grad_norm": 7.063595294952393,
      "learning_rate": 1.4986538461538463e-05,
      "loss": 1.0923,
      "step": 2608
    },
    {
      "epoch": 2.5086538461538463,
      "grad_norm": 7.118251323699951,
      "learning_rate": 1.4984615384615386e-05,
      "loss": 0.9647,
      "step": 2609
    },
    {
      "epoch": 2.5096153846153846,
      "grad_norm": 9.63654899597168,
      "learning_rate": 1.4982692307692309e-05,
      "loss": 0.9454,
      "step": 2610
    },
    {
      "epoch": 2.510576923076923,
      "grad_norm": 8.24724292755127,
      "learning_rate": 1.4980769230769231e-05,
      "loss": 1.159,
      "step": 2611
    },
    {
      "epoch": 2.5115384615384615,
      "grad_norm": 7.8975090980529785,
      "learning_rate": 1.4978846153846156e-05,
      "loss": 0.6813,
      "step": 2612
    },
    {
      "epoch": 2.5125,
      "grad_norm": 8.81673526763916,
      "learning_rate": 1.4976923076923077e-05,
      "loss": 1.8071,
      "step": 2613
    },
    {
      "epoch": 2.5134615384615384,
      "grad_norm": 11.21776294708252,
      "learning_rate": 1.4975000000000001e-05,
      "loss": 0.1778,
      "step": 2614
    },
    {
      "epoch": 2.5144230769230766,
      "grad_norm": 5.479291915893555,
      "learning_rate": 1.4973076923076924e-05,
      "loss": 0.9901,
      "step": 2615
    },
    {
      "epoch": 2.5153846153846153,
      "grad_norm": 12.15798568725586,
      "learning_rate": 1.4971153846153847e-05,
      "loss": 0.2171,
      "step": 2616
    },
    {
      "epoch": 2.516346153846154,
      "grad_norm": 9.108449935913086,
      "learning_rate": 1.4969230769230771e-05,
      "loss": 1.5127,
      "step": 2617
    },
    {
      "epoch": 2.5173076923076922,
      "grad_norm": 10.48243522644043,
      "learning_rate": 1.4967307692307693e-05,
      "loss": 1.2288,
      "step": 2618
    },
    {
      "epoch": 2.518269230769231,
      "grad_norm": 8.332091331481934,
      "learning_rate": 1.4965384615384617e-05,
      "loss": 1.4792,
      "step": 2619
    },
    {
      "epoch": 2.519230769230769,
      "grad_norm": 5.671188831329346,
      "learning_rate": 1.4963461538461538e-05,
      "loss": 1.2513,
      "step": 2620
    },
    {
      "epoch": 2.520192307692308,
      "grad_norm": 5.491037845611572,
      "learning_rate": 1.4961538461538463e-05,
      "loss": 0.5586,
      "step": 2621
    },
    {
      "epoch": 2.521153846153846,
      "grad_norm": 11.344278335571289,
      "learning_rate": 1.4959615384615387e-05,
      "loss": 1.4715,
      "step": 2622
    },
    {
      "epoch": 2.5221153846153848,
      "grad_norm": 7.543335437774658,
      "learning_rate": 1.4957692307692308e-05,
      "loss": 0.8391,
      "step": 2623
    },
    {
      "epoch": 2.523076923076923,
      "grad_norm": 6.921133041381836,
      "learning_rate": 1.4955769230769233e-05,
      "loss": 1.4334,
      "step": 2624
    },
    {
      "epoch": 2.5240384615384617,
      "grad_norm": 6.0944390296936035,
      "learning_rate": 1.4953846153846154e-05,
      "loss": 1.8785,
      "step": 2625
    },
    {
      "epoch": 2.525,
      "grad_norm": 6.484961986541748,
      "learning_rate": 1.4951923076923078e-05,
      "loss": 0.8856,
      "step": 2626
    },
    {
      "epoch": 2.5259615384615386,
      "grad_norm": 7.0244646072387695,
      "learning_rate": 1.4950000000000003e-05,
      "loss": 1.2904,
      "step": 2627
    },
    {
      "epoch": 2.526923076923077,
      "grad_norm": 7.5100603103637695,
      "learning_rate": 1.4948076923076924e-05,
      "loss": 1.5809,
      "step": 2628
    },
    {
      "epoch": 2.5278846153846155,
      "grad_norm": 5.958039283752441,
      "learning_rate": 1.4946153846153848e-05,
      "loss": 1.501,
      "step": 2629
    },
    {
      "epoch": 2.5288461538461537,
      "grad_norm": 7.212804794311523,
      "learning_rate": 1.494423076923077e-05,
      "loss": 0.7065,
      "step": 2630
    },
    {
      "epoch": 2.5298076923076924,
      "grad_norm": 7.2000226974487305,
      "learning_rate": 1.4942307692307694e-05,
      "loss": 1.1227,
      "step": 2631
    },
    {
      "epoch": 2.5307692307692307,
      "grad_norm": 7.983600616455078,
      "learning_rate": 1.4940384615384618e-05,
      "loss": 1.3229,
      "step": 2632
    },
    {
      "epoch": 2.5317307692307693,
      "grad_norm": 5.42219352722168,
      "learning_rate": 1.493846153846154e-05,
      "loss": 1.0929,
      "step": 2633
    },
    {
      "epoch": 2.5326923076923076,
      "grad_norm": 6.7444562911987305,
      "learning_rate": 1.4936538461538464e-05,
      "loss": 1.4345,
      "step": 2634
    },
    {
      "epoch": 2.5336538461538463,
      "grad_norm": 6.278599262237549,
      "learning_rate": 1.4934615384615385e-05,
      "loss": 2.0376,
      "step": 2635
    },
    {
      "epoch": 2.5346153846153845,
      "grad_norm": 6.7891058921813965,
      "learning_rate": 1.493269230769231e-05,
      "loss": 1.0187,
      "step": 2636
    },
    {
      "epoch": 2.535576923076923,
      "grad_norm": 5.420001029968262,
      "learning_rate": 1.493076923076923e-05,
      "loss": 1.0225,
      "step": 2637
    },
    {
      "epoch": 2.5365384615384614,
      "grad_norm": 7.951069355010986,
      "learning_rate": 1.4928846153846155e-05,
      "loss": 1.2776,
      "step": 2638
    },
    {
      "epoch": 2.5375,
      "grad_norm": 10.257678985595703,
      "learning_rate": 1.492692307692308e-05,
      "loss": 1.2583,
      "step": 2639
    },
    {
      "epoch": 2.5384615384615383,
      "grad_norm": 12.00333023071289,
      "learning_rate": 1.4925e-05,
      "loss": 0.5642,
      "step": 2640
    },
    {
      "epoch": 2.539423076923077,
      "grad_norm": 9.077402114868164,
      "learning_rate": 1.4923076923076925e-05,
      "loss": 1.0373,
      "step": 2641
    },
    {
      "epoch": 2.5403846153846152,
      "grad_norm": 6.9191789627075195,
      "learning_rate": 1.4921153846153846e-05,
      "loss": 0.9397,
      "step": 2642
    },
    {
      "epoch": 2.541346153846154,
      "grad_norm": 7.091190814971924,
      "learning_rate": 1.491923076923077e-05,
      "loss": 1.0008,
      "step": 2643
    },
    {
      "epoch": 2.542307692307692,
      "grad_norm": 3.753669500350952,
      "learning_rate": 1.4917307692307695e-05,
      "loss": 0.0239,
      "step": 2644
    },
    {
      "epoch": 2.543269230769231,
      "grad_norm": 5.812277317047119,
      "learning_rate": 1.4915384615384616e-05,
      "loss": 1.3313,
      "step": 2645
    },
    {
      "epoch": 2.544230769230769,
      "grad_norm": 6.9285783767700195,
      "learning_rate": 1.491346153846154e-05,
      "loss": 1.1639,
      "step": 2646
    },
    {
      "epoch": 2.5451923076923078,
      "grad_norm": 8.601593971252441,
      "learning_rate": 1.4911538461538461e-05,
      "loss": 1.1695,
      "step": 2647
    },
    {
      "epoch": 2.546153846153846,
      "grad_norm": 5.291069984436035,
      "learning_rate": 1.4909615384615386e-05,
      "loss": 1.5736,
      "step": 2648
    },
    {
      "epoch": 2.5471153846153847,
      "grad_norm": 8.667254447937012,
      "learning_rate": 1.490769230769231e-05,
      "loss": 1.6029,
      "step": 2649
    },
    {
      "epoch": 2.5480769230769234,
      "grad_norm": 7.175508975982666,
      "learning_rate": 1.4905769230769231e-05,
      "loss": 1.1524,
      "step": 2650
    },
    {
      "epoch": 2.5490384615384616,
      "grad_norm": 5.152206897735596,
      "learning_rate": 1.4903846153846156e-05,
      "loss": 1.4342,
      "step": 2651
    },
    {
      "epoch": 2.55,
      "grad_norm": 6.529228687286377,
      "learning_rate": 1.4901923076923077e-05,
      "loss": 1.386,
      "step": 2652
    },
    {
      "epoch": 2.5509615384615385,
      "grad_norm": 10.226892471313477,
      "learning_rate": 1.4900000000000001e-05,
      "loss": 1.1709,
      "step": 2653
    },
    {
      "epoch": 2.551923076923077,
      "grad_norm": 6.8202805519104,
      "learning_rate": 1.4898076923076926e-05,
      "loss": 1.497,
      "step": 2654
    },
    {
      "epoch": 2.5528846153846154,
      "grad_norm": 5.480862140655518,
      "learning_rate": 1.4896153846153847e-05,
      "loss": 0.5234,
      "step": 2655
    },
    {
      "epoch": 2.5538461538461537,
      "grad_norm": 8.000855445861816,
      "learning_rate": 1.4894230769230771e-05,
      "loss": 1.7406,
      "step": 2656
    },
    {
      "epoch": 2.5548076923076923,
      "grad_norm": 5.336875915527344,
      "learning_rate": 1.4892307692307692e-05,
      "loss": 0.9724,
      "step": 2657
    },
    {
      "epoch": 2.555769230769231,
      "grad_norm": 6.063607215881348,
      "learning_rate": 1.4890384615384617e-05,
      "loss": 1.1935,
      "step": 2658
    },
    {
      "epoch": 2.5567307692307693,
      "grad_norm": 7.833964824676514,
      "learning_rate": 1.4888461538461541e-05,
      "loss": 1.1335,
      "step": 2659
    },
    {
      "epoch": 2.5576923076923075,
      "grad_norm": 8.15710735321045,
      "learning_rate": 1.4886538461538462e-05,
      "loss": 2.1159,
      "step": 2660
    },
    {
      "epoch": 2.558653846153846,
      "grad_norm": 7.311864376068115,
      "learning_rate": 1.4884615384615387e-05,
      "loss": 0.405,
      "step": 2661
    },
    {
      "epoch": 2.559615384615385,
      "grad_norm": 6.983717441558838,
      "learning_rate": 1.4882692307692308e-05,
      "loss": 1.2988,
      "step": 2662
    },
    {
      "epoch": 2.560576923076923,
      "grad_norm": 10.610417366027832,
      "learning_rate": 1.4880769230769232e-05,
      "loss": 1.0606,
      "step": 2663
    },
    {
      "epoch": 2.5615384615384613,
      "grad_norm": 6.684065341949463,
      "learning_rate": 1.4878846153846155e-05,
      "loss": 0.8667,
      "step": 2664
    },
    {
      "epoch": 2.5625,
      "grad_norm": 5.976459503173828,
      "learning_rate": 1.4876923076923078e-05,
      "loss": 1.2233,
      "step": 2665
    },
    {
      "epoch": 2.5634615384615387,
      "grad_norm": 7.180835723876953,
      "learning_rate": 1.4875000000000002e-05,
      "loss": 0.5307,
      "step": 2666
    },
    {
      "epoch": 2.564423076923077,
      "grad_norm": 4.707106590270996,
      "learning_rate": 1.4873076923076923e-05,
      "loss": 0.04,
      "step": 2667
    },
    {
      "epoch": 2.565384615384615,
      "grad_norm": 6.556149482727051,
      "learning_rate": 1.4871153846153848e-05,
      "loss": 1.121,
      "step": 2668
    },
    {
      "epoch": 2.566346153846154,
      "grad_norm": 4.9813761711120605,
      "learning_rate": 1.486923076923077e-05,
      "loss": 0.5404,
      "step": 2669
    },
    {
      "epoch": 2.5673076923076925,
      "grad_norm": 5.429473400115967,
      "learning_rate": 1.4867307692307694e-05,
      "loss": 0.4337,
      "step": 2670
    },
    {
      "epoch": 2.5682692307692307,
      "grad_norm": 6.328002452850342,
      "learning_rate": 1.4865384615384616e-05,
      "loss": 0.6468,
      "step": 2671
    },
    {
      "epoch": 2.569230769230769,
      "grad_norm": 7.536169528961182,
      "learning_rate": 1.4863461538461539e-05,
      "loss": 1.2325,
      "step": 2672
    },
    {
      "epoch": 2.5701923076923077,
      "grad_norm": 7.254493713378906,
      "learning_rate": 1.4861538461538464e-05,
      "loss": 1.3575,
      "step": 2673
    },
    {
      "epoch": 2.5711538461538463,
      "grad_norm": 7.152688980102539,
      "learning_rate": 1.4859615384615386e-05,
      "loss": 0.4189,
      "step": 2674
    },
    {
      "epoch": 2.5721153846153846,
      "grad_norm": 6.1720404624938965,
      "learning_rate": 1.4857692307692309e-05,
      "loss": 0.5617,
      "step": 2675
    },
    {
      "epoch": 2.573076923076923,
      "grad_norm": 5.842126846313477,
      "learning_rate": 1.4855769230769232e-05,
      "loss": 1.6909,
      "step": 2676
    },
    {
      "epoch": 2.5740384615384615,
      "grad_norm": 3.6966164112091064,
      "learning_rate": 1.4853846153846155e-05,
      "loss": 0.0272,
      "step": 2677
    },
    {
      "epoch": 2.575,
      "grad_norm": 9.667275428771973,
      "learning_rate": 1.4851923076923079e-05,
      "loss": 1.2203,
      "step": 2678
    },
    {
      "epoch": 2.5759615384615384,
      "grad_norm": 7.398708820343018,
      "learning_rate": 1.4850000000000002e-05,
      "loss": 1.6871,
      "step": 2679
    },
    {
      "epoch": 2.5769230769230766,
      "grad_norm": 5.757925033569336,
      "learning_rate": 1.4848076923076925e-05,
      "loss": 1.478,
      "step": 2680
    },
    {
      "epoch": 2.5778846153846153,
      "grad_norm": 4.698930740356445,
      "learning_rate": 1.4846153846153847e-05,
      "loss": 1.1566,
      "step": 2681
    },
    {
      "epoch": 2.578846153846154,
      "grad_norm": 5.281269550323486,
      "learning_rate": 1.484423076923077e-05,
      "loss": 0.5114,
      "step": 2682
    },
    {
      "epoch": 2.5798076923076922,
      "grad_norm": 6.892308235168457,
      "learning_rate": 1.4842307692307693e-05,
      "loss": 1.0587,
      "step": 2683
    },
    {
      "epoch": 2.580769230769231,
      "grad_norm": 6.741802215576172,
      "learning_rate": 1.4840384615384617e-05,
      "loss": 1.0748,
      "step": 2684
    },
    {
      "epoch": 2.581730769230769,
      "grad_norm": 9.004743576049805,
      "learning_rate": 1.483846153846154e-05,
      "loss": 1.2863,
      "step": 2685
    },
    {
      "epoch": 2.582692307692308,
      "grad_norm": 6.969964027404785,
      "learning_rate": 1.4836538461538463e-05,
      "loss": 1.0991,
      "step": 2686
    },
    {
      "epoch": 2.583653846153846,
      "grad_norm": 5.261065483093262,
      "learning_rate": 1.4834615384615386e-05,
      "loss": 0.0572,
      "step": 2687
    },
    {
      "epoch": 2.5846153846153848,
      "grad_norm": 7.070499897003174,
      "learning_rate": 1.4832692307692308e-05,
      "loss": 1.3519,
      "step": 2688
    },
    {
      "epoch": 2.585576923076923,
      "grad_norm": 5.866091251373291,
      "learning_rate": 1.4830769230769233e-05,
      "loss": 0.9212,
      "step": 2689
    },
    {
      "epoch": 2.5865384615384617,
      "grad_norm": 14.245523452758789,
      "learning_rate": 1.4828846153846154e-05,
      "loss": 1.5773,
      "step": 2690
    },
    {
      "epoch": 2.5875,
      "grad_norm": 9.568825721740723,
      "learning_rate": 1.4826923076923078e-05,
      "loss": 1.3128,
      "step": 2691
    },
    {
      "epoch": 2.5884615384615386,
      "grad_norm": 5.6560492515563965,
      "learning_rate": 1.4825000000000001e-05,
      "loss": 1.7782,
      "step": 2692
    },
    {
      "epoch": 2.589423076923077,
      "grad_norm": 9.39367389678955,
      "learning_rate": 1.4823076923076924e-05,
      "loss": 0.3005,
      "step": 2693
    },
    {
      "epoch": 2.5903846153846155,
      "grad_norm": 6.577274322509766,
      "learning_rate": 1.4821153846153848e-05,
      "loss": 1.4236,
      "step": 2694
    },
    {
      "epoch": 2.5913461538461537,
      "grad_norm": 7.121721267700195,
      "learning_rate": 1.481923076923077e-05,
      "loss": 1.0342,
      "step": 2695
    },
    {
      "epoch": 2.5923076923076924,
      "grad_norm": 10.962276458740234,
      "learning_rate": 1.4817307692307694e-05,
      "loss": 0.6529,
      "step": 2696
    },
    {
      "epoch": 2.5932692307692307,
      "grad_norm": 7.515076160430908,
      "learning_rate": 1.4815384615384617e-05,
      "loss": 1.0602,
      "step": 2697
    },
    {
      "epoch": 2.5942307692307693,
      "grad_norm": 7.402872562408447,
      "learning_rate": 1.481346153846154e-05,
      "loss": 0.8981,
      "step": 2698
    },
    {
      "epoch": 2.5951923076923076,
      "grad_norm": 8.488374710083008,
      "learning_rate": 1.4811538461538462e-05,
      "loss": 0.5512,
      "step": 2699
    },
    {
      "epoch": 2.5961538461538463,
      "grad_norm": 13.415262222290039,
      "learning_rate": 1.4809615384615385e-05,
      "loss": 1.7892,
      "step": 2700
    },
    {
      "epoch": 2.5971153846153845,
      "grad_norm": 6.579999923706055,
      "learning_rate": 1.480769230769231e-05,
      "loss": 0.4333,
      "step": 2701
    },
    {
      "epoch": 2.598076923076923,
      "grad_norm": 9.534201622009277,
      "learning_rate": 1.480576923076923e-05,
      "loss": 1.429,
      "step": 2702
    },
    {
      "epoch": 2.5990384615384614,
      "grad_norm": 10.225914001464844,
      "learning_rate": 1.4803846153846155e-05,
      "loss": 0.9864,
      "step": 2703
    },
    {
      "epoch": 2.6,
      "grad_norm": 7.561662197113037,
      "learning_rate": 1.4801923076923078e-05,
      "loss": 0.5387,
      "step": 2704
    },
    {
      "epoch": 2.6009615384615383,
      "grad_norm": 6.717405796051025,
      "learning_rate": 1.48e-05,
      "loss": 0.9718,
      "step": 2705
    },
    {
      "epoch": 2.601923076923077,
      "grad_norm": 5.830386161804199,
      "learning_rate": 1.4798076923076925e-05,
      "loss": 0.7329,
      "step": 2706
    },
    {
      "epoch": 2.6028846153846152,
      "grad_norm": 9.198118209838867,
      "learning_rate": 1.4796153846153846e-05,
      "loss": 1.2279,
      "step": 2707
    },
    {
      "epoch": 2.603846153846154,
      "grad_norm": 9.173284530639648,
      "learning_rate": 1.479423076923077e-05,
      "loss": 0.1446,
      "step": 2708
    },
    {
      "epoch": 2.604807692307692,
      "grad_norm": 6.630289077758789,
      "learning_rate": 1.4792307692307692e-05,
      "loss": 1.9674,
      "step": 2709
    },
    {
      "epoch": 2.605769230769231,
      "grad_norm": 7.0158843994140625,
      "learning_rate": 1.4790384615384616e-05,
      "loss": 1.0241,
      "step": 2710
    },
    {
      "epoch": 2.606730769230769,
      "grad_norm": 5.641491413116455,
      "learning_rate": 1.478846153846154e-05,
      "loss": 1.4266,
      "step": 2711
    },
    {
      "epoch": 2.6076923076923078,
      "grad_norm": 6.613699913024902,
      "learning_rate": 1.4786538461538462e-05,
      "loss": 1.6487,
      "step": 2712
    },
    {
      "epoch": 2.608653846153846,
      "grad_norm": 5.4943037033081055,
      "learning_rate": 1.4784615384615386e-05,
      "loss": 0.7433,
      "step": 2713
    },
    {
      "epoch": 2.6096153846153847,
      "grad_norm": 6.887964725494385,
      "learning_rate": 1.4782692307692307e-05,
      "loss": 1.4869,
      "step": 2714
    },
    {
      "epoch": 2.6105769230769234,
      "grad_norm": 8.925804138183594,
      "learning_rate": 1.4780769230769232e-05,
      "loss": 1.7566,
      "step": 2715
    },
    {
      "epoch": 2.6115384615384616,
      "grad_norm": 5.933945655822754,
      "learning_rate": 1.4778846153846156e-05,
      "loss": 0.5227,
      "step": 2716
    },
    {
      "epoch": 2.6125,
      "grad_norm": 3.609348773956299,
      "learning_rate": 1.4776923076923077e-05,
      "loss": 0.0406,
      "step": 2717
    },
    {
      "epoch": 2.6134615384615385,
      "grad_norm": 5.923745155334473,
      "learning_rate": 1.4775000000000002e-05,
      "loss": 1.1505,
      "step": 2718
    },
    {
      "epoch": 2.614423076923077,
      "grad_norm": 7.33380126953125,
      "learning_rate": 1.4773076923076923e-05,
      "loss": 1.009,
      "step": 2719
    },
    {
      "epoch": 2.6153846153846154,
      "grad_norm": 6.283395290374756,
      "learning_rate": 1.4771153846153847e-05,
      "loss": 0.2322,
      "step": 2720
    },
    {
      "epoch": 2.6163461538461537,
      "grad_norm": 6.4134392738342285,
      "learning_rate": 1.4769230769230772e-05,
      "loss": 1.3582,
      "step": 2721
    },
    {
      "epoch": 2.6173076923076923,
      "grad_norm": 10.34315299987793,
      "learning_rate": 1.4767307692307693e-05,
      "loss": 1.2807,
      "step": 2722
    },
    {
      "epoch": 2.618269230769231,
      "grad_norm": 5.134404182434082,
      "learning_rate": 1.4765384615384617e-05,
      "loss": 1.1302,
      "step": 2723
    },
    {
      "epoch": 2.6192307692307693,
      "grad_norm": 7.773992538452148,
      "learning_rate": 1.4763461538461538e-05,
      "loss": 0.9274,
      "step": 2724
    },
    {
      "epoch": 2.6201923076923075,
      "grad_norm": 8.219335556030273,
      "learning_rate": 1.4761538461538463e-05,
      "loss": 0.4784,
      "step": 2725
    },
    {
      "epoch": 2.621153846153846,
      "grad_norm": 7.681543827056885,
      "learning_rate": 1.4759615384615387e-05,
      "loss": 1.4856,
      "step": 2726
    },
    {
      "epoch": 2.622115384615385,
      "grad_norm": 8.613635063171387,
      "learning_rate": 1.4757692307692308e-05,
      "loss": 2.1875,
      "step": 2727
    },
    {
      "epoch": 2.623076923076923,
      "grad_norm": 5.842676639556885,
      "learning_rate": 1.4755769230769233e-05,
      "loss": 0.8875,
      "step": 2728
    },
    {
      "epoch": 2.6240384615384613,
      "grad_norm": 4.641250133514404,
      "learning_rate": 1.4753846153846154e-05,
      "loss": 0.7917,
      "step": 2729
    },
    {
      "epoch": 2.625,
      "grad_norm": 7.555029392242432,
      "learning_rate": 1.4751923076923078e-05,
      "loss": 1.4935,
      "step": 2730
    },
    {
      "epoch": 2.6259615384615387,
      "grad_norm": 7.555189609527588,
      "learning_rate": 1.4750000000000003e-05,
      "loss": 0.939,
      "step": 2731
    },
    {
      "epoch": 2.626923076923077,
      "grad_norm": 7.500111103057861,
      "learning_rate": 1.4748076923076924e-05,
      "loss": 1.5229,
      "step": 2732
    },
    {
      "epoch": 2.627884615384615,
      "grad_norm": 6.1259446144104,
      "learning_rate": 1.4746153846153848e-05,
      "loss": 0.8811,
      "step": 2733
    },
    {
      "epoch": 2.628846153846154,
      "grad_norm": 7.318794250488281,
      "learning_rate": 1.474423076923077e-05,
      "loss": 0.5888,
      "step": 2734
    },
    {
      "epoch": 2.6298076923076925,
      "grad_norm": 8.662911415100098,
      "learning_rate": 1.4742307692307694e-05,
      "loss": 1.0149,
      "step": 2735
    },
    {
      "epoch": 2.6307692307692307,
      "grad_norm": 7.798264503479004,
      "learning_rate": 1.4740384615384618e-05,
      "loss": 1.3203,
      "step": 2736
    },
    {
      "epoch": 2.631730769230769,
      "grad_norm": 6.416342735290527,
      "learning_rate": 1.473846153846154e-05,
      "loss": 0.9133,
      "step": 2737
    },
    {
      "epoch": 2.6326923076923077,
      "grad_norm": 7.680052280426025,
      "learning_rate": 1.4736538461538464e-05,
      "loss": 1.3031,
      "step": 2738
    },
    {
      "epoch": 2.6336538461538463,
      "grad_norm": 1.0501856803894043,
      "learning_rate": 1.4734615384615385e-05,
      "loss": 0.0087,
      "step": 2739
    },
    {
      "epoch": 2.6346153846153846,
      "grad_norm": 9.503276824951172,
      "learning_rate": 1.473269230769231e-05,
      "loss": 1.3645,
      "step": 2740
    },
    {
      "epoch": 2.635576923076923,
      "grad_norm": 6.549286365509033,
      "learning_rate": 1.4730769230769232e-05,
      "loss": 0.6844,
      "step": 2741
    },
    {
      "epoch": 2.6365384615384615,
      "grad_norm": 7.261868000030518,
      "learning_rate": 1.4728846153846155e-05,
      "loss": 1.7516,
      "step": 2742
    },
    {
      "epoch": 2.6375,
      "grad_norm": 7.6128926277160645,
      "learning_rate": 1.472692307692308e-05,
      "loss": 0.9517,
      "step": 2743
    },
    {
      "epoch": 2.6384615384615384,
      "grad_norm": 6.205256938934326,
      "learning_rate": 1.4725e-05,
      "loss": 0.7984,
      "step": 2744
    },
    {
      "epoch": 2.6394230769230766,
      "grad_norm": 7.633115768432617,
      "learning_rate": 1.4723076923076925e-05,
      "loss": 0.776,
      "step": 2745
    },
    {
      "epoch": 2.6403846153846153,
      "grad_norm": 6.349609851837158,
      "learning_rate": 1.4721153846153848e-05,
      "loss": 0.7336,
      "step": 2746
    },
    {
      "epoch": 2.641346153846154,
      "grad_norm": 6.161956787109375,
      "learning_rate": 1.471923076923077e-05,
      "loss": 0.1886,
      "step": 2747
    },
    {
      "epoch": 2.6423076923076922,
      "grad_norm": 7.326989650726318,
      "learning_rate": 1.4717307692307693e-05,
      "loss": 1.4951,
      "step": 2748
    },
    {
      "epoch": 2.643269230769231,
      "grad_norm": 9.554471015930176,
      "learning_rate": 1.4715384615384616e-05,
      "loss": 0.7528,
      "step": 2749
    },
    {
      "epoch": 2.644230769230769,
      "grad_norm": 8.434410095214844,
      "learning_rate": 1.471346153846154e-05,
      "loss": 1.4569,
      "step": 2750
    },
    {
      "epoch": 2.645192307692308,
      "grad_norm": 9.209571838378906,
      "learning_rate": 1.4711538461538463e-05,
      "loss": 1.1416,
      "step": 2751
    },
    {
      "epoch": 2.646153846153846,
      "grad_norm": 10.949616432189941,
      "learning_rate": 1.4709615384615386e-05,
      "loss": 2.1275,
      "step": 2752
    },
    {
      "epoch": 2.6471153846153848,
      "grad_norm": 7.611222267150879,
      "learning_rate": 1.4707692307692309e-05,
      "loss": 1.46,
      "step": 2753
    },
    {
      "epoch": 2.648076923076923,
      "grad_norm": 7.767470836639404,
      "learning_rate": 1.4705769230769232e-05,
      "loss": 1.218,
      "step": 2754
    },
    {
      "epoch": 2.6490384615384617,
      "grad_norm": 6.423694133758545,
      "learning_rate": 1.4703846153846156e-05,
      "loss": 1.4825,
      "step": 2755
    },
    {
      "epoch": 2.65,
      "grad_norm": 6.004734516143799,
      "learning_rate": 1.4701923076923079e-05,
      "loss": 1.0806,
      "step": 2756
    },
    {
      "epoch": 2.6509615384615386,
      "grad_norm": 11.331968307495117,
      "learning_rate": 1.4700000000000002e-05,
      "loss": 1.0782,
      "step": 2757
    },
    {
      "epoch": 2.651923076923077,
      "grad_norm": 5.310741901397705,
      "learning_rate": 1.4698076923076924e-05,
      "loss": 1.3733,
      "step": 2758
    },
    {
      "epoch": 2.6528846153846155,
      "grad_norm": 11.314741134643555,
      "learning_rate": 1.4696153846153847e-05,
      "loss": 1.309,
      "step": 2759
    },
    {
      "epoch": 2.6538461538461537,
      "grad_norm": 6.735047817230225,
      "learning_rate": 1.469423076923077e-05,
      "loss": 1.1439,
      "step": 2760
    },
    {
      "epoch": 2.6548076923076924,
      "grad_norm": 3.9450223445892334,
      "learning_rate": 1.4692307692307694e-05,
      "loss": 0.1598,
      "step": 2761
    },
    {
      "epoch": 2.6557692307692307,
      "grad_norm": 6.682232856750488,
      "learning_rate": 1.4690384615384617e-05,
      "loss": 0.98,
      "step": 2762
    },
    {
      "epoch": 2.6567307692307693,
      "grad_norm": 5.541898250579834,
      "learning_rate": 1.468846153846154e-05,
      "loss": 0.9591,
      "step": 2763
    },
    {
      "epoch": 2.6576923076923076,
      "grad_norm": 7.958212375640869,
      "learning_rate": 1.4686538461538463e-05,
      "loss": 0.2159,
      "step": 2764
    },
    {
      "epoch": 2.6586538461538463,
      "grad_norm": 5.209351062774658,
      "learning_rate": 1.4684615384615385e-05,
      "loss": 0.2343,
      "step": 2765
    },
    {
      "epoch": 2.6596153846153845,
      "grad_norm": 8.085962295532227,
      "learning_rate": 1.4682692307692308e-05,
      "loss": 0.6946,
      "step": 2766
    },
    {
      "epoch": 2.660576923076923,
      "grad_norm": 6.748722076416016,
      "learning_rate": 1.4680769230769231e-05,
      "loss": 1.0101,
      "step": 2767
    },
    {
      "epoch": 2.6615384615384614,
      "grad_norm": 4.491825103759766,
      "learning_rate": 1.4678846153846155e-05,
      "loss": 1.6428,
      "step": 2768
    },
    {
      "epoch": 2.6625,
      "grad_norm": 5.8404717445373535,
      "learning_rate": 1.4676923076923078e-05,
      "loss": 0.2122,
      "step": 2769
    },
    {
      "epoch": 2.6634615384615383,
      "grad_norm": 7.431770324707031,
      "learning_rate": 1.4675000000000001e-05,
      "loss": 0.9715,
      "step": 2770
    },
    {
      "epoch": 2.664423076923077,
      "grad_norm": 7.053267478942871,
      "learning_rate": 1.4673076923076924e-05,
      "loss": 1.4351,
      "step": 2771
    },
    {
      "epoch": 2.6653846153846152,
      "grad_norm": 22.498470306396484,
      "learning_rate": 1.4671153846153846e-05,
      "loss": 0.4331,
      "step": 2772
    },
    {
      "epoch": 2.666346153846154,
      "grad_norm": 9.400181770324707,
      "learning_rate": 1.4669230769230771e-05,
      "loss": 0.5517,
      "step": 2773
    },
    {
      "epoch": 2.667307692307692,
      "grad_norm": 5.2625508308410645,
      "learning_rate": 1.4667307692307694e-05,
      "loss": 0.3929,
      "step": 2774
    },
    {
      "epoch": 2.668269230769231,
      "grad_norm": 5.747198104858398,
      "learning_rate": 1.4665384615384616e-05,
      "loss": 1.7388,
      "step": 2775
    },
    {
      "epoch": 2.669230769230769,
      "grad_norm": 0.3038317859172821,
      "learning_rate": 1.466346153846154e-05,
      "loss": 0.0023,
      "step": 2776
    },
    {
      "epoch": 2.6701923076923078,
      "grad_norm": 5.279207229614258,
      "learning_rate": 1.4661538461538462e-05,
      "loss": 1.6927,
      "step": 2777
    },
    {
      "epoch": 2.671153846153846,
      "grad_norm": 11.255815505981445,
      "learning_rate": 1.4659615384615386e-05,
      "loss": 0.4021,
      "step": 2778
    },
    {
      "epoch": 2.6721153846153847,
      "grad_norm": 9.890560150146484,
      "learning_rate": 1.4657692307692308e-05,
      "loss": 1.6656,
      "step": 2779
    },
    {
      "epoch": 2.6730769230769234,
      "grad_norm": 12.603599548339844,
      "learning_rate": 1.4655769230769232e-05,
      "loss": 1.2829,
      "step": 2780
    },
    {
      "epoch": 2.6740384615384616,
      "grad_norm": 5.366214752197266,
      "learning_rate": 1.4653846153846155e-05,
      "loss": 1.29,
      "step": 2781
    },
    {
      "epoch": 2.675,
      "grad_norm": 7.772274494171143,
      "learning_rate": 1.4651923076923078e-05,
      "loss": 1.0691,
      "step": 2782
    },
    {
      "epoch": 2.6759615384615385,
      "grad_norm": 7.505376815795898,
      "learning_rate": 1.4650000000000002e-05,
      "loss": 0.7351,
      "step": 2783
    },
    {
      "epoch": 2.676923076923077,
      "grad_norm": 8.630126953125,
      "learning_rate": 1.4648076923076923e-05,
      "loss": 1.0107,
      "step": 2784
    },
    {
      "epoch": 2.6778846153846154,
      "grad_norm": 4.723398208618164,
      "learning_rate": 1.4646153846153848e-05,
      "loss": 0.0512,
      "step": 2785
    },
    {
      "epoch": 2.6788461538461537,
      "grad_norm": 7.434558391571045,
      "learning_rate": 1.4644230769230769e-05,
      "loss": 1.4595,
      "step": 2786
    },
    {
      "epoch": 2.6798076923076923,
      "grad_norm": 6.672041893005371,
      "learning_rate": 1.4642307692307693e-05,
      "loss": 1.1883,
      "step": 2787
    },
    {
      "epoch": 2.680769230769231,
      "grad_norm": 14.435195922851562,
      "learning_rate": 1.4640384615384618e-05,
      "loss": 0.545,
      "step": 2788
    },
    {
      "epoch": 2.6817307692307693,
      "grad_norm": 11.378100395202637,
      "learning_rate": 1.4638461538461539e-05,
      "loss": 0.6363,
      "step": 2789
    },
    {
      "epoch": 2.6826923076923075,
      "grad_norm": 8.05688190460205,
      "learning_rate": 1.4636538461538463e-05,
      "loss": 2.3287,
      "step": 2790
    },
    {
      "epoch": 2.683653846153846,
      "grad_norm": 8.89818000793457,
      "learning_rate": 1.4634615384615384e-05,
      "loss": 1.6579,
      "step": 2791
    },
    {
      "epoch": 2.684615384615385,
      "grad_norm": 11.095808029174805,
      "learning_rate": 1.4632692307692309e-05,
      "loss": 1.0667,
      "step": 2792
    },
    {
      "epoch": 2.685576923076923,
      "grad_norm": 16.97924041748047,
      "learning_rate": 1.4630769230769233e-05,
      "loss": 0.4338,
      "step": 2793
    },
    {
      "epoch": 2.6865384615384613,
      "grad_norm": 9.850091934204102,
      "learning_rate": 1.4628846153846154e-05,
      "loss": 1.1742,
      "step": 2794
    },
    {
      "epoch": 2.6875,
      "grad_norm": 8.623310089111328,
      "learning_rate": 1.4626923076923079e-05,
      "loss": 1.0925,
      "step": 2795
    },
    {
      "epoch": 2.6884615384615387,
      "grad_norm": 6.742800712585449,
      "learning_rate": 1.4625e-05,
      "loss": 1.3755,
      "step": 2796
    },
    {
      "epoch": 2.689423076923077,
      "grad_norm": 5.591403007507324,
      "learning_rate": 1.4623076923076924e-05,
      "loss": 1.5673,
      "step": 2797
    },
    {
      "epoch": 2.690384615384615,
      "grad_norm": 9.700176239013672,
      "learning_rate": 1.4621153846153849e-05,
      "loss": 1.2463,
      "step": 2798
    },
    {
      "epoch": 2.691346153846154,
      "grad_norm": 15.564559936523438,
      "learning_rate": 1.461923076923077e-05,
      "loss": 0.3823,
      "step": 2799
    },
    {
      "epoch": 2.6923076923076925,
      "grad_norm": 5.166794300079346,
      "learning_rate": 1.4617307692307694e-05,
      "loss": 1.3427,
      "step": 2800
    },
    {
      "epoch": 2.6932692307692307,
      "grad_norm": 6.9399518966674805,
      "learning_rate": 1.4615384615384615e-05,
      "loss": 1.6792,
      "step": 2801
    },
    {
      "epoch": 2.694230769230769,
      "grad_norm": 10.434247016906738,
      "learning_rate": 1.461346153846154e-05,
      "loss": 1.4703,
      "step": 2802
    },
    {
      "epoch": 2.6951923076923077,
      "grad_norm": 7.94122838973999,
      "learning_rate": 1.4611538461538464e-05,
      "loss": 1.1764,
      "step": 2803
    },
    {
      "epoch": 2.6961538461538463,
      "grad_norm": 7.151255130767822,
      "learning_rate": 1.4609615384615385e-05,
      "loss": 1.559,
      "step": 2804
    },
    {
      "epoch": 2.6971153846153846,
      "grad_norm": 5.308572292327881,
      "learning_rate": 1.460769230769231e-05,
      "loss": 1.4446,
      "step": 2805
    },
    {
      "epoch": 2.698076923076923,
      "grad_norm": 7.803584575653076,
      "learning_rate": 1.460576923076923e-05,
      "loss": 1.3677,
      "step": 2806
    },
    {
      "epoch": 2.6990384615384615,
      "grad_norm": 2.727543830871582,
      "learning_rate": 1.4603846153846155e-05,
      "loss": 0.0213,
      "step": 2807
    },
    {
      "epoch": 2.7,
      "grad_norm": 12.543903350830078,
      "learning_rate": 1.460192307692308e-05,
      "loss": 0.1279,
      "step": 2808
    },
    {
      "epoch": 2.7009615384615384,
      "grad_norm": 7.8319926261901855,
      "learning_rate": 1.46e-05,
      "loss": 1.5307,
      "step": 2809
    },
    {
      "epoch": 2.7019230769230766,
      "grad_norm": 7.071638107299805,
      "learning_rate": 1.4598076923076925e-05,
      "loss": 1.1514,
      "step": 2810
    },
    {
      "epoch": 2.7028846153846153,
      "grad_norm": 4.98270320892334,
      "learning_rate": 1.4596153846153846e-05,
      "loss": 1.1641,
      "step": 2811
    },
    {
      "epoch": 2.703846153846154,
      "grad_norm": 3.4894816875457764,
      "learning_rate": 1.459423076923077e-05,
      "loss": 0.1748,
      "step": 2812
    },
    {
      "epoch": 2.7048076923076922,
      "grad_norm": 9.566900253295898,
      "learning_rate": 1.4592307692307695e-05,
      "loss": 1.061,
      "step": 2813
    },
    {
      "epoch": 2.705769230769231,
      "grad_norm": 10.015462875366211,
      "learning_rate": 1.4590384615384616e-05,
      "loss": 1.5365,
      "step": 2814
    },
    {
      "epoch": 2.706730769230769,
      "grad_norm": 8.817849159240723,
      "learning_rate": 1.458846153846154e-05,
      "loss": 0.7549,
      "step": 2815
    },
    {
      "epoch": 2.707692307692308,
      "grad_norm": 8.865824699401855,
      "learning_rate": 1.4586538461538462e-05,
      "loss": 0.738,
      "step": 2816
    },
    {
      "epoch": 2.708653846153846,
      "grad_norm": 7.734373092651367,
      "learning_rate": 1.4584615384615386e-05,
      "loss": 1.0615,
      "step": 2817
    },
    {
      "epoch": 2.7096153846153848,
      "grad_norm": 4.882964611053467,
      "learning_rate": 1.4582692307692309e-05,
      "loss": 0.1049,
      "step": 2818
    },
    {
      "epoch": 2.710576923076923,
      "grad_norm": 6.894124984741211,
      "learning_rate": 1.4580769230769232e-05,
      "loss": 1.5194,
      "step": 2819
    },
    {
      "epoch": 2.7115384615384617,
      "grad_norm": 3.314626932144165,
      "learning_rate": 1.4578846153846156e-05,
      "loss": 0.0465,
      "step": 2820
    },
    {
      "epoch": 2.7125,
      "grad_norm": 3.1530067920684814,
      "learning_rate": 1.4576923076923077e-05,
      "loss": 0.046,
      "step": 2821
    },
    {
      "epoch": 2.7134615384615386,
      "grad_norm": 24.96685028076172,
      "learning_rate": 1.4575000000000002e-05,
      "loss": 1.135,
      "step": 2822
    },
    {
      "epoch": 2.714423076923077,
      "grad_norm": 9.682369232177734,
      "learning_rate": 1.4573076923076925e-05,
      "loss": 1.0645,
      "step": 2823
    },
    {
      "epoch": 2.7153846153846155,
      "grad_norm": 4.435543537139893,
      "learning_rate": 1.4571153846153847e-05,
      "loss": 1.3104,
      "step": 2824
    },
    {
      "epoch": 2.7163461538461537,
      "grad_norm": 20.077884674072266,
      "learning_rate": 1.4569230769230772e-05,
      "loss": 1.6075,
      "step": 2825
    },
    {
      "epoch": 2.7173076923076924,
      "grad_norm": 6.0779032707214355,
      "learning_rate": 1.4567307692307693e-05,
      "loss": 0.7609,
      "step": 2826
    },
    {
      "epoch": 2.7182692307692307,
      "grad_norm": 6.741405010223389,
      "learning_rate": 1.4565384615384617e-05,
      "loss": 1.4862,
      "step": 2827
    },
    {
      "epoch": 2.7192307692307693,
      "grad_norm": 6.2420196533203125,
      "learning_rate": 1.456346153846154e-05,
      "loss": 0.8452,
      "step": 2828
    },
    {
      "epoch": 2.7201923076923076,
      "grad_norm": 5.26533317565918,
      "learning_rate": 1.4561538461538463e-05,
      "loss": 1.6254,
      "step": 2829
    },
    {
      "epoch": 2.7211538461538463,
      "grad_norm": 8.904937744140625,
      "learning_rate": 1.4559615384615386e-05,
      "loss": 0.9206,
      "step": 2830
    },
    {
      "epoch": 2.7221153846153845,
      "grad_norm": 4.91141939163208,
      "learning_rate": 1.4557692307692309e-05,
      "loss": 0.0422,
      "step": 2831
    },
    {
      "epoch": 2.723076923076923,
      "grad_norm": 6.133670330047607,
      "learning_rate": 1.4555769230769233e-05,
      "loss": 1.394,
      "step": 2832
    },
    {
      "epoch": 2.7240384615384614,
      "grad_norm": 6.139618396759033,
      "learning_rate": 1.4553846153846154e-05,
      "loss": 0.8246,
      "step": 2833
    },
    {
      "epoch": 2.725,
      "grad_norm": 11.816710472106934,
      "learning_rate": 1.4551923076923079e-05,
      "loss": 0.9063,
      "step": 2834
    },
    {
      "epoch": 2.7259615384615383,
      "grad_norm": 11.751398086547852,
      "learning_rate": 1.4550000000000001e-05,
      "loss": 1.3333,
      "step": 2835
    },
    {
      "epoch": 2.726923076923077,
      "grad_norm": 7.285253524780273,
      "learning_rate": 1.4548076923076924e-05,
      "loss": 1.4449,
      "step": 2836
    },
    {
      "epoch": 2.7278846153846152,
      "grad_norm": 6.6484575271606445,
      "learning_rate": 1.4546153846153847e-05,
      "loss": 1.0588,
      "step": 2837
    },
    {
      "epoch": 2.728846153846154,
      "grad_norm": 8.182372093200684,
      "learning_rate": 1.454423076923077e-05,
      "loss": 0.0815,
      "step": 2838
    },
    {
      "epoch": 2.729807692307692,
      "grad_norm": 2.87222957611084,
      "learning_rate": 1.4542307692307694e-05,
      "loss": 0.0174,
      "step": 2839
    },
    {
      "epoch": 2.730769230769231,
      "grad_norm": 7.2374491691589355,
      "learning_rate": 1.4540384615384617e-05,
      "loss": 1.4336,
      "step": 2840
    },
    {
      "epoch": 2.731730769230769,
      "grad_norm": 7.45576286315918,
      "learning_rate": 1.453846153846154e-05,
      "loss": 1.2225,
      "step": 2841
    },
    {
      "epoch": 2.7326923076923078,
      "grad_norm": 7.6227898597717285,
      "learning_rate": 1.4536538461538462e-05,
      "loss": 0.9968,
      "step": 2842
    },
    {
      "epoch": 2.733653846153846,
      "grad_norm": 8.964864730834961,
      "learning_rate": 1.4534615384615385e-05,
      "loss": 1.3578,
      "step": 2843
    },
    {
      "epoch": 2.7346153846153847,
      "grad_norm": 7.6152024269104,
      "learning_rate": 1.4532692307692308e-05,
      "loss": 1.5736,
      "step": 2844
    },
    {
      "epoch": 2.7355769230769234,
      "grad_norm": 11.053255081176758,
      "learning_rate": 1.4530769230769232e-05,
      "loss": 0.7413,
      "step": 2845
    },
    {
      "epoch": 2.7365384615384616,
      "grad_norm": 7.856919765472412,
      "learning_rate": 1.4528846153846155e-05,
      "loss": 1.3096,
      "step": 2846
    },
    {
      "epoch": 2.7375,
      "grad_norm": 5.519260883331299,
      "learning_rate": 1.4526923076923078e-05,
      "loss": 1.6538,
      "step": 2847
    },
    {
      "epoch": 2.7384615384615385,
      "grad_norm": 5.675639629364014,
      "learning_rate": 1.4525e-05,
      "loss": 1.0372,
      "step": 2848
    },
    {
      "epoch": 2.739423076923077,
      "grad_norm": 6.510134696960449,
      "learning_rate": 1.4523076923076923e-05,
      "loss": 1.1023,
      "step": 2849
    },
    {
      "epoch": 2.7403846153846154,
      "grad_norm": 6.259585380554199,
      "learning_rate": 1.4521153846153848e-05,
      "loss": 0.9277,
      "step": 2850
    },
    {
      "epoch": 2.7413461538461537,
      "grad_norm": 6.590251445770264,
      "learning_rate": 1.451923076923077e-05,
      "loss": 0.5852,
      "step": 2851
    },
    {
      "epoch": 2.7423076923076923,
      "grad_norm": 5.57434606552124,
      "learning_rate": 1.4517307692307693e-05,
      "loss": 1.7149,
      "step": 2852
    },
    {
      "epoch": 2.743269230769231,
      "grad_norm": 7.748754978179932,
      "learning_rate": 1.4515384615384616e-05,
      "loss": 1.2263,
      "step": 2853
    },
    {
      "epoch": 2.7442307692307693,
      "grad_norm": 6.7162017822265625,
      "learning_rate": 1.4513461538461539e-05,
      "loss": 1.6443,
      "step": 2854
    },
    {
      "epoch": 2.7451923076923075,
      "grad_norm": 7.476668357849121,
      "learning_rate": 1.4511538461538463e-05,
      "loss": 0.7245,
      "step": 2855
    },
    {
      "epoch": 2.746153846153846,
      "grad_norm": 6.241189479827881,
      "learning_rate": 1.4509615384615385e-05,
      "loss": 1.6335,
      "step": 2856
    },
    {
      "epoch": 2.747115384615385,
      "grad_norm": 12.037490844726562,
      "learning_rate": 1.4507692307692309e-05,
      "loss": 1.305,
      "step": 2857
    },
    {
      "epoch": 2.748076923076923,
      "grad_norm": 9.810636520385742,
      "learning_rate": 1.4505769230769232e-05,
      "loss": 0.9855,
      "step": 2858
    },
    {
      "epoch": 2.7490384615384613,
      "grad_norm": 7.96348237991333,
      "learning_rate": 1.4503846153846155e-05,
      "loss": 0.8566,
      "step": 2859
    },
    {
      "epoch": 2.75,
      "grad_norm": 10.107809066772461,
      "learning_rate": 1.4501923076923079e-05,
      "loss": 1.0192,
      "step": 2860
    },
    {
      "epoch": 2.7509615384615387,
      "grad_norm": 5.989548683166504,
      "learning_rate": 1.45e-05,
      "loss": 1.8573,
      "step": 2861
    },
    {
      "epoch": 2.751923076923077,
      "grad_norm": 6.177335739135742,
      "learning_rate": 1.4498076923076925e-05,
      "loss": 1.5007,
      "step": 2862
    },
    {
      "epoch": 2.752884615384615,
      "grad_norm": 8.185067176818848,
      "learning_rate": 1.4496153846153846e-05,
      "loss": 1.3013,
      "step": 2863
    },
    {
      "epoch": 2.753846153846154,
      "grad_norm": 6.513592720031738,
      "learning_rate": 1.449423076923077e-05,
      "loss": 1.016,
      "step": 2864
    },
    {
      "epoch": 2.7548076923076925,
      "grad_norm": 6.414878845214844,
      "learning_rate": 1.4492307692307695e-05,
      "loss": 1.1304,
      "step": 2865
    },
    {
      "epoch": 2.7557692307692307,
      "grad_norm": 7.4414544105529785,
      "learning_rate": 1.4490384615384616e-05,
      "loss": 1.5623,
      "step": 2866
    },
    {
      "epoch": 2.756730769230769,
      "grad_norm": 9.749595642089844,
      "learning_rate": 1.448846153846154e-05,
      "loss": 1.4661,
      "step": 2867
    },
    {
      "epoch": 2.7576923076923077,
      "grad_norm": 5.249487400054932,
      "learning_rate": 1.4486538461538461e-05,
      "loss": 0.6408,
      "step": 2868
    },
    {
      "epoch": 2.7586538461538463,
      "grad_norm": 6.071270942687988,
      "learning_rate": 1.4484615384615386e-05,
      "loss": 0.9356,
      "step": 2869
    },
    {
      "epoch": 2.7596153846153846,
      "grad_norm": 6.496845722198486,
      "learning_rate": 1.448269230769231e-05,
      "loss": 0.4855,
      "step": 2870
    },
    {
      "epoch": 2.760576923076923,
      "grad_norm": 5.4573845863342285,
      "learning_rate": 1.4480769230769231e-05,
      "loss": 1.6178,
      "step": 2871
    },
    {
      "epoch": 2.7615384615384615,
      "grad_norm": 8.20557975769043,
      "learning_rate": 1.4478846153846156e-05,
      "loss": 0.9776,
      "step": 2872
    },
    {
      "epoch": 2.7625,
      "grad_norm": 7.745553016662598,
      "learning_rate": 1.4476923076923077e-05,
      "loss": 1.4735,
      "step": 2873
    },
    {
      "epoch": 2.7634615384615384,
      "grad_norm": 9.277450561523438,
      "learning_rate": 1.4475000000000001e-05,
      "loss": 0.5318,
      "step": 2874
    },
    {
      "epoch": 2.7644230769230766,
      "grad_norm": 9.07568359375,
      "learning_rate": 1.4473076923076926e-05,
      "loss": 1.2167,
      "step": 2875
    },
    {
      "epoch": 2.7653846153846153,
      "grad_norm": 7.177707195281982,
      "learning_rate": 1.4471153846153847e-05,
      "loss": 0.7971,
      "step": 2876
    },
    {
      "epoch": 2.766346153846154,
      "grad_norm": 8.055814743041992,
      "learning_rate": 1.4469230769230771e-05,
      "loss": 0.6395,
      "step": 2877
    },
    {
      "epoch": 2.7673076923076922,
      "grad_norm": 7.530578136444092,
      "learning_rate": 1.4467307692307692e-05,
      "loss": 1.6984,
      "step": 2878
    },
    {
      "epoch": 2.768269230769231,
      "grad_norm": 8.276847839355469,
      "learning_rate": 1.4465384615384617e-05,
      "loss": 0.9732,
      "step": 2879
    },
    {
      "epoch": 2.769230769230769,
      "grad_norm": 7.030978679656982,
      "learning_rate": 1.4463461538461541e-05,
      "loss": 1.088,
      "step": 2880
    },
    {
      "epoch": 2.770192307692308,
      "grad_norm": 6.122035026550293,
      "learning_rate": 1.4461538461538462e-05,
      "loss": 0.1071,
      "step": 2881
    },
    {
      "epoch": 2.771153846153846,
      "grad_norm": 5.112480640411377,
      "learning_rate": 1.4459615384615387e-05,
      "loss": 0.7606,
      "step": 2882
    },
    {
      "epoch": 2.7721153846153848,
      "grad_norm": 10.826226234436035,
      "learning_rate": 1.4457692307692308e-05,
      "loss": 1.0018,
      "step": 2883
    },
    {
      "epoch": 2.773076923076923,
      "grad_norm": 8.198243141174316,
      "learning_rate": 1.4455769230769232e-05,
      "loss": 1.0677,
      "step": 2884
    },
    {
      "epoch": 2.7740384615384617,
      "grad_norm": 24.126846313476562,
      "learning_rate": 1.4453846153846157e-05,
      "loss": 1.0428,
      "step": 2885
    },
    {
      "epoch": 2.775,
      "grad_norm": 6.096242427825928,
      "learning_rate": 1.4451923076923078e-05,
      "loss": 0.19,
      "step": 2886
    },
    {
      "epoch": 2.7759615384615386,
      "grad_norm": 7.409319877624512,
      "learning_rate": 1.4450000000000002e-05,
      "loss": 1.2166,
      "step": 2887
    },
    {
      "epoch": 2.776923076923077,
      "grad_norm": 11.30475902557373,
      "learning_rate": 1.4448076923076923e-05,
      "loss": 0.1922,
      "step": 2888
    },
    {
      "epoch": 2.7778846153846155,
      "grad_norm": 6.555605888366699,
      "learning_rate": 1.4446153846153848e-05,
      "loss": 1.2447,
      "step": 2889
    },
    {
      "epoch": 2.7788461538461537,
      "grad_norm": 8.40888786315918,
      "learning_rate": 1.4444230769230772e-05,
      "loss": 0.6548,
      "step": 2890
    },
    {
      "epoch": 2.7798076923076924,
      "grad_norm": 7.032017707824707,
      "learning_rate": 1.4442307692307693e-05,
      "loss": 1.3072,
      "step": 2891
    },
    {
      "epoch": 2.7807692307692307,
      "grad_norm": 8.756611824035645,
      "learning_rate": 1.4440384615384618e-05,
      "loss": 1.755,
      "step": 2892
    },
    {
      "epoch": 2.7817307692307693,
      "grad_norm": 12.913969993591309,
      "learning_rate": 1.4438461538461539e-05,
      "loss": 1.5426,
      "step": 2893
    },
    {
      "epoch": 2.7826923076923076,
      "grad_norm": 8.616232872009277,
      "learning_rate": 1.4436538461538463e-05,
      "loss": 0.8849,
      "step": 2894
    },
    {
      "epoch": 2.7836538461538463,
      "grad_norm": 6.471970558166504,
      "learning_rate": 1.4434615384615384e-05,
      "loss": 0.7215,
      "step": 2895
    },
    {
      "epoch": 2.7846153846153845,
      "grad_norm": 9.144292831420898,
      "learning_rate": 1.4432692307692309e-05,
      "loss": 1.6488,
      "step": 2896
    },
    {
      "epoch": 2.785576923076923,
      "grad_norm": 6.287542819976807,
      "learning_rate": 1.4430769230769233e-05,
      "loss": 0.443,
      "step": 2897
    },
    {
      "epoch": 2.7865384615384614,
      "grad_norm": 6.538000106811523,
      "learning_rate": 1.4428846153846154e-05,
      "loss": 1.6349,
      "step": 2898
    },
    {
      "epoch": 2.7875,
      "grad_norm": 6.422068119049072,
      "learning_rate": 1.4426923076923079e-05,
      "loss": 1.4863,
      "step": 2899
    },
    {
      "epoch": 2.7884615384615383,
      "grad_norm": 7.313255786895752,
      "learning_rate": 1.4425e-05,
      "loss": 1.6848,
      "step": 2900
    },
    {
      "epoch": 2.789423076923077,
      "grad_norm": 5.673619270324707,
      "learning_rate": 1.4423076923076924e-05,
      "loss": 0.1269,
      "step": 2901
    },
    {
      "epoch": 2.7903846153846152,
      "grad_norm": 8.306770324707031,
      "learning_rate": 1.4421153846153849e-05,
      "loss": 1.4395,
      "step": 2902
    },
    {
      "epoch": 2.791346153846154,
      "grad_norm": 7.513195037841797,
      "learning_rate": 1.441923076923077e-05,
      "loss": 1.1323,
      "step": 2903
    },
    {
      "epoch": 2.792307692307692,
      "grad_norm": 8.453449249267578,
      "learning_rate": 1.4417307692307694e-05,
      "loss": 1.7896,
      "step": 2904
    },
    {
      "epoch": 2.793269230769231,
      "grad_norm": 10.24975299835205,
      "learning_rate": 1.4415384615384615e-05,
      "loss": 1.4466,
      "step": 2905
    },
    {
      "epoch": 2.794230769230769,
      "grad_norm": 5.364595413208008,
      "learning_rate": 1.441346153846154e-05,
      "loss": 1.4174,
      "step": 2906
    },
    {
      "epoch": 2.7951923076923078,
      "grad_norm": 8.163854598999023,
      "learning_rate": 1.4411538461538463e-05,
      "loss": 1.1943,
      "step": 2907
    },
    {
      "epoch": 2.796153846153846,
      "grad_norm": 7.625896453857422,
      "learning_rate": 1.4409615384615386e-05,
      "loss": 0.6836,
      "step": 2908
    },
    {
      "epoch": 2.7971153846153847,
      "grad_norm": 5.593891620635986,
      "learning_rate": 1.440769230769231e-05,
      "loss": 1.1141,
      "step": 2909
    },
    {
      "epoch": 2.7980769230769234,
      "grad_norm": 6.53892183303833,
      "learning_rate": 1.4405769230769231e-05,
      "loss": 1.0718,
      "step": 2910
    },
    {
      "epoch": 2.7990384615384616,
      "grad_norm": 9.51450252532959,
      "learning_rate": 1.4403846153846156e-05,
      "loss": 1.0151,
      "step": 2911
    },
    {
      "epoch": 2.8,
      "grad_norm": 9.928689002990723,
      "learning_rate": 1.4401923076923078e-05,
      "loss": 1.2772,
      "step": 2912
    },
    {
      "epoch": 2.8009615384615385,
      "grad_norm": 7.965191841125488,
      "learning_rate": 1.4400000000000001e-05,
      "loss": 1.7498,
      "step": 2913
    },
    {
      "epoch": 2.801923076923077,
      "grad_norm": 9.019084930419922,
      "learning_rate": 1.4398076923076924e-05,
      "loss": 1.5677,
      "step": 2914
    },
    {
      "epoch": 2.8028846153846154,
      "grad_norm": 7.658770561218262,
      "learning_rate": 1.4396153846153847e-05,
      "loss": 1.2141,
      "step": 2915
    },
    {
      "epoch": 2.8038461538461537,
      "grad_norm": 5.962653636932373,
      "learning_rate": 1.4394230769230771e-05,
      "loss": 0.9796,
      "step": 2916
    },
    {
      "epoch": 2.8048076923076923,
      "grad_norm": 7.3548665046691895,
      "learning_rate": 1.4392307692307694e-05,
      "loss": 1.5247,
      "step": 2917
    },
    {
      "epoch": 2.805769230769231,
      "grad_norm": 7.418818950653076,
      "learning_rate": 1.4390384615384617e-05,
      "loss": 1.5475,
      "step": 2918
    },
    {
      "epoch": 2.8067307692307693,
      "grad_norm": 7.4696550369262695,
      "learning_rate": 1.438846153846154e-05,
      "loss": 1.0421,
      "step": 2919
    },
    {
      "epoch": 2.8076923076923075,
      "grad_norm": 9.358729362487793,
      "learning_rate": 1.4386538461538462e-05,
      "loss": 1.504,
      "step": 2920
    },
    {
      "epoch": 2.808653846153846,
      "grad_norm": 6.7309393882751465,
      "learning_rate": 1.4384615384615387e-05,
      "loss": 0.9859,
      "step": 2921
    },
    {
      "epoch": 2.809615384615385,
      "grad_norm": 1.132612705230713,
      "learning_rate": 1.438269230769231e-05,
      "loss": 0.0066,
      "step": 2922
    },
    {
      "epoch": 2.810576923076923,
      "grad_norm": 6.146915435791016,
      "learning_rate": 1.4380769230769232e-05,
      "loss": 1.4687,
      "step": 2923
    },
    {
      "epoch": 2.8115384615384613,
      "grad_norm": 5.7339768409729,
      "learning_rate": 1.4378846153846155e-05,
      "loss": 1.1229,
      "step": 2924
    },
    {
      "epoch": 2.8125,
      "grad_norm": 6.526726722717285,
      "learning_rate": 1.4376923076923078e-05,
      "loss": 0.5992,
      "step": 2925
    },
    {
      "epoch": 2.8134615384615387,
      "grad_norm": 7.386462211608887,
      "learning_rate": 1.4375e-05,
      "loss": 0.9289,
      "step": 2926
    },
    {
      "epoch": 2.814423076923077,
      "grad_norm": 10.329784393310547,
      "learning_rate": 1.4373076923076925e-05,
      "loss": 0.4936,
      "step": 2927
    },
    {
      "epoch": 2.815384615384615,
      "grad_norm": 7.038339614868164,
      "learning_rate": 1.4371153846153848e-05,
      "loss": 0.4768,
      "step": 2928
    },
    {
      "epoch": 2.816346153846154,
      "grad_norm": 7.312256336212158,
      "learning_rate": 1.436923076923077e-05,
      "loss": 1.4075,
      "step": 2929
    },
    {
      "epoch": 2.8173076923076925,
      "grad_norm": 0.4709143042564392,
      "learning_rate": 1.4367307692307693e-05,
      "loss": 0.0042,
      "step": 2930
    },
    {
      "epoch": 2.8182692307692307,
      "grad_norm": 5.6882147789001465,
      "learning_rate": 1.4365384615384616e-05,
      "loss": 1.0301,
      "step": 2931
    },
    {
      "epoch": 2.819230769230769,
      "grad_norm": 12.90567684173584,
      "learning_rate": 1.436346153846154e-05,
      "loss": 0.3683,
      "step": 2932
    },
    {
      "epoch": 2.8201923076923077,
      "grad_norm": 11.500916481018066,
      "learning_rate": 1.4361538461538462e-05,
      "loss": 0.5736,
      "step": 2933
    },
    {
      "epoch": 2.8211538461538463,
      "grad_norm": 7.029038906097412,
      "learning_rate": 1.4359615384615386e-05,
      "loss": 1.1922,
      "step": 2934
    },
    {
      "epoch": 2.8221153846153846,
      "grad_norm": 6.938485622406006,
      "learning_rate": 1.4357692307692309e-05,
      "loss": 0.6873,
      "step": 2935
    },
    {
      "epoch": 2.823076923076923,
      "grad_norm": 6.825139045715332,
      "learning_rate": 1.4355769230769232e-05,
      "loss": 0.823,
      "step": 2936
    },
    {
      "epoch": 2.8240384615384615,
      "grad_norm": 19.34718132019043,
      "learning_rate": 1.4353846153846156e-05,
      "loss": 0.4173,
      "step": 2937
    },
    {
      "epoch": 2.825,
      "grad_norm": 9.615177154541016,
      "learning_rate": 1.4351923076923077e-05,
      "loss": 0.8595,
      "step": 2938
    },
    {
      "epoch": 2.8259615384615384,
      "grad_norm": 5.714629650115967,
      "learning_rate": 1.4350000000000002e-05,
      "loss": 1.2434,
      "step": 2939
    },
    {
      "epoch": 2.8269230769230766,
      "grad_norm": 72.58190155029297,
      "learning_rate": 1.4348076923076924e-05,
      "loss": 0.9494,
      "step": 2940
    },
    {
      "epoch": 2.8278846153846153,
      "grad_norm": 6.173705101013184,
      "learning_rate": 1.4346153846153847e-05,
      "loss": 1.1658,
      "step": 2941
    },
    {
      "epoch": 2.828846153846154,
      "grad_norm": 7.843838691711426,
      "learning_rate": 1.4344230769230772e-05,
      "loss": 1.8115,
      "step": 2942
    },
    {
      "epoch": 2.8298076923076922,
      "grad_norm": 5.8021345138549805,
      "learning_rate": 1.4342307692307693e-05,
      "loss": 1.8987,
      "step": 2943
    },
    {
      "epoch": 2.830769230769231,
      "grad_norm": 7.409119606018066,
      "learning_rate": 1.4340384615384617e-05,
      "loss": 1.6443,
      "step": 2944
    },
    {
      "epoch": 2.831730769230769,
      "grad_norm": 6.064990997314453,
      "learning_rate": 1.4338461538461538e-05,
      "loss": 1.102,
      "step": 2945
    },
    {
      "epoch": 2.832692307692308,
      "grad_norm": 20.447433471679688,
      "learning_rate": 1.4336538461538463e-05,
      "loss": 2.9803,
      "step": 2946
    },
    {
      "epoch": 2.833653846153846,
      "grad_norm": 7.677769184112549,
      "learning_rate": 1.4334615384615387e-05,
      "loss": 1.2856,
      "step": 2947
    },
    {
      "epoch": 2.8346153846153848,
      "grad_norm": 8.285064697265625,
      "learning_rate": 1.4332692307692308e-05,
      "loss": 1.0112,
      "step": 2948
    },
    {
      "epoch": 2.835576923076923,
      "grad_norm": 6.674293518066406,
      "learning_rate": 1.4330769230769233e-05,
      "loss": 1.3506,
      "step": 2949
    },
    {
      "epoch": 2.8365384615384617,
      "grad_norm": 8.079033851623535,
      "learning_rate": 1.4328846153846154e-05,
      "loss": 0.784,
      "step": 2950
    },
    {
      "epoch": 2.8375,
      "grad_norm": 8.051945686340332,
      "learning_rate": 1.4326923076923078e-05,
      "loss": 0.9787,
      "step": 2951
    },
    {
      "epoch": 2.8384615384615386,
      "grad_norm": 6.635240077972412,
      "learning_rate": 1.4325000000000003e-05,
      "loss": 1.6155,
      "step": 2952
    },
    {
      "epoch": 2.839423076923077,
      "grad_norm": 5.799230575561523,
      "learning_rate": 1.4323076923076924e-05,
      "loss": 1.1408,
      "step": 2953
    },
    {
      "epoch": 2.8403846153846155,
      "grad_norm": 7.068485736846924,
      "learning_rate": 1.4321153846153848e-05,
      "loss": 1.775,
      "step": 2954
    },
    {
      "epoch": 2.8413461538461537,
      "grad_norm": 7.51945161819458,
      "learning_rate": 1.431923076923077e-05,
      "loss": 1.6348,
      "step": 2955
    },
    {
      "epoch": 2.8423076923076924,
      "grad_norm": 6.970227241516113,
      "learning_rate": 1.4317307692307694e-05,
      "loss": 0.7169,
      "step": 2956
    },
    {
      "epoch": 2.8432692307692307,
      "grad_norm": 7.984292030334473,
      "learning_rate": 1.4315384615384618e-05,
      "loss": 0.409,
      "step": 2957
    },
    {
      "epoch": 2.8442307692307693,
      "grad_norm": 11.058501243591309,
      "learning_rate": 1.431346153846154e-05,
      "loss": 0.9812,
      "step": 2958
    },
    {
      "epoch": 2.8451923076923076,
      "grad_norm": 10.466535568237305,
      "learning_rate": 1.4311538461538464e-05,
      "loss": 0.4344,
      "step": 2959
    },
    {
      "epoch": 2.8461538461538463,
      "grad_norm": 6.384847640991211,
      "learning_rate": 1.4309615384615385e-05,
      "loss": 1.3399,
      "step": 2960
    },
    {
      "epoch": 2.8471153846153845,
      "grad_norm": 6.324575901031494,
      "learning_rate": 1.430769230769231e-05,
      "loss": 1.3266,
      "step": 2961
    },
    {
      "epoch": 2.848076923076923,
      "grad_norm": 7.159202575683594,
      "learning_rate": 1.430576923076923e-05,
      "loss": 0.9236,
      "step": 2962
    },
    {
      "epoch": 2.8490384615384614,
      "grad_norm": 12.709992408752441,
      "learning_rate": 1.4303846153846155e-05,
      "loss": 1.1982,
      "step": 2963
    },
    {
      "epoch": 2.85,
      "grad_norm": 10.674498558044434,
      "learning_rate": 1.430192307692308e-05,
      "loss": 1.9317,
      "step": 2964
    },
    {
      "epoch": 2.8509615384615383,
      "grad_norm": 5.770914077758789,
      "learning_rate": 1.43e-05,
      "loss": 0.9769,
      "step": 2965
    },
    {
      "epoch": 2.851923076923077,
      "grad_norm": 11.786354064941406,
      "learning_rate": 1.4298076923076925e-05,
      "loss": 1.5836,
      "step": 2966
    },
    {
      "epoch": 2.8528846153846152,
      "grad_norm": 7.02716588973999,
      "learning_rate": 1.4296153846153846e-05,
      "loss": 1.5634,
      "step": 2967
    },
    {
      "epoch": 2.853846153846154,
      "grad_norm": 10.246938705444336,
      "learning_rate": 1.429423076923077e-05,
      "loss": 0.9454,
      "step": 2968
    },
    {
      "epoch": 2.854807692307692,
      "grad_norm": 9.446233749389648,
      "learning_rate": 1.4292307692307695e-05,
      "loss": 1.0525,
      "step": 2969
    },
    {
      "epoch": 2.855769230769231,
      "grad_norm": 5.852631092071533,
      "learning_rate": 1.4290384615384616e-05,
      "loss": 0.6306,
      "step": 2970
    },
    {
      "epoch": 2.856730769230769,
      "grad_norm": 9.251442909240723,
      "learning_rate": 1.428846153846154e-05,
      "loss": 0.946,
      "step": 2971
    },
    {
      "epoch": 2.8576923076923078,
      "grad_norm": 10.187267303466797,
      "learning_rate": 1.4286538461538461e-05,
      "loss": 0.4326,
      "step": 2972
    },
    {
      "epoch": 2.858653846153846,
      "grad_norm": 9.657772064208984,
      "learning_rate": 1.4284615384615386e-05,
      "loss": 0.4418,
      "step": 2973
    },
    {
      "epoch": 2.8596153846153847,
      "grad_norm": 6.4054036140441895,
      "learning_rate": 1.428269230769231e-05,
      "loss": 1.2405,
      "step": 2974
    },
    {
      "epoch": 2.8605769230769234,
      "grad_norm": 16.502471923828125,
      "learning_rate": 1.4280769230769231e-05,
      "loss": 2.4817,
      "step": 2975
    },
    {
      "epoch": 2.8615384615384616,
      "grad_norm": 6.546433448791504,
      "learning_rate": 1.4278846153846156e-05,
      "loss": 1.0629,
      "step": 2976
    },
    {
      "epoch": 2.8625,
      "grad_norm": 7.087227821350098,
      "learning_rate": 1.4276923076923077e-05,
      "loss": 1.4591,
      "step": 2977
    },
    {
      "epoch": 2.8634615384615385,
      "grad_norm": 8.565171241760254,
      "learning_rate": 1.4275000000000001e-05,
      "loss": 0.7745,
      "step": 2978
    },
    {
      "epoch": 2.864423076923077,
      "grad_norm": 11.156481742858887,
      "learning_rate": 1.4273076923076926e-05,
      "loss": 1.1686,
      "step": 2979
    },
    {
      "epoch": 2.8653846153846154,
      "grad_norm": 6.283524036407471,
      "learning_rate": 1.4271153846153847e-05,
      "loss": 1.3119,
      "step": 2980
    },
    {
      "epoch": 2.8663461538461537,
      "grad_norm": 3.443256378173828,
      "learning_rate": 1.4269230769230771e-05,
      "loss": 0.0232,
      "step": 2981
    },
    {
      "epoch": 2.8673076923076923,
      "grad_norm": 7.687058448791504,
      "learning_rate": 1.4267307692307692e-05,
      "loss": 1.3855,
      "step": 2982
    },
    {
      "epoch": 2.868269230769231,
      "grad_norm": 9.333635330200195,
      "learning_rate": 1.4265384615384617e-05,
      "loss": 0.9929,
      "step": 2983
    },
    {
      "epoch": 2.8692307692307693,
      "grad_norm": 8.450946807861328,
      "learning_rate": 1.426346153846154e-05,
      "loss": 1.7252,
      "step": 2984
    },
    {
      "epoch": 2.8701923076923075,
      "grad_norm": 6.648272514343262,
      "learning_rate": 1.4261538461538462e-05,
      "loss": 1.2736,
      "step": 2985
    },
    {
      "epoch": 2.871153846153846,
      "grad_norm": 8.378210067749023,
      "learning_rate": 1.4259615384615387e-05,
      "loss": 1.2118,
      "step": 2986
    },
    {
      "epoch": 2.872115384615385,
      "grad_norm": 5.917113780975342,
      "learning_rate": 1.4257692307692308e-05,
      "loss": 1.3909,
      "step": 2987
    },
    {
      "epoch": 2.873076923076923,
      "grad_norm": 5.7965006828308105,
      "learning_rate": 1.4255769230769232e-05,
      "loss": 1.6881,
      "step": 2988
    },
    {
      "epoch": 2.8740384615384613,
      "grad_norm": 6.741889953613281,
      "learning_rate": 1.4253846153846155e-05,
      "loss": 1.1236,
      "step": 2989
    },
    {
      "epoch": 2.875,
      "grad_norm": 8.687253952026367,
      "learning_rate": 1.4251923076923078e-05,
      "loss": 0.8052,
      "step": 2990
    },
    {
      "epoch": 2.8759615384615387,
      "grad_norm": 9.394966125488281,
      "learning_rate": 1.425e-05,
      "loss": 0.5667,
      "step": 2991
    },
    {
      "epoch": 2.876923076923077,
      "grad_norm": 10.726757049560547,
      "learning_rate": 1.4248076923076924e-05,
      "loss": 0.4593,
      "step": 2992
    },
    {
      "epoch": 2.877884615384615,
      "grad_norm": 10.874190330505371,
      "learning_rate": 1.4246153846153848e-05,
      "loss": 1.0487,
      "step": 2993
    },
    {
      "epoch": 2.878846153846154,
      "grad_norm": 9.058426856994629,
      "learning_rate": 1.424423076923077e-05,
      "loss": 1.6959,
      "step": 2994
    },
    {
      "epoch": 2.8798076923076925,
      "grad_norm": 6.829958438873291,
      "learning_rate": 1.4242307692307694e-05,
      "loss": 0.6158,
      "step": 2995
    },
    {
      "epoch": 2.8807692307692307,
      "grad_norm": 10.149948120117188,
      "learning_rate": 1.4240384615384616e-05,
      "loss": 1.2293,
      "step": 2996
    },
    {
      "epoch": 2.881730769230769,
      "grad_norm": 5.722532272338867,
      "learning_rate": 1.4238461538461539e-05,
      "loss": 0.9261,
      "step": 2997
    },
    {
      "epoch": 2.8826923076923077,
      "grad_norm": 7.905673503875732,
      "learning_rate": 1.4236538461538464e-05,
      "loss": 1.1001,
      "step": 2998
    },
    {
      "epoch": 2.8836538461538463,
      "grad_norm": 9.247363090515137,
      "learning_rate": 1.4234615384615386e-05,
      "loss": 0.6694,
      "step": 2999
    },
    {
      "epoch": 2.8846153846153846,
      "grad_norm": 6.0770583152771,
      "learning_rate": 1.4232692307692309e-05,
      "loss": 0.6972,
      "step": 3000
    },
    {
      "epoch": 2.885576923076923,
      "grad_norm": 10.97352123260498,
      "learning_rate": 1.4230769230769232e-05,
      "loss": 0.7208,
      "step": 3001
    },
    {
      "epoch": 2.8865384615384615,
      "grad_norm": 8.687952041625977,
      "learning_rate": 1.4228846153846155e-05,
      "loss": 1.3972,
      "step": 3002
    },
    {
      "epoch": 2.8875,
      "grad_norm": 4.579394340515137,
      "learning_rate": 1.4226923076923077e-05,
      "loss": 0.0514,
      "step": 3003
    },
    {
      "epoch": 2.8884615384615384,
      "grad_norm": 7.104768753051758,
      "learning_rate": 1.4225000000000002e-05,
      "loss": 1.1869,
      "step": 3004
    },
    {
      "epoch": 2.8894230769230766,
      "grad_norm": 12.151468276977539,
      "learning_rate": 1.4223076923076925e-05,
      "loss": 0.036,
      "step": 3005
    },
    {
      "epoch": 2.8903846153846153,
      "grad_norm": 7.165835380554199,
      "learning_rate": 1.4221153846153847e-05,
      "loss": 1.0651,
      "step": 3006
    },
    {
      "epoch": 2.891346153846154,
      "grad_norm": 9.313802719116211,
      "learning_rate": 1.421923076923077e-05,
      "loss": 0.9258,
      "step": 3007
    },
    {
      "epoch": 2.8923076923076922,
      "grad_norm": 7.5753679275512695,
      "learning_rate": 1.4217307692307693e-05,
      "loss": 0.937,
      "step": 3008
    },
    {
      "epoch": 2.893269230769231,
      "grad_norm": 10.63892650604248,
      "learning_rate": 1.4215384615384617e-05,
      "loss": 0.1961,
      "step": 3009
    },
    {
      "epoch": 2.894230769230769,
      "grad_norm": 6.849754333496094,
      "learning_rate": 1.4213461538461538e-05,
      "loss": 0.0174,
      "step": 3010
    },
    {
      "epoch": 2.895192307692308,
      "grad_norm": 6.670915126800537,
      "learning_rate": 1.4211538461538463e-05,
      "loss": 1.0974,
      "step": 3011
    },
    {
      "epoch": 2.896153846153846,
      "grad_norm": 11.26742935180664,
      "learning_rate": 1.4209615384615386e-05,
      "loss": 0.6881,
      "step": 3012
    },
    {
      "epoch": 2.8971153846153848,
      "grad_norm": 7.108786582946777,
      "learning_rate": 1.4207692307692308e-05,
      "loss": 1.3153,
      "step": 3013
    },
    {
      "epoch": 2.898076923076923,
      "grad_norm": 4.861522674560547,
      "learning_rate": 1.4205769230769233e-05,
      "loss": 0.9468,
      "step": 3014
    },
    {
      "epoch": 2.8990384615384617,
      "grad_norm": 7.198997974395752,
      "learning_rate": 1.4203846153846154e-05,
      "loss": 1.5517,
      "step": 3015
    },
    {
      "epoch": 2.9,
      "grad_norm": 6.982017517089844,
      "learning_rate": 1.4201923076923078e-05,
      "loss": 1.5126,
      "step": 3016
    },
    {
      "epoch": 2.9009615384615386,
      "grad_norm": 9.49084186553955,
      "learning_rate": 1.4200000000000001e-05,
      "loss": 0.9373,
      "step": 3017
    },
    {
      "epoch": 2.901923076923077,
      "grad_norm": 5.8373918533325195,
      "learning_rate": 1.4198076923076924e-05,
      "loss": 0.9677,
      "step": 3018
    },
    {
      "epoch": 2.9028846153846155,
      "grad_norm": 24.823219299316406,
      "learning_rate": 1.4196153846153848e-05,
      "loss": 0.5423,
      "step": 3019
    },
    {
      "epoch": 2.9038461538461537,
      "grad_norm": 9.340476989746094,
      "learning_rate": 1.419423076923077e-05,
      "loss": 1.4535,
      "step": 3020
    },
    {
      "epoch": 2.9048076923076924,
      "grad_norm": 7.272890567779541,
      "learning_rate": 1.4192307692307694e-05,
      "loss": 0.9751,
      "step": 3021
    },
    {
      "epoch": 2.9057692307692307,
      "grad_norm": 6.284069538116455,
      "learning_rate": 1.4190384615384615e-05,
      "loss": 1.5704,
      "step": 3022
    },
    {
      "epoch": 2.9067307692307693,
      "grad_norm": 12.368765830993652,
      "learning_rate": 1.418846153846154e-05,
      "loss": 2.0224,
      "step": 3023
    },
    {
      "epoch": 2.9076923076923076,
      "grad_norm": 7.910043716430664,
      "learning_rate": 1.4186538461538462e-05,
      "loss": 1.665,
      "step": 3024
    },
    {
      "epoch": 2.9086538461538463,
      "grad_norm": 6.122280120849609,
      "learning_rate": 1.4184615384615385e-05,
      "loss": 1.1004,
      "step": 3025
    },
    {
      "epoch": 2.9096153846153845,
      "grad_norm": 12.316595077514648,
      "learning_rate": 1.418269230769231e-05,
      "loss": 0.8518,
      "step": 3026
    },
    {
      "epoch": 2.910576923076923,
      "grad_norm": 8.058989524841309,
      "learning_rate": 1.418076923076923e-05,
      "loss": 0.9408,
      "step": 3027
    },
    {
      "epoch": 2.9115384615384614,
      "grad_norm": 5.470203876495361,
      "learning_rate": 1.4178846153846155e-05,
      "loss": 0.4118,
      "step": 3028
    },
    {
      "epoch": 2.9125,
      "grad_norm": 6.959498882293701,
      "learning_rate": 1.4176923076923076e-05,
      "loss": 1.3014,
      "step": 3029
    },
    {
      "epoch": 2.9134615384615383,
      "grad_norm": 8.660746574401855,
      "learning_rate": 1.4175e-05,
      "loss": 1.0001,
      "step": 3030
    },
    {
      "epoch": 2.914423076923077,
      "grad_norm": 5.361863613128662,
      "learning_rate": 1.4173076923076925e-05,
      "loss": 0.3205,
      "step": 3031
    },
    {
      "epoch": 2.9153846153846152,
      "grad_norm": 6.085785388946533,
      "learning_rate": 1.4171153846153846e-05,
      "loss": 0.8468,
      "step": 3032
    },
    {
      "epoch": 2.916346153846154,
      "grad_norm": 9.0612211227417,
      "learning_rate": 1.416923076923077e-05,
      "loss": 0.7262,
      "step": 3033
    },
    {
      "epoch": 2.917307692307692,
      "grad_norm": 7.746124744415283,
      "learning_rate": 1.4167307692307692e-05,
      "loss": 1.5413,
      "step": 3034
    },
    {
      "epoch": 2.918269230769231,
      "grad_norm": 8.280800819396973,
      "learning_rate": 1.4165384615384616e-05,
      "loss": 1.119,
      "step": 3035
    },
    {
      "epoch": 2.919230769230769,
      "grad_norm": 7.788355827331543,
      "learning_rate": 1.416346153846154e-05,
      "loss": 0.5692,
      "step": 3036
    },
    {
      "epoch": 2.9201923076923078,
      "grad_norm": 9.09414291381836,
      "learning_rate": 1.4161538461538462e-05,
      "loss": 0.9359,
      "step": 3037
    },
    {
      "epoch": 2.921153846153846,
      "grad_norm": 9.689297676086426,
      "learning_rate": 1.4159615384615386e-05,
      "loss": 1.3819,
      "step": 3038
    },
    {
      "epoch": 2.9221153846153847,
      "grad_norm": 8.012947082519531,
      "learning_rate": 1.4157692307692307e-05,
      "loss": 1.0439,
      "step": 3039
    },
    {
      "epoch": 2.9230769230769234,
      "grad_norm": 5.6324687004089355,
      "learning_rate": 1.4155769230769232e-05,
      "loss": 0.8245,
      "step": 3040
    },
    {
      "epoch": 2.9240384615384616,
      "grad_norm": 10.103038787841797,
      "learning_rate": 1.4153846153846156e-05,
      "loss": 1.0019,
      "step": 3041
    },
    {
      "epoch": 2.925,
      "grad_norm": 5.378406047821045,
      "learning_rate": 1.4151923076923077e-05,
      "loss": 1.0807,
      "step": 3042
    },
    {
      "epoch": 2.9259615384615385,
      "grad_norm": 19.461856842041016,
      "learning_rate": 1.4150000000000002e-05,
      "loss": 0.8993,
      "step": 3043
    },
    {
      "epoch": 2.926923076923077,
      "grad_norm": 12.498778343200684,
      "learning_rate": 1.4148076923076923e-05,
      "loss": 1.3076,
      "step": 3044
    },
    {
      "epoch": 2.9278846153846154,
      "grad_norm": 6.8000383377075195,
      "learning_rate": 1.4146153846153847e-05,
      "loss": 0.0501,
      "step": 3045
    },
    {
      "epoch": 2.9288461538461537,
      "grad_norm": 8.949390411376953,
      "learning_rate": 1.4144230769230772e-05,
      "loss": 1.8661,
      "step": 3046
    },
    {
      "epoch": 2.9298076923076923,
      "grad_norm": 7.874557971954346,
      "learning_rate": 1.4142307692307693e-05,
      "loss": 1.4861,
      "step": 3047
    },
    {
      "epoch": 2.930769230769231,
      "grad_norm": 11.797818183898926,
      "learning_rate": 1.4140384615384617e-05,
      "loss": 1.0395,
      "step": 3048
    },
    {
      "epoch": 2.9317307692307693,
      "grad_norm": 6.59977912902832,
      "learning_rate": 1.4138461538461538e-05,
      "loss": 0.8418,
      "step": 3049
    },
    {
      "epoch": 2.9326923076923075,
      "grad_norm": 5.601043224334717,
      "learning_rate": 1.4136538461538463e-05,
      "loss": 1.061,
      "step": 3050
    },
    {
      "epoch": 2.933653846153846,
      "grad_norm": 8.112560272216797,
      "learning_rate": 1.4134615384615387e-05,
      "loss": 1.1781,
      "step": 3051
    },
    {
      "epoch": 2.934615384615385,
      "grad_norm": 6.611487865447998,
      "learning_rate": 1.4132692307692308e-05,
      "loss": 1.3733,
      "step": 3052
    },
    {
      "epoch": 2.935576923076923,
      "grad_norm": 8.931314468383789,
      "learning_rate": 1.4130769230769233e-05,
      "loss": 0.9438,
      "step": 3053
    },
    {
      "epoch": 2.9365384615384613,
      "grad_norm": 6.5715155601501465,
      "learning_rate": 1.4128846153846154e-05,
      "loss": 1.0149,
      "step": 3054
    },
    {
      "epoch": 2.9375,
      "grad_norm": 7.666360378265381,
      "learning_rate": 1.4126923076923078e-05,
      "loss": 1.5887,
      "step": 3055
    },
    {
      "epoch": 2.9384615384615387,
      "grad_norm": 4.538503646850586,
      "learning_rate": 1.4125000000000003e-05,
      "loss": 0.529,
      "step": 3056
    },
    {
      "epoch": 2.939423076923077,
      "grad_norm": 9.650897979736328,
      "learning_rate": 1.4123076923076924e-05,
      "loss": 1.3441,
      "step": 3057
    },
    {
      "epoch": 2.940384615384615,
      "grad_norm": 5.237680435180664,
      "learning_rate": 1.4121153846153848e-05,
      "loss": 0.8869,
      "step": 3058
    },
    {
      "epoch": 2.941346153846154,
      "grad_norm": 6.941783905029297,
      "learning_rate": 1.411923076923077e-05,
      "loss": 1.0009,
      "step": 3059
    },
    {
      "epoch": 2.9423076923076925,
      "grad_norm": 7.625612258911133,
      "learning_rate": 1.4117307692307694e-05,
      "loss": 0.9612,
      "step": 3060
    },
    {
      "epoch": 2.9432692307692307,
      "grad_norm": 14.217068672180176,
      "learning_rate": 1.4115384615384617e-05,
      "loss": 0.5487,
      "step": 3061
    },
    {
      "epoch": 2.944230769230769,
      "grad_norm": 7.440915107727051,
      "learning_rate": 1.411346153846154e-05,
      "loss": 1.0592,
      "step": 3062
    },
    {
      "epoch": 2.9451923076923077,
      "grad_norm": 8.938641548156738,
      "learning_rate": 1.4111538461538464e-05,
      "loss": 1.1369,
      "step": 3063
    },
    {
      "epoch": 2.9461538461538463,
      "grad_norm": 8.22891902923584,
      "learning_rate": 1.4109615384615385e-05,
      "loss": 1.2666,
      "step": 3064
    },
    {
      "epoch": 2.9471153846153846,
      "grad_norm": 6.5109028816223145,
      "learning_rate": 1.410769230769231e-05,
      "loss": 1.3155,
      "step": 3065
    },
    {
      "epoch": 2.948076923076923,
      "grad_norm": 8.135738372802734,
      "learning_rate": 1.4105769230769232e-05,
      "loss": 1.8544,
      "step": 3066
    },
    {
      "epoch": 2.9490384615384615,
      "grad_norm": 6.981268405914307,
      "learning_rate": 1.4103846153846155e-05,
      "loss": 0.7613,
      "step": 3067
    },
    {
      "epoch": 2.95,
      "grad_norm": 4.48288631439209,
      "learning_rate": 1.4101923076923078e-05,
      "loss": 0.953,
      "step": 3068
    },
    {
      "epoch": 2.9509615384615384,
      "grad_norm": 11.541730880737305,
      "learning_rate": 1.41e-05,
      "loss": 1.0048,
      "step": 3069
    },
    {
      "epoch": 2.9519230769230766,
      "grad_norm": 10.697747230529785,
      "learning_rate": 1.4098076923076925e-05,
      "loss": 0.7705,
      "step": 3070
    },
    {
      "epoch": 2.9528846153846153,
      "grad_norm": 6.693622589111328,
      "learning_rate": 1.4096153846153848e-05,
      "loss": 0.3977,
      "step": 3071
    },
    {
      "epoch": 2.953846153846154,
      "grad_norm": 17.02107048034668,
      "learning_rate": 1.409423076923077e-05,
      "loss": 0.4293,
      "step": 3072
    },
    {
      "epoch": 2.9548076923076922,
      "grad_norm": 9.932378768920898,
      "learning_rate": 1.4092307692307693e-05,
      "loss": 0.9206,
      "step": 3073
    },
    {
      "epoch": 2.955769230769231,
      "grad_norm": 6.993398189544678,
      "learning_rate": 1.4090384615384616e-05,
      "loss": 0.1732,
      "step": 3074
    },
    {
      "epoch": 2.956730769230769,
      "grad_norm": 7.0348896980285645,
      "learning_rate": 1.408846153846154e-05,
      "loss": 1.0229,
      "step": 3075
    },
    {
      "epoch": 2.957692307692308,
      "grad_norm": 8.05526065826416,
      "learning_rate": 1.4086538461538463e-05,
      "loss": 0.5752,
      "step": 3076
    },
    {
      "epoch": 2.958653846153846,
      "grad_norm": 6.4778571128845215,
      "learning_rate": 1.4084615384615386e-05,
      "loss": 0.9325,
      "step": 3077
    },
    {
      "epoch": 2.9596153846153848,
      "grad_norm": 6.476144790649414,
      "learning_rate": 1.4082692307692309e-05,
      "loss": 1.3064,
      "step": 3078
    },
    {
      "epoch": 2.960576923076923,
      "grad_norm": 8.310114860534668,
      "learning_rate": 1.4080769230769232e-05,
      "loss": 0.4988,
      "step": 3079
    },
    {
      "epoch": 2.9615384615384617,
      "grad_norm": 4.9466423988342285,
      "learning_rate": 1.4078846153846154e-05,
      "loss": 0.8226,
      "step": 3080
    },
    {
      "epoch": 2.9625,
      "grad_norm": 7.566082954406738,
      "learning_rate": 1.4076923076923079e-05,
      "loss": 1.0986,
      "step": 3081
    },
    {
      "epoch": 2.9634615384615386,
      "grad_norm": 8.512104034423828,
      "learning_rate": 1.4075000000000002e-05,
      "loss": 0.3477,
      "step": 3082
    },
    {
      "epoch": 2.964423076923077,
      "grad_norm": 7.39701509475708,
      "learning_rate": 1.4073076923076924e-05,
      "loss": 1.3513,
      "step": 3083
    },
    {
      "epoch": 2.9653846153846155,
      "grad_norm": 7.560978889465332,
      "learning_rate": 1.4071153846153847e-05,
      "loss": 1.018,
      "step": 3084
    },
    {
      "epoch": 2.9663461538461537,
      "grad_norm": 6.465576171875,
      "learning_rate": 1.406923076923077e-05,
      "loss": 0.5762,
      "step": 3085
    },
    {
      "epoch": 2.9673076923076924,
      "grad_norm": 5.8972954750061035,
      "learning_rate": 1.4067307692307694e-05,
      "loss": 0.3907,
      "step": 3086
    },
    {
      "epoch": 2.9682692307692307,
      "grad_norm": 6.525661468505859,
      "learning_rate": 1.4065384615384615e-05,
      "loss": 1.0043,
      "step": 3087
    },
    {
      "epoch": 2.9692307692307693,
      "grad_norm": 10.330144882202148,
      "learning_rate": 1.406346153846154e-05,
      "loss": 0.8054,
      "step": 3088
    },
    {
      "epoch": 2.9701923076923076,
      "grad_norm": 8.264640808105469,
      "learning_rate": 1.4061538461538463e-05,
      "loss": 1.0409,
      "step": 3089
    },
    {
      "epoch": 2.9711538461538463,
      "grad_norm": 8.90652847290039,
      "learning_rate": 1.4059615384615385e-05,
      "loss": 1.3022,
      "step": 3090
    },
    {
      "epoch": 2.9721153846153845,
      "grad_norm": 6.201253414154053,
      "learning_rate": 1.4057692307692308e-05,
      "loss": 1.3126,
      "step": 3091
    },
    {
      "epoch": 2.973076923076923,
      "grad_norm": 6.37420654296875,
      "learning_rate": 1.4055769230769231e-05,
      "loss": 1.3021,
      "step": 3092
    },
    {
      "epoch": 2.9740384615384614,
      "grad_norm": 6.544337272644043,
      "learning_rate": 1.4053846153846155e-05,
      "loss": 0.8704,
      "step": 3093
    },
    {
      "epoch": 2.975,
      "grad_norm": 6.01552677154541,
      "learning_rate": 1.4051923076923078e-05,
      "loss": 1.0233,
      "step": 3094
    },
    {
      "epoch": 2.9759615384615383,
      "grad_norm": 8.693875312805176,
      "learning_rate": 1.4050000000000001e-05,
      "loss": 0.8132,
      "step": 3095
    },
    {
      "epoch": 2.976923076923077,
      "grad_norm": 8.380510330200195,
      "learning_rate": 1.4048076923076924e-05,
      "loss": 1.0726,
      "step": 3096
    },
    {
      "epoch": 2.9778846153846152,
      "grad_norm": 6.1365838050842285,
      "learning_rate": 1.4046153846153847e-05,
      "loss": 1.4261,
      "step": 3097
    },
    {
      "epoch": 2.978846153846154,
      "grad_norm": 8.257534980773926,
      "learning_rate": 1.4044230769230771e-05,
      "loss": 0.7748,
      "step": 3098
    },
    {
      "epoch": 2.979807692307692,
      "grad_norm": 5.710333824157715,
      "learning_rate": 1.4042307692307692e-05,
      "loss": 0.9817,
      "step": 3099
    },
    {
      "epoch": 2.980769230769231,
      "grad_norm": 9.970440864562988,
      "learning_rate": 1.4040384615384617e-05,
      "loss": 1.3864,
      "step": 3100
    },
    {
      "epoch": 2.981730769230769,
      "grad_norm": 7.981698513031006,
      "learning_rate": 1.403846153846154e-05,
      "loss": 1.4835,
      "step": 3101
    },
    {
      "epoch": 2.9826923076923078,
      "grad_norm": 7.601302146911621,
      "learning_rate": 1.4036538461538462e-05,
      "loss": 1.1066,
      "step": 3102
    },
    {
      "epoch": 2.983653846153846,
      "grad_norm": 5.186030864715576,
      "learning_rate": 1.4034615384615387e-05,
      "loss": 0.5305,
      "step": 3103
    },
    {
      "epoch": 2.9846153846153847,
      "grad_norm": 11.735115051269531,
      "learning_rate": 1.4032692307692308e-05,
      "loss": 0.4789,
      "step": 3104
    },
    {
      "epoch": 2.9855769230769234,
      "grad_norm": 7.8074750900268555,
      "learning_rate": 1.4030769230769232e-05,
      "loss": 0.4652,
      "step": 3105
    },
    {
      "epoch": 2.9865384615384616,
      "grad_norm": 9.282951354980469,
      "learning_rate": 1.4028846153846153e-05,
      "loss": 1.1193,
      "step": 3106
    },
    {
      "epoch": 2.9875,
      "grad_norm": 6.546450138092041,
      "learning_rate": 1.4026923076923078e-05,
      "loss": 0.1385,
      "step": 3107
    },
    {
      "epoch": 2.9884615384615385,
      "grad_norm": 6.956435680389404,
      "learning_rate": 1.4025000000000002e-05,
      "loss": 1.477,
      "step": 3108
    },
    {
      "epoch": 2.989423076923077,
      "grad_norm": 6.7435736656188965,
      "learning_rate": 1.4023076923076923e-05,
      "loss": 1.5222,
      "step": 3109
    },
    {
      "epoch": 2.9903846153846154,
      "grad_norm": 8.113624572753906,
      "learning_rate": 1.4021153846153848e-05,
      "loss": 1.0481,
      "step": 3110
    },
    {
      "epoch": 2.9913461538461537,
      "grad_norm": 9.408413887023926,
      "learning_rate": 1.4019230769230769e-05,
      "loss": 0.5811,
      "step": 3111
    },
    {
      "epoch": 2.9923076923076923,
      "grad_norm": 5.604166030883789,
      "learning_rate": 1.4017307692307693e-05,
      "loss": 1.7873,
      "step": 3112
    },
    {
      "epoch": 2.993269230769231,
      "grad_norm": 5.5345587730407715,
      "learning_rate": 1.4015384615384618e-05,
      "loss": 1.2034,
      "step": 3113
    },
    {
      "epoch": 2.9942307692307693,
      "grad_norm": 6.395693778991699,
      "learning_rate": 1.4013461538461539e-05,
      "loss": 1.1798,
      "step": 3114
    },
    {
      "epoch": 2.9951923076923075,
      "grad_norm": 7.159302711486816,
      "learning_rate": 1.4011538461538463e-05,
      "loss": 1.3855,
      "step": 3115
    },
    {
      "epoch": 2.996153846153846,
      "grad_norm": 7.415891647338867,
      "learning_rate": 1.4009615384615384e-05,
      "loss": 1.3546,
      "step": 3116
    },
    {
      "epoch": 2.997115384615385,
      "grad_norm": 6.833553314208984,
      "learning_rate": 1.4007692307692309e-05,
      "loss": 0.3919,
      "step": 3117
    },
    {
      "epoch": 2.998076923076923,
      "grad_norm": 6.21543550491333,
      "learning_rate": 1.4005769230769233e-05,
      "loss": 0.0645,
      "step": 3118
    },
    {
      "epoch": 2.9990384615384613,
      "grad_norm": 6.415500640869141,
      "learning_rate": 1.4003846153846154e-05,
      "loss": 1.217,
      "step": 3119
    },
    {
      "epoch": 3.0,
      "grad_norm": 53.39745330810547,
      "learning_rate": 1.4001923076923079e-05,
      "loss": 0.3199,
      "step": 3120
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.0630760192871094,
      "eval_runtime": 22.8523,
      "eval_samples_per_second": 10.108,
      "eval_steps_per_second": 10.108,
      "step": 3120
    }
  ],
  "logging_steps": 1,
  "max_steps": 10400,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3798070363422720.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
